"""
RAG (Retrieval-Augmented Generation) API Routes

This module provides comprehensive FastAPI endpoints for RAG functionality including
document embedding, querying, chat interface, and document management.
"""

import logging
import os
import base64
from datetime import datetime
from typing import Dict, List, Optional, Any
from uuid import uuid4

from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form, Query, status, BackgroundTasks
from fastapi.responses import JSONResponse
import asyncio
try:
    # Try Pydantic v2 first
    from pydantic import BaseModel, Field, field_validator as validator
except ImportError:
    # Fall back to Pydantic v1
    from pydantic import BaseModel, Field, validator

from app.config import get_settings
from app.services.llamaindex_service import LlamaIndexService
from app.services.real_embeddings_service import RealEmbeddingsService
from app.services.advanced_search_service import QueryType, SearchOperator
from app.services.product_creation_service import ProductCreationService
from app.services.job_recovery_service import JobRecoveryService
from app.services.checkpoint_recovery_service import checkpoint_recovery_service, ProcessingStage
from app.services.supabase_client import get_supabase_client, SupabaseClient
from app.services.ai_model_tracker import AIModelTracker
from app.services.focused_product_extractor import get_focused_product_extractor
from app.services.product_relationship_service import ProductRelationshipService
from app.services.search_prompt_service import SearchPromptService
from app.services.stuck_job_analyzer import stuck_job_analyzer
from app.services.vecs_service import get_vecs_service
from app.utils.logging import PDFProcessingLogger
from app.utils.timeout_guard import with_timeout, TimeoutConstants, TimeoutError, ProgressiveTimeoutStrategy
from app.utils.circuit_breaker import claude_breaker, llama_breaker, clip_breaker, CircuitBreakerError
from app.utils.memory_monitor import memory_monitor

logger = logging.getLogger(__name__)

# ============================================================================
# Background Task Helper for Async Functions
# ============================================================================

def run_async_in_background(async_func):
    """
    Wrapper to run async functions in FastAPI BackgroundTasks.

    FastAPI's BackgroundTasks.add_task() expects synchronous functions.
    When an async function is passed, it doesn't execute properly because
    there's no event loop in the background thread.

    This wrapper creates a new event loop specifically for the background task,
    allowing async functions to run correctly in background threads.

    Usage:
        background_tasks.add_task(
            run_async_in_background(process_document_with_discovery),
            job_id=job_id,
            document_id=document_id,
            ...
        )

    Args:
        async_func: The async function to wrap

    Returns:
        A synchronous wrapper function that can be used with BackgroundTasks
    """
    def wrapper(*args, **kwargs):
        logger.info(f"üöÄ Background task wrapper started for {async_func.__name__}")
        # Create a new event loop for this background task
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            logger.info(f"‚ñ∂Ô∏è  Executing async function {async_func.__name__} in background")
            # Run the async function to completion
            loop.run_until_complete(async_func(*args, **kwargs))
            logger.info(f"‚úÖ Background task {async_func.__name__} completed successfully")
        except Exception as e:
            logger.error(f"‚ùå Background task {async_func.__name__} failed: {str(e)}", exc_info=True)
            raise
        finally:
            # Clean up the event loop
            loop.close()
            logger.info(f"üîö Background task wrapper finished for {async_func.__name__}")
    return wrapper

# Initialize router
router = APIRouter(prefix="/api/rag", tags=["RAG"])

# Job storage for async processing (in-memory cache)
job_storage: Dict[str, Dict[str, Any]] = {}

# Job recovery service (initialized on startup)
job_recovery_service: Optional[JobRecoveryService] = None


async def initialize_job_recovery():
    """
    Initialize job recovery service and mark any interrupted jobs.
    This should be called on application startup.
    """
    global job_recovery_service

    try:
        logger.info("üîÑ Initializing job recovery service...")

        supabase_client = get_supabase_client()
        job_recovery_service = JobRecoveryService(supabase_client)

        # Mark all processing jobs as interrupted (they were interrupted by restart)
        interrupted_count = await job_recovery_service.mark_all_processing_as_interrupted(
            reason="Service restart detected"
        )

        if interrupted_count > 0:
            logger.warning(f"üõë Marked {interrupted_count} jobs as interrupted due to service restart")

        # Get statistics
        stats = await job_recovery_service.get_job_statistics()
        logger.info(f"üìä Job statistics: {stats}")

        # Cleanup old jobs (older than 7 days)
        cleaned = await job_recovery_service.cleanup_old_jobs(days=7)
        if cleaned > 0:
            logger.info(f"üßπ Cleaned up {cleaned} old jobs")

        logger.info("‚úÖ Job recovery service initialized successfully")

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize job recovery service: {e}", exc_info=True)
        # Don't fail startup if job recovery fails
        job_recovery_service = None

# Pydantic models for request/response validation
class DocumentUploadRequest(BaseModel):
    """Request model for document upload and processing."""
    title: Optional[str] = Field(None, description="Document title")
    description: Optional[str] = Field(None, description="Document description")
    tags: Optional[List[str]] = Field(default_factory=list, description="Document tags")
    chunk_size: Optional[int] = Field(1000, ge=100, le=4000, description="Chunk size for processing")
    chunk_overlap: Optional[int] = Field(200, ge=0, le=1000, description="Chunk overlap")
    enable_embedding: bool = Field(True, description="Enable automatic embedding generation")

    # NEW: Consolidated upload parameters
    processing_mode: Optional[str] = Field(
        "standard",
        description="Processing mode: 'quick' (extract only), 'standard' (full RAG), 'deep' (complete analysis)"
    )
    categories: Optional[str] = Field(
        "all",
        description="Categories to extract: 'products', 'certificates', 'logos', 'specifications', 'all', 'extract_only'. Comma-separated."
    )
    file_url: Optional[str] = Field(
        None,
        description="URL to download PDF from (alternative to file upload)"
    )
    discovery_model: Optional[str] = Field(
        "claude",
        description="AI model for discovery: 'claude' (default), 'gpt', 'haiku'"
    )
    enable_prompt_enhancement: bool = Field(
        True,
        description="Enable AI prompt enhancement with admin customizations"
    )
    agent_prompt: Optional[str] = Field(
        None,
        description="Natural language instruction (e.g., 'extract products', 'search for NOVA')"
    )
    workspace_id: Optional[str] = Field(
        "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
        description="Workspace ID"
    )

class DocumentUploadResponse(BaseModel):
    """Response model for document upload."""
    document_id: str = Field(..., description="Unique document identifier")
    title: str = Field(..., description="Document title")
    status: str = Field(..., description="Processing status")
    chunks_created: int = Field(..., description="Number of chunks created")
    embeddings_generated: bool = Field(..., description="Whether embeddings were generated")
    processing_time: float = Field(..., description="Processing time in seconds")
    message: str = Field(..., description="Status message")

class QueryRequest(BaseModel):
    """Request model for RAG queries."""
    query: str = Field(..., min_length=1, max_length=2000, description="Query text")
    top_k: Optional[int] = Field(5, ge=1, le=20, description="Number of top results to retrieve")
    similarity_threshold: Optional[float] = Field(0.7, ge=0.0, le=1.0, description="Similarity threshold")
    include_metadata: bool = Field(True, description="Include document metadata in response")
    enable_reranking: bool = Field(True, description="Enable result reranking")
    document_ids: Optional[List[str]] = Field(None, description="Filter by specific document IDs")

class QueryResponse(BaseModel):
    """Response model for RAG queries."""
    query: str = Field(..., description="Original query")
    answer: str = Field(..., description="Generated answer")
    sources: List[Dict[str, Any]] = Field(..., description="Source documents and chunks")
    confidence_score: float = Field(..., description="Confidence score for the answer")
    processing_time: float = Field(..., description="Query processing time in seconds")
    retrieved_chunks: int = Field(..., description="Number of chunks retrieved")

class ChatRequest(BaseModel):
    """Request model for conversational RAG."""
    message: str = Field(..., min_length=1, max_length=2000, description="Chat message")
    conversation_id: Optional[str] = Field(None, description="Conversation ID for context")
    top_k: Optional[int] = Field(5, ge=1, le=20, description="Number of context chunks to retrieve")
    include_history: bool = Field(True, description="Include conversation history in context")
    document_ids: Optional[List[str]] = Field(None, description="Filter by specific document IDs")

class ChatResponse(BaseModel):
    """Response model for conversational RAG."""
    message: str = Field(..., description="Original message")
    response: str = Field(..., description="AI response")
    conversation_id: str = Field(..., description="Conversation ID")
    sources: List[Dict[str, Any]] = Field(..., description="Source documents used")
    processing_time: float = Field(..., description="Response generation time")

class SearchRequest(BaseModel):
    """Request model for semantic search."""
    query: str = Field(..., min_length=1, max_length=1000, description="Search query")
    search_type: str = Field("semantic", pattern="^(semantic|hybrid|keyword)$", description="Search type")
    top_k: Optional[int] = Field(10, ge=1, le=50, description="Number of results to return")
    similarity_threshold: Optional[float] = Field(0.6, ge=0.0, le=1.0, description="Similarity threshold")
    document_ids: Optional[List[str]] = Field(None, description="Filter by document IDs")
    include_content: bool = Field(True, description="Include chunk content in results")
    workspace_id: str = Field(..., description="Workspace ID for scoped search and related products")
    include_related_products: bool = Field(True, description="Include related products in results")
    related_products_limit: int = Field(3, ge=1, le=10, description="Max related products per result")
    use_search_prompts: bool = Field(True, description="Apply admin-configured search prompts")
    custom_formatting_prompt: Optional[str] = Field(None, description="Custom formatting prompt (overrides default)")

    # New fields for Issue #54 - Multi-Strategy Search
    material_filters: Optional[Dict[str, Any]] = Field(None, description="Material property filters for material search strategy")
    image_url: Optional[str] = Field(None, description="Image URL for image similarity search strategy")
    image_base64: Optional[str] = Field(None, description="Base64-encoded image for image similarity search strategy")

class SearchResponse(BaseModel):
    """Response model for semantic search."""
    query: str = Field(..., description="Original search query")
    enhanced_query: Optional[str] = Field(None, description="Enhanced query (if prompts applied)")
    results: List[Dict[str, Any]] = Field(..., description="Search results")
    total_results: int = Field(..., description="Total number of results")
    search_type: str = Field(..., description="Type of search performed")
    processing_time: float = Field(..., description="Search processing time")
    search_metadata: Optional[Dict[str, Any]] = Field(None, description="Search metadata (prompts applied, etc.)")

class DocumentListResponse(BaseModel):
    """Response model for document listing."""
    documents: List[Dict[str, Any]] = Field(..., description="List of documents")
    total_count: int = Field(..., description="Total number of documents")
    page: int = Field(..., description="Current page number")
    page_size: int = Field(..., description="Page size")

class HealthCheckResponse(BaseModel):
    """Response model for RAG health check."""
    status: str = Field(..., description="Health status")
    services: Dict[str, Dict[str, Any]] = Field(..., description="Service health details")
    timestamp: str = Field(..., description="Health check timestamp")

# Advanced Search Models for Phase 7 Features
class MMRSearchRequest(BaseModel):
    """Request model for MMR (Maximal Marginal Relevance) search."""
    query: str = Field(..., min_length=1, max_length=2000, description="Search query")
    top_k: Optional[int] = Field(10, ge=1, le=50, description="Number of initial results to retrieve")
    diversity_threshold: Optional[float] = Field(0.7, ge=0.0, le=1.0, description="MMR diversity threshold")
    lambda_param: Optional[float] = Field(0.5, ge=0.0, le=1.0, description="MMR lambda parameter for relevance vs diversity balance")
    document_ids: Optional[List[str]] = Field(None, description="Filter by specific document IDs")
    include_metadata: bool = Field(True, description="Include document metadata in response")

class MMRSearchResponse(BaseModel):
    """Response model for MMR search."""
    query: str = Field(..., description="Original search query")
    results: List[Dict[str, Any]] = Field(..., description="MMR search results with diversity scores")
    total_results: int = Field(..., description="Total number of results")
    diversity_score: float = Field(..., description="Overall diversity score of results")
    processing_time: float = Field(..., description="Search processing time in seconds")

class AdvancedQueryRequest(BaseModel):
    """Request model for advanced query with optimization."""
    query: str = Field(..., min_length=1, max_length=2000, description="Query text")
    query_type: str = Field("semantic", pattern="^(factual|analytical|conversational|boolean|fuzzy|semantic)$", description="Type of query")
    top_k: Optional[int] = Field(10, ge=1, le=50, description="Number of results to retrieve")
    enable_expansion: bool = Field(True, description="Enable query expansion")
    enable_rewriting: bool = Field(True, description="Enable query rewriting")
    similarity_threshold: Optional[float] = Field(0.6, ge=0.0, le=1.0, description="Similarity threshold")
    document_ids: Optional[List[str]] = Field(None, description="Filter by specific document IDs")
    metadata_filters: Optional[Dict[str, Any]] = Field(None, description="Metadata-based filters")
    search_operator: str = Field("AND", pattern="^(AND|OR|NOT)$", description="Search operator for multiple terms")

class AdvancedQueryResponse(BaseModel):
    """Response model for advanced query."""
    original_query: str = Field(..., description="Original query text")
    optimized_query: str = Field(..., description="Optimized/expanded query")
    query_type: str = Field(..., description="Type of query processed")
    results: List[Dict[str, Any]] = Field(..., description="Search results with relevance scores")
    total_results: int = Field(..., description="Total number of results")
    expansion_terms: List[str] = Field(..., description="Terms added during query expansion")
    processing_time: float = Field(..., description="Query processing time in seconds")
    confidence_score: float = Field(..., description="Overall confidence score")

# Dependency functions
async def get_llamaindex_service() -> LlamaIndexService:
    """Get LlamaIndex service instance using lazy loading."""
    from app.main import app
    import inspect

    # Try to use component_manager for lazy loading
    if hasattr(app.state, 'component_manager') and app.state.component_manager:
        try:
            service = await app.state.component_manager.get("llamaindex_service")
            if service:
                return service
        except Exception as e:
            logger.error(f"Failed to load LlamaIndex service via component_manager: {e}")

    # Fallback to direct service if already loaded (but not if it's a coroutine)
    if hasattr(app.state, 'llamaindex_service') and app.state.llamaindex_service is not None:
        # Check if it's actually a service instance, not a coroutine
        if not inspect.iscoroutine(app.state.llamaindex_service):
            return app.state.llamaindex_service
        else:
            logger.warning("app.state.llamaindex_service is a coroutine, not a service instance")

    # Service not available
    raise HTTPException(
        status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
        detail="LlamaIndex service is not available"
    )

async def get_embedding_service() -> RealEmbeddingsService:
    """Get embedding service instance."""
    try:
        supabase_client = get_supabase_client()
        return RealEmbeddingsService(supabase_client=supabase_client)
    except Exception as e:
        logger.error(f"Failed to initialize embedding service: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Embedding service is not available"
        )

# API Endpoints

# ============================================================================
# CONSOLIDATED UPLOAD ENDPOINT - Replaces all upload endpoints
# ============================================================================

@router.post("/documents/upload")
async def upload_document(
    background_tasks: BackgroundTasks,
    file: Optional[UploadFile] = File(None, description="PDF file to upload (required unless file_url is provided)"),

    # Basic metadata
    title: Optional[str] = Form(None, description="Document title"),
    description: Optional[str] = Form(None, description="Document description"),
    tags: Optional[str] = Form(None, description="Comma-separated tags"),

    # NEW: Processing mode parameter
    processing_mode: str = Form(
        "standard",
        description="Processing mode: 'quick' (extract only), 'standard' (full RAG), 'deep' (complete analysis)"
    ),

    # NEW: Category-based extraction
    categories: str = Form(
        "all",
        description="Categories to extract: 'products', 'certificates', 'logos', 'specifications', 'all', 'extract_only'. Comma-separated."
    ),

    # NEW: URL-based upload
    file_url: Optional[str] = Form(
        None,
        description="URL to download PDF from (alternative to file upload)"
    ),

    # Discovery settings
    discovery_model: str = Form(
        "claude",
        description="AI model for discovery: 'claude' (default), 'gpt', 'haiku'"
    ),

    # Processing settings
    chunk_size: int = Form(1000, ge=100, le=4000, description="Chunk size for text processing"),
    chunk_overlap: int = Form(200, ge=0, le=1000, description="Chunk overlap"),

    # Prompt enhancement
    enable_prompt_enhancement: bool = Form(
        True,
        description="Enable AI prompt enhancement with admin customizations"
    ),

    # Agent prompt - Optional natural language instruction
    agent_prompt: Optional[str] = Form(
        None,
        description="Natural language instruction (e.g., 'extract products', 'search for NOVA')"
    ),

    # Workspace
    workspace_id: str = Form(
        "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
        description="Workspace ID"
    )
):
    """
    **üéØ CONSOLIDATED UPLOAD ENDPOINT - Single Entry Point for All Upload Scenarios**

    This endpoint replaces:
    - `/api/documents/process` (simple extraction)
    - `/api/documents/process-url` (URL processing)
    - `/api/documents/upload` (old unified upload)

    ## üìã Processing Modes

    ### Quick Mode (`processing_mode="quick"`)
    - Fast extraction without RAG
    - No embeddings generated
    - No product discovery
    - Use for: Simple text/image extraction

    ### Standard Mode (`processing_mode="standard"`) - DEFAULT
    - Full RAG pipeline
    - Text embeddings generated
    - Product discovery and extraction
    - Use for: Normal document processing

    ### Deep Mode (`processing_mode="deep"`)
    - Complete analysis with all AI models
    - Image embeddings (CLIP)
    - Advanced product enrichment
    - Quality validation
    - Use for: High-quality catalog processing

    ## üé® Category-Based Extraction

    Control what gets extracted:
    - `categories="products"` - Extract only products
    - `categories="certificates"` - Extract only certificates
    - `categories="products,certificates"` - Extract multiple categories
    - `categories="all"` - Extract everything (default)
    - `categories="extract_only"` - Just extract text/images, no categorization

    ## üåê URL Processing

    Upload from URL instead of file:
    - Set `file_url="https://example.com/catalog.pdf"`
    - Leave `file` parameter empty
    - System downloads and processes automatically

    ## ü§ñ AI Model Selection

    Choose discovery model:
    - `discovery_model="claude"` - Claude Sonnet 4.5 (best quality, default)
    - `discovery_model="gpt"` - GPT-5 (fast, good quality)
    - `discovery_model="haiku"` - Claude Haiku 4.5 (fastest, lower cost)

    ## üí¨ Agent Prompts

    Use natural language instructions:
    - `agent_prompt="extract all products"` - Enhanced with product extraction details
    - `agent_prompt="search for NOVA"` - Enhanced with search context
    - `agent_prompt="find certificates"` - Enhanced with certificate extraction details

    ## üìä Processing Pipeline

    **Stage 0: Discovery (0-15%)**
    - AI analyzes entire PDF
    - Identifies all content by category
    - Maps images to entities
    - Extracts metadata

    **Stage 1: Extraction (15-30%)**
    - Extracts content for specified categories
    - Filters pages based on discovery results

    **Stage 2: Chunking (30-50%)**
    - Creates semantic chunks
    - Tags chunks with categories
    - Generates text embeddings

    **Stage 3: Image Processing (50-70%)**
    - Processes images for specified categories
    - AI analysis (Llama Vision)
    - Generates image embeddings (CLIP)

    **Stage 4: Entity Creation (70-90%)**
    - Creates products, certificates, logos, specifications
    - Links chunks and images
    - Attaches metadata

    **Stage 5: Quality Enhancement (90-100%)**
    - Async quality validation
    - Advanced embeddings
    - Entity enrichment

    ## üìù Examples

    ### Simple Extraction (Quick Mode)
    ```bash
    curl -X POST "/api/rag/documents/upload" \\
      -F "file=@document.pdf" \\
      -F "processing_mode=quick" \\
      -F "categories=extract_only"
    ```

    ### Standard Product Extraction
    ```bash
    curl -X POST "/api/rag/documents/upload" \\
      -F "file=@catalog.pdf" \\
      -F "processing_mode=standard" \\
      -F "categories=products"
    ```

    ### Deep Analysis with Multiple Categories
    ```bash
    curl -X POST "/api/rag/documents/upload" \\
      -F "file=@catalog.pdf" \\
      -F "processing_mode=deep" \\
      -F "categories=products,certificates,logos"
    ```

    ### URL Processing
    ```bash
    curl -X POST "/api/rag/documents/upload" \\
      -F "file_url=https://example.com/catalog.pdf" \\
      -F "processing_mode=standard" \\
      -F "categories=all"
    ```

    ### Agent-Driven Extraction
    ```bash
    curl -X POST "/api/rag/documents/upload" \\
      -F "file=@catalog.pdf" \\
      -F "agent_prompt=search for NOVA product" \\
      -F "categories=products"
    ```

    ## ‚úÖ Response Example

    ```json
    {
      "job_id": "550e8400-e29b-41d4-a716-446655440000",
      "document_id": "660e8400-e29b-41d4-a716-446655440001",
      "status": "pending",
      "message": "Document upload successful. Processing started.",
      "status_url": "/api/rag/documents/job/550e8400-e29b-41d4-a716-446655440000",
      "processing_mode": "standard",
      "categories": ["products", "certificates"],
      "estimated_time": "2-5 minutes"
    }
    ```

    ## üìä Monitoring Progress

    Poll the status URL to track processing:
    ```bash
    curl -X GET "/api/rag/documents/job/{job_id}"
    ```

    Response includes:
    - Current stage and progress percentage
    - Checkpoint information
    - AI model usage statistics
    - Extracted entities count (chunks, images, products)
    - Error details if failed

    ## ‚ö†Ô∏è Error Codes

    - **400 Bad Request**: Invalid parameters (missing file/URL, invalid mode, unsupported file type)
    - **401 Unauthorized**: Missing or invalid authentication
    - **413 Payload Too Large**: File exceeds size limit (100MB)
    - **415 Unsupported Media Type**: Non-PDF file uploaded
    - **500 Internal Server Error**: Processing initialization failed
    - **503 Service Unavailable**: Background job queue full

    ## üìè Limits

    - **Max file size**: 100MB
    - **Max concurrent jobs**: 5 per workspace
    - **Supported formats**: PDF only
    - **URL download timeout**: 60 seconds

    ## üîÑ Migration from Old Endpoints

    **Old:** `POST /api/documents/process`
    **New:** `POST /api/rag/documents/upload` with `processing_mode="quick"`

    **Old:** `POST /api/documents/process-url`
    **New:** `POST /api/rag/documents/upload` with `file_url` parameter

    **Old:** `POST /api/documents/upload`
    **New:** `POST /api/rag/documents/upload` (same endpoint, enhanced parameters)
    """

    try:
        # Validate input: either file or file_url must be provided
        if not file and not file_url:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Either 'file' or 'file_url' must be provided"
            )

        if file and file_url:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Provide either 'file' or 'file_url', not both"
            )

        # Validate processing mode
        valid_modes = ['quick', 'standard', 'deep']
        if processing_mode not in valid_modes:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid processing_mode '{processing_mode}'. Valid modes: {', '.join(valid_modes)}"
            )

        # Parse and validate categories
        category_list = [cat.strip() for cat in categories.split(',')]
        valid_categories = ['products', 'certificates', 'logos', 'specifications', 'all', 'extract_only']
        for cat in category_list:
            if cat not in valid_categories:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Invalid category '{cat}'. Valid categories: {', '.join(valid_categories)}"
                )

        # Expand 'all' to all categories
        # When 'all' is specified, we want FULL extraction, not focused extraction
        use_focused_extraction = True
        if 'all' in category_list:
            category_list = ['products', 'certificates', 'logos', 'specifications']
            use_focused_extraction = False  # Process ALL pages, not just category pages

        # Handle file upload or URL download
        file_content = None
        filename = None

        if file:
            # Validate file
            if not file.filename.lower().endswith('.pdf'):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Only PDF files are supported"
                )
            filename = file.filename
            file_content = await file.read()

        elif file_url:
            # Download from URL
            import aiohttp
            import tempfile

            logger.info(f"üì• Downloading PDF from URL: {file_url}")

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(file_url, timeout=aiohttp.ClientTimeout(total=60)) as response:
                        if response.status != 200:
                            raise HTTPException(
                                status_code=status.HTTP_400_BAD_REQUEST,
                                detail=f"Failed to download PDF from URL: HTTP {response.status}"
                            )

                        file_content = await response.read()

                        # Extract filename from URL or use default
                        from urllib.parse import urlparse
                        parsed_url = urlparse(file_url)
                        filename = parsed_url.path.split('/')[-1] or "downloaded.pdf"

                        if not filename.lower().endswith('.pdf'):
                            filename += '.pdf'

                        logger.info(f"‚úÖ Downloaded {len(file_content)} bytes from URL")

            except aiohttp.ClientError as e:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Failed to download PDF from URL: {str(e)}"
                )

        # Generate IDs
        from uuid import uuid4
        job_id = str(uuid4())
        document_id = str(uuid4())

        logger.info(f"üì§ CONSOLIDATED UPLOAD")
        logger.info(f"   Job ID: {job_id}")
        logger.info(f"   Document ID: {document_id}")
        logger.info(f"   Filename: {filename}")
        logger.info(f"   Processing Mode: {processing_mode}")
        logger.info(f"   Categories: {category_list}")
        logger.info(f"   Discovery Model: {discovery_model}")
        logger.info(f"   Source: {'URL' if file_url else 'Upload'}")
        if agent_prompt:
            logger.info(f"   Agent Prompt: {agent_prompt}")

        # Parse tags
        document_tags = []
        if tags:
            document_tags = [tag.strip() for tag in tags.split(',')]

        # Save file temporarily
        import tempfile
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
        temp_file.write(file_content)
        temp_file.close()
        file_path = temp_file.name

        # Get Supabase client
        supabase_client = get_supabase_client()

        # Create document record
        try:
            from datetime import datetime
            supabase_client.client.table('documents').insert({
                "id": document_id,
                "workspace_id": workspace_id,
                "filename": filename,
                "content_type": "application/pdf",
                "file_size": len(file_content),
                "file_path": file_path,
                "processing_status": "processing",
                "metadata": {
                    "title": title or filename,
                    "description": description or f"Document with {', '.join(category_list)} extraction",
                    "tags": document_tags,
                    "source": "consolidated_upload",
                    "processing_mode": processing_mode,
                    "categories": category_list,
                    "discovery_model": discovery_model,
                    "prompt_enhancement_enabled": enable_prompt_enhancement,
                    "agent_prompt": agent_prompt,
                    "file_url": file_url
                },
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }).execute()
            logger.info(f"‚úÖ Created document record {document_id}")
        except Exception as e:
            logger.error(f"‚ùå Failed to create document record: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to create document record: {str(e)}"
            )

        # Create processed_documents record (required for job_progress foreign key)
        # Use upsert to handle cases where record already exists
        try:
            supabase_client.client.table('processed_documents').upsert({
                "id": document_id,  # Use same ID as documents table
                "workspace_id": workspace_id,
                "pdf_document_id": document_id,
                "content": "",  # Will be populated during processing
                "processing_status": "processing",
                "processing_started_at": datetime.utcnow().isoformat(),
                "metadata": {
                    "processing_mode": processing_mode,
                    "categories": category_list
                },
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }).execute()
            logger.info(f"‚úÖ Created/updated processed_documents record: {document_id}")
        except Exception as proc_doc_error:
            logger.error(f"‚ùå Failed to create processed_documents record: {proc_doc_error}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to create processed_documents record: {str(proc_doc_error)}"
            )

        # Create background job record
        try:
            supabase_client.client.table('background_jobs').insert({
                "id": job_id,
                "filename": filename,
                "job_type": "product_discovery_upload",  # CRITICAL: Must match resume logic check
                "status": "processing",
                "progress": 0,
                "document_id": document_id,
                "workspace_id": workspace_id,
                "metadata": {
                    "filename": filename,
                    "processing_mode": processing_mode,
                    "categories": category_list,
                    "discovery_model": discovery_model,
                    "prompt_enhancement_enabled": enable_prompt_enhancement,
                    "agent_prompt": agent_prompt,
                    "file_url": file_url
                },
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }).execute()
            logger.info(f"‚úÖ Created background job record {job_id}")
        except Exception as e:
            logger.error(f"‚ùå Failed to create background job record: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to create background job record: {str(e)}"
            )

        # Start background processing
        # Note: "quick" mode is not implemented - all processing uses standard mode
        if processing_mode == "quick":
            logger.info("Quick mode requested but not implemented, using standard mode")
            processing_mode = "standard"

        # Use the existing process_document_with_discovery function
        # Wrap async function for background execution
        background_tasks.add_task(
            run_async_in_background(process_document_with_discovery),
            job_id=job_id,
            document_id=document_id,
            file_content=file_content,
            filename=filename,
            title=title,
            description=description,
            document_tags=document_tags,
            discovery_model=discovery_model,
            focused_extraction=use_focused_extraction,  # Use focused extraction based on categories
            extract_categories=category_list,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            workspace_id=workspace_id,
            agent_prompt=agent_prompt,
            enable_prompt_enhancement=enable_prompt_enhancement
        )

        return {
            "job_id": job_id,
            "document_id": document_id,
            "status": "processing",
            "message": f"Document upload started with {processing_mode} mode and {', '.join(category_list)} extraction",
            "status_url": f"/api/rag/documents/job/{job_id}",
            "processing_mode": processing_mode,
            "categories": category_list,
            "discovery_model": discovery_model,
            "prompt_enhancement_enabled": enable_prompt_enhancement,
            "source": "url" if file_url else "upload"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Consolidated upload failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Upload failed: {str(e)}"
        )


@router.get("/documents/job/{job_id}")
async def get_job_status(job_id: str):
    """
    Get the status of an async document processing job with checkpoint information.

    ALWAYS queries the database first as the source of truth, then optionally merges
    with in-memory data for additional real-time details.

    Returns:
        - Job status and progress (from database)
        - Latest checkpoint information
        - Detailed metadata including AI usage, chunks, images, products
        - In-memory state comparison (if available)
    """
    # ALWAYS check database FIRST - this is the source of truth
    try:
        supabase_client = get_supabase_client()
        logger.info(f"üîç [DB QUERY] Checking database for job {job_id}")
        response = supabase_client.client.table('background_jobs').select('*').eq('id', job_id).execute()
        logger.info(f"üîç [DB QUERY] Database response: data={response.data}, count={len(response.data) if response.data else 0}")

        if response.data and len(response.data) > 0:
            job = response.data[0]
            logger.info(f"‚úÖ [DB QUERY] Found job in database: {job['id']}, status={job['status']}, progress={job.get('progress', 0)}%")

            # Build response from DATABASE data (source of truth)
            job_response = {
                "job_id": job['id'],
                "status": job['status'],
                "document_id": job.get('document_id'),
                "progress": job.get('progress', 0),
                "error": job.get('error'),
                "metadata": job.get('metadata', {}),
                "created_at": job.get('created_at'),
                "updated_at": job.get('updated_at'),
                "source": "database"  # Indicate this came from DB
            }

            # Optionally merge with in-memory data for comparison/debugging
            if job_id in job_storage:
                memory_data = job_storage[job_id]
                logger.info(f"üìä [COMPARISON] In-memory status: {memory_data.get('status')}, progress: {memory_data.get('progress', 0)}%")

                # Add comparison data
                job_response["memory_state"] = {
                    "status": memory_data.get('status'),
                    "progress": memory_data.get('progress', 0),
                    "matches_db": (
                        memory_data.get('status') == job['status'] and
                        memory_data.get('progress', 0) == job.get('progress', 0)
                    )
                }

                # Log discrepancies
                if not job_response["memory_state"]["matches_db"]:
                    logger.warning(
                        f"‚ö†Ô∏è [MISMATCH] DB vs Memory mismatch for job {job_id}: "
                        f"DB({job['status']}, {job.get('progress', 0)}%) vs "
                        f"Memory({memory_data.get('status')}, {memory_data.get('progress', 0)}%)"
                    )

            # Add checkpoint information
            try:
                last_checkpoint = await checkpoint_recovery_service.get_last_checkpoint(job_id)
                if last_checkpoint:
                    job_response["last_checkpoint"] = {
                        "stage": last_checkpoint.get('stage'),
                        "created_at": last_checkpoint.get('created_at'),
                        "data": last_checkpoint.get('checkpoint_data', {})
                    }
            except Exception as e:
                logger.error(f"Failed to get checkpoint for job {job_id}: {e}")

            return JSONResponse(content=job_response)
        else:
            logger.warning(f"‚ö†Ô∏è [DB QUERY] Job {job_id} not found in database")

            # Check if it exists in memory (shouldn't happen in normal flow)
            if job_id in job_storage:
                logger.error(
                    f"üö® [CRITICAL] Job {job_id} exists in memory but NOT in database! "
                    f"This indicates a database sync failure."
                )
                # Create serializable copy of job_storage (exclude ai_tracker)
                memory_state = {k: v for k, v in job_storage[job_id].items() if k != 'ai_tracker'}
                return JSONResponse(
                    status_code=500,
                    content={
                        "error": "Database sync failure",
                        "detail": "Job exists in memory but not in database",
                        "job_id": job_id,
                        "memory_state": memory_state
                    }
                )

    except Exception as e:
        logger.error(f"‚ùå [DB ERROR] Error checking database for job {job_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Database query failed: {str(e)}"
        )

    # Job not found in database or memory
    raise HTTPException(
        status_code=status.HTTP_404_NOT_FOUND,
        detail=f"Job {job_id} not found in database"
    )


@router.get("/jobs/{job_id}/checkpoints")
async def get_job_checkpoints(job_id: str):
    """
    Get all checkpoints for a job.

    Returns:
        - List of all checkpoints with stage, data, and metadata
        - Checkpoint count
        - Processing timeline
    """
    try:
        checkpoints = await checkpoint_recovery_service.get_all_checkpoints(job_id)

        return JSONResponse(content={
            "job_id": job_id,
            "checkpoints": checkpoints,
            "count": len(checkpoints),
            "stages_completed": [cp.get('stage') for cp in checkpoints]
        })
    except Exception as e:
        logger.error(f"Failed to get checkpoints for job {job_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve checkpoints: {str(e)}"
        )


@router.post("/jobs/{job_id}/restart")
async def restart_job_from_checkpoint(job_id: str, background_tasks: BackgroundTasks):
    """
    Manually restart a job from its last checkpoint.

    This endpoint allows manual recovery of stuck or failed jobs.
    The job will resume from the last successful checkpoint.
    """
    try:
        # Get last checkpoint
        last_checkpoint = await checkpoint_recovery_service.get_last_checkpoint(job_id)

        if not last_checkpoint:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No checkpoint found for job {job_id}"
            )

        # Verify checkpoint data exists
        resume_stage_str = last_checkpoint.get('stage')
        resume_stage = ProcessingStage(resume_stage_str)
        can_resume = await checkpoint_recovery_service.verify_checkpoint_data(job_id, resume_stage)

        if not can_resume:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Checkpoint data verification failed for stage {resume_stage}"
            )

        # Get job details from database
        supabase_client = get_supabase_client()
        job_result = supabase_client.client.table('background_jobs').select('*').eq('id', job_id).execute()

        if not job_result.data or len(job_result.data) == 0:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} not found in database"
            )

        job_data = job_result.data[0]
        document_id = job_data['document_id']

        # Mark job for restart
        supabase_client.client.table('background_jobs').update({
            "status": "processing",  # ‚úÖ Set to processing immediately
            "error": None,  # Clear previous error
            "interrupted_at": None,  # Clear interrupted timestamp
            "started_at": datetime.utcnow().isoformat(),
            "metadata": {
                **job_data.get('metadata', {}),
                "restart_from_stage": resume_stage.value,
                "restart_reason": "manual_restart",
                "restart_at": datetime.utcnow().isoformat()
            }
        }).eq('id', job_id).execute()

        logger.info(f"‚úÖ Job {job_id} marked for restart from {resume_stage}")

        # ‚úÖ CRITICAL FIX: Restart the job by calling the LlamaIndex service directly
        # The process_document_background function doesn't support resume_from_checkpoint
        # Instead, we need to trigger the processing through the service layer

        # Get the file content from storage
        try:
            # Get document details
            doc_result = supabase_client.client.table('documents').select('*').eq('id', document_id).execute()
            if not doc_result.data or len(doc_result.data) == 0:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Document {document_id} not found"
                )

            doc_data = doc_result.data[0]
            file_path = doc_data.get('file_path')
            filename = doc_data.get('filename', 'document.pdf')
            metadata = doc_data.get('metadata', {})

            # CRITICAL FIX: If file_path is a local temp file, use file_url from metadata instead
            if file_path and file_path.startswith('/tmp/'):
                file_url = metadata.get('file_url')
                if file_url:
                    logger.info(f"‚ö†Ô∏è file_path is local temp file ({file_path}), using file_url from metadata: {file_url}")
                    file_path = file_url
                else:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail=f"Document {document_id} has local temp file_path but no file_url in metadata"
                    )

            if not file_path:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Document {document_id} has no file_path"
                )

            # Download file from storage or URL
            logger.info(f"üì• Downloading file from: {file_path}")

            # Check if file_path is a full URL (starts with http:// or https://)
            if file_path.startswith('http://') or file_path.startswith('https://'):
                # Download from URL
                import httpx
                async with httpx.AsyncClient() as client:
                    response = await client.get(file_path)
                    response.raise_for_status()
                    file_response = response.content
                    logger.info(f"‚úÖ Downloaded file from URL: {len(file_response)} bytes")
            else:
                # Download from Supabase storage
                bucket_name = file_path.split('/')[0] if '/' in file_path else 'pdf-documents'
                storage_path = '/'.join(file_path.split('/')[1:]) if '/' in file_path else file_path
                file_response = supabase_client.client.storage.from_(bucket_name).download(storage_path)
            if not file_response:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"File not found in storage: {file_path}"
                )

            file_content = file_response
            logger.info(f"‚úÖ Downloaded file: {len(file_content)} bytes")

            # Initialize job in job_storage (CRITICAL: required by process_document_background)
            job_storage[job_id] = {
                "job_id": job_id,
                "document_id": document_id,
                "status": "processing",
                "progress": job_data.get('progress', 0),
                "metadata": job_data.get('metadata', {})
            }
            logger.info(f"‚úÖ Job {job_id} added to job_storage for resume")

            # Determine which processing function to use based on job_type
            job_type = job_data.get('job_type', 'document_upload')

            if job_type == 'product_discovery_upload':
                # Use product discovery pipeline for resume
                logger.info(f"üîÑ Resuming product discovery job {job_id}")

                # Extract parameters from job metadata
                job_metadata = job_data.get('metadata', {})
                discovery_model = job_metadata.get('discovery_model', 'claude-sonnet-4.5')
                categories = job_metadata.get('categories', ['products'])
                enable_prompt_enhancement = job_metadata.get('prompt_enhancement_enabled', False)
                agent_prompt = job_metadata.get('agent_prompt')

                # Determine focused extraction based on categories
                use_focused_extraction = 'all' not in categories

                logger.info(f"   Resume parameters: discovery_model={discovery_model}, categories={categories}, focused={use_focused_extraction}")

                background_tasks.add_task(
                    run_async_in_background(process_document_with_discovery),
                    job_id=job_id,
                    document_id=document_id,
                    file_content=file_content,
                    filename=filename,
                    workspace_id=doc_data.get('workspace_id', 'ffafc28b-1b8b-4b0d-b226-9f9a6154004e'),
                    title=doc_data.get('title'),
                    description=doc_data.get('description'),
                    document_tags=doc_data.get('tags', []),
                    discovery_model=discovery_model,
                    focused_extraction=use_focused_extraction,
                    extract_categories=categories,
                    chunk_size=1000,
                    chunk_overlap=200,
                    agent_prompt=agent_prompt,
                    enable_prompt_enhancement=enable_prompt_enhancement
                )
            else:
                # Use standard processing for resume
                logger.info(f"üîÑ Resuming standard document job {job_id}")
                background_tasks.add_task(
                    run_async_in_background(process_document_background),
                    job_id=job_id,
                    document_id=document_id,
                    file_content=file_content,
                    filename=filename,
                    title=doc_data.get('title'),
                    description=doc_data.get('description'),
                    document_tags=doc_data.get('tags', []),
                    chunk_size=1000,
                    chunk_overlap=200,
                    llamaindex_service=None  # Will be retrieved from app state
                )

            logger.info(f"‚úÖ Background task triggered for job {job_id} (type: {job_type})")

        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to download file for restart: {e}", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to download file for restart: {str(e)}"
            )

        return JSONResponse(content={
            "success": True,
            "message": f"Job restarted from checkpoint: {resume_stage}",
            "job_id": job_id,
            "restart_stage": resume_stage.value,
            "checkpoint_data": last_checkpoint.get('checkpoint_data', {})
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to restart job {job_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to restart job: {str(e)}"
        )


@router.post("/documents/job/{job_id}/resume")
async def resume_job(job_id: str, background_tasks: BackgroundTasks):
    """
    Resume a job from its last checkpoint (alias for restart).

    This endpoint is the same as /jobs/{job_id}/restart but with a more intuitive name.
    """
    return await restart_job_from_checkpoint(job_id, background_tasks)


@router.delete("/documents/job/{job_id}")
async def delete_job(job_id: str):
    """
    Delete a job and all its associated data.

    This endpoint:
    1. Removes job from in-memory job_storage
    2. Deletes job record from database
    3. Cleans up any temporary files associated with the job

    Args:
        job_id: The unique identifier of the job to delete

    Returns:
        Success message with deleted job_id

    Raises:
        HTTPException: If job not found or deletion fails
    """
    try:
        logger.info(f"üóëÔ∏è DELETE /documents/job/{job_id} - Deleting job")

        # Remove from in-memory storage if exists
        if job_id in job_storage:
            del job_storage[job_id]
            logger.info(f"   ‚úÖ Removed job {job_id} from job_storage")

        # Delete from database
        supabase_client = get_supabase_client()
        result = supabase_client.client.table('async_jobs').delete().eq('job_id', job_id).execute()

        if not result.data:
            logger.warning(f"   ‚ö†Ô∏è Job {job_id} not found in database")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} not found"
            )

        logger.info(f"   ‚úÖ Deleted job {job_id} from database")

        # TODO: Clean up temporary files if needed
        # This would require tracking temp file paths in job metadata

        return {
            "success": True,
            "message": f"Job {job_id} deleted successfully",
            "job_id": job_id
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete job {job_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete job: {str(e)}"
        )


@router.get("/documents/jobs")
async def list_jobs(
    limit: int = 10,
    offset: int = 0,
    status_filter: Optional[str] = None,
    sort: str = "created_at:desc"
):
    """
    List all background jobs with optional filtering and sorting.

    Args:
        limit: Maximum number of jobs to return (default: 10)
        offset: Number of jobs to skip (default: 0)
        status_filter: Filter by status (pending, processing, completed, failed, interrupted)
        sort: Sort order (created_at:desc, created_at:asc, progress:desc, progress:asc)

    Returns:
        List of jobs with status, progress, and metadata
    """
    try:
        supabase_client = get_supabase_client()

        # Build query
        query = supabase_client.client.table('background_jobs').select('*')

        # Apply status filter
        if status_filter:
            query = query.eq('status', status_filter)

        # Apply sorting
        if ':' in sort:
            field, direction = sort.split(':')
            ascending = direction.lower() == 'asc'
            query = query.order(field, desc=not ascending)
        else:
            query = query.order('created_at', desc=True)

        # Apply pagination
        query = query.range(offset, offset + limit - 1)

        # Execute query
        result = query.execute()

        jobs = result.data if result.data else []

        return JSONResponse(content={
            "jobs": jobs,
            "count": len(jobs),
            "limit": limit,
            "offset": offset
        })

    except Exception as e:
        logger.error(f"Failed to list jobs: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list jobs: {str(e)}"
        )


@router.get("/chunks")
async def get_chunks(
    document_id: Optional[str] = Query(None, description="Filter by document ID"),
    limit: int = Query(100, ge=1, le=1000, description="Maximum number of chunks to return"),
    offset: int = Query(0, ge=0, description="Number of chunks to skip"),
    include_embeddings: bool = Query(True, description="Include embeddings in response")
):
    """
    Get chunks for a document with embeddings.

    Args:
        document_id: Document ID to filter chunks
        limit: Maximum number of chunks to return
        offset: Pagination offset
        include_embeddings: Whether to include embeddings (default: True)

    Returns:
        List of chunks with metadata and embeddings
    """
    try:
        supabase_client = get_supabase_client()

        if not document_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="document_id is required"
            )

        # Query chunks
        query = supabase_client.client.table('document_chunks').select('*').eq('document_id', document_id)
        query = query.range(offset, offset + limit - 1)
        result = query.execute()

        chunks = result.data if result.data else []

        # Add embeddings to each chunk if requested
        if include_embeddings and chunks:
            for chunk in chunks:
                embeddings_response = supabase_client.client.table('embeddings').select('*').eq('chunk_id', chunk['id']).execute()
                embeddings = embeddings_response.data or []
                # Add embedding field for backward compatibility
                chunk['embedding'] = embeddings[0]['embedding'] if embeddings else None
                chunk['embeddings'] = embeddings
                chunk['has_embedding'] = len(embeddings) > 0

        return JSONResponse(content={
            "document_id": document_id,
            "chunks": chunks,
            "count": len(chunks),
            "limit": limit,
            "offset": offset
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get chunks for document {document_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve chunks: {str(e)}"
        )


@router.get("/images")
async def get_images(
    document_id: Optional[str] = Query(None, description="Filter by document ID"),
    limit: int = Query(100, ge=1, le=1000, description="Maximum number of images to return"),
    offset: int = Query(0, ge=0, description="Number of images to skip")
):
    """
    Get images for a document.

    Args:
        document_id: Document ID to filter images
        limit: Maximum number of images to return
        offset: Pagination offset

    Returns:
        List of images with metadata
    """
    try:
        supabase_client = get_supabase_client()

        if not document_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="document_id is required"
            )

        # Query images
        query = supabase_client.client.table('document_images').select('*').eq('document_id', document_id)
        query = query.range(offset, offset + limit - 1)
        result = query.execute()

        images = result.data if result.data else []

        return JSONResponse(content={
            "document_id": document_id,
            "images": images,
            "count": len(images),
            "limit": limit,
            "offset": offset
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get images for document {document_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve images: {str(e)}"
        )


@router.get("/products")
async def get_products(
    document_id: Optional[str] = Query(None, description="Filter by document ID"),
    limit: int = Query(100, ge=1, le=1000, description="Maximum number of products to return"),
    offset: int = Query(0, ge=0, description="Number of products to skip")
):
    """
    Get products for a document.

    Args:
        document_id: Document ID to filter products
        limit: Maximum number of products to return
        offset: Pagination offset

    Returns:
        List of products with metadata
    """
    try:
        supabase_client = get_supabase_client()

        if not document_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="document_id is required"
            )

        # Query products
        query = supabase_client.client.table('products').select('*').eq('source_document_id', document_id)
        query = query.range(offset, offset + limit - 1)
        result = query.execute()

        products = result.data if result.data else []

        return JSONResponse(content={
            "document_id": document_id,
            "products": products,
            "count": len(products),
            "limit": limit,
            "offset": offset
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get products for document {document_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve products: {str(e)}"
        )


@router.get("/embeddings")
async def get_embeddings(
    document_id: Optional[str] = Query(None, description="Filter by document ID"),
    embedding_type: Optional[str] = Query(None, description="Filter by embedding type (text, visual, multimodal, color, texture, application)"),
    limit: int = Query(100, ge=1, le=1000, description="Maximum number of embeddings to return"),
    offset: int = Query(0, ge=0, description="Number of embeddings to skip")
):
    """
    Get embeddings for a document.

    Args:
        document_id: Document ID to filter embeddings
        embedding_type: Type of embedding (text, visual, multimodal, color, texture, application)
        limit: Maximum number of embeddings to return
        offset: Pagination offset

    Returns:
        List of embeddings with metadata including:
        - id: Embedding ID
        - document_id: Source document
        - chunk_id: Associated chunk (for text embeddings)
        - image_id: Associated image (for visual embeddings)
        - embedding_type: Type of embedding
        - embedding_model: Model used to generate embedding
        - embedding_dimensions: Vector dimensions
        - metadata: Additional metadata (quality scores, confidence, etc.)
    """
    try:
        supabase_client = get_supabase_client()

        if not document_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="document_id is required"
            )

        # Query embeddings - JOIN through chunks to get document_id
        # embeddings table has chunk_id, not document_id
        query = supabase_client.client.table('embeddings').select(
            '*, document_chunks!inner(document_id)'
        ).eq('document_chunks.document_id', document_id)

        # Filter by embedding type if specified
        if embedding_type:
            query = query.eq('embedding_type', embedding_type)

        query = query.range(offset, offset + limit - 1)
        result = query.execute()

        embeddings = result.data if result.data else []

        # Enrich embeddings with summary statistics
        embedding_stats = {}
        for emb in embeddings:
            emb_type = emb.get('embedding_type', 'unknown')
            if emb_type not in embedding_stats:
                embedding_stats[emb_type] = 0
            embedding_stats[emb_type] += 1

        return JSONResponse(content={
            "document_id": document_id,
            "embeddings": embeddings,
            "count": len(embeddings),
            "limit": limit,
            "offset": offset,
            "statistics": {
                "total_embeddings": len(embeddings),
                "by_type": embedding_stats
            }
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get embeddings for document {document_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve embeddings: {str(e)}"
        )


@router.get("/relevancies")
async def get_relevancies(
    document_id: Optional[str] = Query(None, description="Filter by document ID"),
    limit: int = Query(100, ge=1, le=1000, description="Maximum number of relevancies to return"),
    offset: int = Query(0, ge=0, description="Number of relevancies to skip"),
    min_score: float = Query(0.0, ge=0.0, le=1.0, description="Minimum relevance score")
):
    """
    Get chunk-image relevancy relationships for a document.

    Args:
        document_id: Document ID to filter relevancies
        limit: Maximum number of relevancies to return
        offset: Pagination offset
        min_score: Minimum relevance score threshold

    Returns:
        List of chunk-image relationships with relevance scores
    """
    try:
        supabase_client = get_supabase_client()

        if not document_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="document_id is required"
            )

        # Query chunk-image relationships through chunks
        query = supabase_client.client.table('chunk_image_relationships').select(
            '*, document_chunks!inner(document_id, content), document_images(image_url, caption)'
        ).eq('document_chunks.document_id', document_id)

        if min_score > 0:
            query = query.gte('relevance_score', min_score)

        query = query.order('relevance_score', desc=True)
        query = query.range(offset, offset + limit - 1)
        result = query.execute()

        relevancies = result.data if result.data else []

        # Calculate statistics
        relationship_types = {}
        for rel in relevancies:
            rel_type = rel.get('relationship_type', 'unknown')
            if rel_type not in relationship_types:
                relationship_types[rel_type] = 0
            relationship_types[rel_type] += 1

        return JSONResponse(content={
            "document_id": document_id,
            "relevancies": relevancies,
            "count": len(relevancies),
            "limit": limit,
            "offset": offset,
            "statistics": {
                "total_relevancies": len(relevancies),
                "by_relationship_type": relationship_types,
                "min_score_filter": min_score
            }
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get relevancies for document {document_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve relevancies: {str(e)}"
        )


# REMOVED: Duplicate endpoint - use /api/admin/jobs/{job_id}/status instead
# This endpoint was conflicting with admin.py and never being reached due to router registration order


async def create_products_background(
    document_id: str,
    workspace_id: str,
    job_id: str
):
    """
    Background task to create products from chunks with checkpoint support.
    Runs separately to avoid blocking main PDF processing.
    Creates a sub-job for tracking.
    """
    import uuid
    supabase_client = get_supabase_client()
    sub_job_id = str(uuid.uuid4())  # ‚úÖ FIX: Generate proper UUID instead of appending "_products"

    try:
        logger.info(f"üè≠ Starting background product creation for document {document_id}")

        # Create sub-job in database
        try:
            supabase_client.client.table('background_jobs').insert({
                "id": sub_job_id,
                "parent_job_id": job_id,
                "job_type": "product_creation",
                "document_id": document_id,
                "status": "processing",
                "progress": 0,
                "metadata": {
                    "workspace_id": workspace_id,
                    "started_at": datetime.utcnow().isoformat()
                },
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }).execute()
            logger.info(f"‚úÖ Created sub-job {sub_job_id} for product creation")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to create sub-job: {e}")

        product_service = ProductCreationService(supabase_client)

        # Create PRODUCTS_DETECTED checkpoint before detection
        await checkpoint_recovery_service.create_checkpoint(
            job_id=job_id,
            stage=ProcessingStage.PRODUCTS_DETECTED,
            data={
                "document_id": document_id,
                "workspace_id": workspace_id,
                "detection_started": True
            },
            metadata={
                "current_step": "Detecting product candidates",
                "timestamp": datetime.utcnow().isoformat()
            }
        )
        logger.info(f"‚úÖ Created PRODUCTS_DETECTED checkpoint for job {job_id}")

        # Use layout-based product detection
        product_result = await product_service.create_products_from_layout_candidates(
            document_id=document_id,
            workspace_id=workspace_id,
            min_confidence=0.5,
            min_quality_score=0.5
        )

        products_created = product_result.get('products_created', 0)
        logger.info(f"‚úÖ Background product creation completed: {products_created} products created")

        # Update sub-job status to completed
        try:
            supabase_client.client.table('background_jobs').update({
                "status": "completed",
                "progress": 100,
                "metadata": {
                    "workspace_id": workspace_id,
                    "products_created": products_created,
                    "candidates_detected": product_result.get('candidates_detected', 0),
                    "validation_passed": product_result.get('validation_passed', 0),
                    "completed_at": datetime.utcnow().isoformat()
                },
                "updated_at": datetime.utcnow().isoformat()
            }).eq('id', sub_job_id).execute()
            logger.info(f"‚úÖ Marked sub-job {sub_job_id} as completed")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to update sub-job: {e}")

        # Create PRODUCTS_CREATED checkpoint after successful creation
        await checkpoint_recovery_service.create_checkpoint(
            job_id=job_id,
            stage=ProcessingStage.PRODUCTS_CREATED,
            data={
                "document_id": document_id,
                "products_created": products_created,
                "product_ids": product_result.get('product_ids', [])
            },
            metadata={
                "current_step": "Products created successfully",
                "candidates_detected": product_result.get('candidates_detected', 0),
                "validation_passed": product_result.get('validation_passed', 0),
                "timestamp": datetime.utcnow().isoformat()
            }
        )
        logger.info(f"‚úÖ Created PRODUCTS_CREATED checkpoint for job {job_id}")

        # Update job metadata with product count (in-memory)
        if job_id in job_storage:
            job_storage[job_id]["progress"] = 100  # Now fully complete
            if "result" in job_storage[job_id]:
                job_storage[job_id]["result"]["products_created"] = products_created
                job_storage[job_id]["result"]["message"] = f"Document processed successfully: {products_created} products created"

        # ‚úÖ FIX: Persist products_created to database background_jobs table
        try:
            job_recovery_service = JobRecoveryService(supabase_client)

            # Get current job from database
            current_job = await job_recovery_service.get_job_status(job_id)  # ‚úÖ FIX: Use correct method name get_job_status
            if current_job:
                # Update metadata with products_created
                updated_metadata = current_job.get('metadata', {})
                updated_metadata['products_created'] = products_created

                # Persist updated metadata
                await job_recovery_service.persist_job(
                    job_id=job_id,
                    document_id=document_id,
                    filename=current_job.get('filename', 'unknown'),
                    status='completed',
                    progress=100,
                    metadata=updated_metadata
                )
                logger.info(f"‚úÖ Persisted products_created={products_created} to database for job {job_id}")
        except Exception as persist_error:
            logger.error(f"‚ùå Failed to persist products_created to database: {persist_error}")

    except Exception as e:
        logger.error(f"‚ùå Background product creation failed: {e}", exc_info=True)

        # Mark sub-job as failed
        try:
            supabase_client.client.table('background_jobs').update({
                "status": "failed",
                "error": str(e),
                "metadata": {
                    "workspace_id": workspace_id,
                    "error_message": str(e),
                    "failed_at": datetime.utcnow().isoformat()
                },
                "updated_at": datetime.utcnow().isoformat()
            }).eq('id', sub_job_id).execute()
            logger.info(f"‚úÖ Marked sub-job {sub_job_id} as failed")
        except Exception as sub_error:
            logger.warning(f"‚ö†Ô∏è Failed to update sub-job: {sub_error}")

        # Create failed checkpoint
        try:
            await checkpoint_recovery_service.create_checkpoint(
                job_id=job_id,
                stage=ProcessingStage.PRODUCTS_DETECTED,
                data={
                    "document_id": document_id,
                    "error": str(e),
                    "failed": True
                },
                metadata={
                    "current_step": "Product creation failed",
                    "error_message": str(e),
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
        except:
            pass  # Don't fail if checkpoint creation fails


async def process_images_background(
    document_id: str,
    job_id: str,
    images_count: int
):
    """
    Background task to process image AI analysis with checkpoint support.
    Runs separately to avoid blocking main PDF processing.
    Creates a sub-job for tracking.
    """
    import uuid
    supabase_client = get_supabase_client()
    sub_job_id = str(uuid.uuid4())  # ‚úÖ FIX: Generate proper UUID instead of appending "_images"

    try:
        logger.info(f"üñºÔ∏è Starting background image AI analysis for document {document_id}")

        # Create sub-job in database
        try:
            supabase_client.client.table('background_jobs').insert({
                "id": sub_job_id,
                "parent_job_id": job_id,
                "job_type": "image_analysis",
                "document_id": document_id,
                "status": "processing",
                "progress": 0,
                "metadata": {
                    "images_count": images_count,
                    "started_at": datetime.utcnow().isoformat()
                },
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }).execute()
            logger.info(f"‚úÖ Created sub-job {sub_job_id} for image analysis")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to create sub-job: {e}")

        # Run background image processing
        from app.services.background_image_processor import start_background_image_processing
        result = await start_background_image_processing(
            document_id=document_id,
            supabase_client=supabase_client
        )

        images_processed = result.get('total_processed', 0)
        images_failed = result.get('total_failed', 0)
        logger.info(f"‚úÖ Background image analysis completed: {images_processed} processed, {images_failed} failed")

        # Update sub-job status to completed
        try:
            supabase_client.client.table('background_jobs').update({
                "status": "completed",
                "progress": 100,
                "metadata": {
                    "images_count": images_count,
                    "images_processed": images_processed,
                    "images_failed": images_failed,
                    "batches_processed": result.get('batches_processed', 0),
                    "completed_at": datetime.utcnow().isoformat()
                },
                "updated_at": datetime.utcnow().isoformat()
            }).eq('id', sub_job_id).execute()
            logger.info(f"‚úÖ Marked sub-job {sub_job_id} as completed")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to update sub-job: {e}")

        # Create IMAGE_ANALYSIS_COMPLETED checkpoint
        try:
            await checkpoint_recovery_service.create_checkpoint(
                job_id=job_id,
                stage=ProcessingStage.COMPLETED,  # Use COMPLETED since this is the final stage
                data={
                    "document_id": document_id,
                    "images_processed": images_processed,
                    "images_failed": images_failed
                },
                metadata={
                    "current_step": "Image AI analysis completed",
                    "images_count": images_count,
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
            logger.info(f"‚úÖ Created IMAGE_ANALYSIS_COMPLETED checkpoint for job {job_id}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to create checkpoint: {e}")

    except Exception as e:
        logger.error(f"‚ùå Background image analysis failed: {e}", exc_info=True)

        # Mark sub-job as failed
        try:
            supabase_client.client.table('background_jobs').update({
                "status": "failed",
                "error": str(e),
                "metadata": {
                    "images_count": images_count,
                    "error_message": str(e),
                    "failed_at": datetime.utcnow().isoformat()
                },
                "updated_at": datetime.utcnow().isoformat()
            }).eq('id', sub_job_id).execute()
            logger.info(f"‚úÖ Marked sub-job {sub_job_id} as failed")
        except Exception as sub_error:
            logger.warning(f"‚ö†Ô∏è Failed to update sub-job: {sub_error}")


async def process_document_background(
    job_id: str,
    document_id: str,
    file_content: bytes,
    filename: str,
    title: Optional[str],
    description: Optional[str],
    document_tags: List[str],
    chunk_size: int,
    chunk_overlap: int,
    llamaindex_service: Optional[LlamaIndexService] = None,
    file_url: Optional[str] = None
):
    """
    Background task to process document with checkpoint recovery support.
    """
    start_time = datetime.utcnow()

    logger.info("=" * 80)
    logger.info(f"üöÄ [BACKGROUND TASK] STARTING PROCESS_DOCUMENT_BACKGROUND")
    logger.info("=" * 80)
    logger.info(f"üìã Job ID: {job_id}")
    logger.info(f"üìÑ Document ID: {document_id}")
    logger.info(f"üìù Filename: {filename}")
    logger.info(f"üì¶ File size: {len(file_content)} bytes")
    logger.info(f"‚è∞ Started at: {start_time.isoformat()}")
    logger.info(f"üîß Chunk size: {chunk_size}, Overlap: {chunk_overlap}")
    logger.info(f"üìö Title: {title}")
    logger.info(f"üìù Description: {description}")
    logger.info(f"üè∑Ô∏è  Tags: {document_tags}")
    logger.info("=" * 80)

    # Get LlamaIndex service from app state if not provided
    logger.info("üîß [STEP 1] Getting LlamaIndex service...")
    if llamaindex_service is None:
        try:
            from app.main import app
            if hasattr(app.state, 'llamaindex_service'):
                llamaindex_service = app.state.llamaindex_service
                logger.info("‚úÖ [STEP 1] Retrieved LlamaIndex service from app state")
            else:
                logger.error("‚ùå [STEP 1] LlamaIndex service not available in app state")
                job_storage[job_id]["status"] = "failed"
                job_storage[job_id]["error"] = "LlamaIndex service not available"
                return
        except Exception as e:
            logger.error(f"‚ùå [STEP 1] Failed to get LlamaIndex service: {e}")
            job_storage[job_id]["status"] = "failed"
            job_storage[job_id]["error"] = str(e)
            return
    else:
        logger.info("‚úÖ [STEP 1] LlamaIndex service provided as parameter")

    # Check for existing checkpoint to resume from
    logger.info("üîç [STEP 2] Checking for existing checkpoints...")
    last_checkpoint = None
    resume_from_stage = None
    try:
        last_checkpoint = await checkpoint_recovery_service.get_last_checkpoint(job_id)
        if last_checkpoint:
            resume_from_stage_str = last_checkpoint.get('stage')
            resume_from_stage = ProcessingStage(resume_from_stage_str)
            logger.info(f"üîÑ [STEP 2] RESUMING FROM CHECKPOINT: {resume_from_stage.value}")
            logger.info(f"   üìÖ Checkpoint created at: {last_checkpoint.get('created_at')}")
            logger.info(f"   üì¶ Checkpoint data keys: {list(last_checkpoint.get('checkpoint_data', {}).keys())}")

            # Verify checkpoint data exists in database
            logger.info(f"üîç [STEP 2] Verifying checkpoint data for stage {resume_from_stage.value}...")
            can_resume = await checkpoint_recovery_service.verify_checkpoint_data(job_id, resume_from_stage)
            if not can_resume:
                logger.warning(f"‚ö†Ô∏è [STEP 2] Checkpoint data verification failed, starting from scratch")
                resume_from_stage = None
                last_checkpoint = None
            else:
                logger.info(f"‚úÖ [STEP 2] Checkpoint data verified successfully")
        else:
            logger.info("üìù [STEP 2] No checkpoint found, starting fresh processing")
    except Exception as e:
        logger.error(f"‚ùå [STEP 2] Failed to check for checkpoint: {e}", exc_info=True)
        resume_from_stage = None

    try:
        # Create placeholder document record FIRST (required for foreign key constraint)
        supabase_client = get_supabase_client()

        # Skip document creation if resuming from checkpoint
        if not resume_from_stage:
            try:
                # Generate file_url if not provided
                if not file_url:
                    file_url = f"https://bgbavxtjlbvgplozizxu.supabase.co/storage/v1/object/public/pdf-documents/{document_id}/{filename}"

                supabase_client.client.table('documents').insert({
                    "id": document_id,
                    "workspace_id": "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
                    "filename": filename,
                    "content_type": "application/pdf",
                    "file_size": len(file_content),
                    "file_url": file_url,
                    "processing_status": "processing",
                    "metadata": {
                        "title": title or filename,
                        "description": description,
                        "tags": document_tags,
                        "source": "rag_upload_async"
                    },
                    "created_at": start_time.isoformat(),
                    "updated_at": start_time.isoformat()
                }).execute()
                logger.info(f"‚úÖ Created placeholder document record: {document_id}")
                logger.info(f"   File URL: {file_url}")

                # Also create processed_documents record (required for ai_analysis_queue foreign key)
                try:
                    supabase_client.client.table('processed_documents').upsert({
                        "id": document_id,  # Use same ID as documents table
                        "workspace_id": "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
                        "pdf_document_id": document_id,
                        "content": "",  # Will be populated during processing
                        "processing_status": "processing",
                        "processing_started_at": start_time.isoformat(),
                        "metadata": {},
                        "created_at": start_time.isoformat(),
                        "updated_at": start_time.isoformat()
                    }).execute()
                    logger.info(f"‚úÖ Created processed_documents record: {document_id}")
                except Exception as proc_doc_error:
                    logger.error(f"Failed to create processed_documents record: {proc_doc_error}")
                    # Continue anyway

            except Exception as doc_error:
                logger.error(f"Failed to create document record: {doc_error}")
                # Continue anyway - the document might already exist

            # Create INITIALIZED checkpoint
            await checkpoint_recovery_service.create_checkpoint(
                job_id=job_id,
                stage=ProcessingStage.INITIALIZED,
                data={
                    "document_id": document_id,
                    "filename": filename,
                    "file_size": len(file_content)
                },
                metadata={
                    "title": title or filename,
                    "description": description,
                    "tags": document_tags
                }
            )
            logger.info(f"‚úÖ Created INITIALIZED checkpoint for job {job_id}")

        # Update status (in-memory)
        job_storage[job_id]["status"] = "processing"
        job_storage[job_id]["started_at"] = start_time.isoformat()
        job_storage[job_id]["document_id"] = document_id
        job_storage[job_id]["progress"] = 10

        # Persist status change to database (now document exists)
        if job_recovery_service:
            await job_recovery_service.persist_job(
                job_id=job_id,
                document_id=document_id,
                filename=filename,
                status="processing",
                progress=10
            )

        # Initialize AI model tracker for this job
        ai_tracker = AIModelTracker(job_id)
        job_storage[job_id]["ai_tracker"] = ai_tracker

        # Define progress callback to update job progress with detailed metadata
        async def update_progress(progress: int, details: dict = None):
            """Update job progress in memory and database with detailed stats and AI tracking"""
            job_storage[job_id]["progress"] = progress

            # Build detailed metadata
            detailed_metadata = {
                "document_id": document_id,  # ‚úÖ FIX 1: Add document_id
                "filename": filename,
                "workspace_id": "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
                "title": title or filename,
                "description": description,
                "tags": document_tags,
                "source": "rag_upload_async"
            }

            # ‚úÖ FIX 2: Add detailed progress stats
            if details:
                detailed_metadata.update({
                    "current_page": details.get("current_page"),
                    "total_pages": details.get("total_pages"),
                    "chunks_created": details.get("chunks_created", 0),
                    "images_extracted": details.get("images_extracted", 0),
                    "products_created": details.get("products_created", 0),
                    "ai_usage": {
                        "llama_calls": details.get("llama_calls", 0),
                        "claude_calls": details.get("claude_calls", 0),
                        "openai_calls": details.get("openai_calls", 0),
                        "clip_embeddings": details.get("clip_embeddings", 0)
                    },
                    "embeddings_generated": {
                        "text": details.get("text_embeddings", 0),
                        "visual": details.get("visual_embeddings", 0),
                        "color": details.get("color_embeddings", 0),
                        "texture": details.get("texture_embeddings", 0),
                        "application": details.get("application_embeddings", 0)
                    },
                    "current_step": details.get("current_step", "Processing")
                })

                # ‚úÖ Log AI model calls if provided
                if details.get("ai_model_call"):
                    ai_call = details["ai_model_call"]
                    ai_tracker.log_model_call(
                        model_name=ai_call.get("model_name", "Unknown"),
                        stage=ai_call.get("stage", "unknown"),
                        task=ai_call.get("task", "unknown"),
                        latency_ms=ai_call.get("latency_ms", 0),
                        confidence_score=ai_call.get("confidence_score"),
                        result_summary=ai_call.get("result_summary"),
                        items_processed=ai_call.get("items_processed", 0),
                        input_tokens=ai_call.get("input_tokens"),
                        output_tokens=ai_call.get("output_tokens"),
                        success=ai_call.get("success", True),
                        error=ai_call.get("error")
                    )

            # Add AI tracker summary to metadata
            detailed_metadata["ai_tracking"] = ai_tracker.format_for_metadata()

            # ‚úÖ FIX: Store metadata in job_storage so it's returned by get_job_status
            job_storage[job_id]["metadata"] = detailed_metadata

            if job_recovery_service:
                await job_recovery_service.persist_job(
                    job_id=job_id,
                    document_id=document_id,
                    filename=filename,
                    status="processing",
                    progress=progress,
                    metadata=detailed_metadata
                )
            logger.info(f"üìä Job {job_id} progress: {progress}% - {detailed_metadata.get('current_step', 'Processing')}")



        # Process document through LlamaIndex service with progress tracking and granular checkpoints
        # Skip if resuming from a later checkpoint
        if not resume_from_stage or resume_from_stage == ProcessingStage.INITIALIZED:
            # Enhanced progress callback that creates checkpoints at each stage
            async def enhanced_progress_callback(progress: int, details: dict = None):
                """Enhanced progress callback with checkpoint creation"""
                await update_progress(progress, details)

                # Create checkpoints at specific progress milestones
                current_step = details.get('current_step', '') if details else ''

                # PDF_EXTRACTED checkpoint (after PDF extraction - 20%)
                if progress == 20 and 'Extracting text and images' in current_step:
                    await checkpoint_recovery_service.create_checkpoint(
                        job_id=job_id,
                        stage=ProcessingStage.PDF_EXTRACTED,
                        data={
                            "document_id": document_id,
                            "total_pages": details.get('total_pages', 0) if details else 0
                        },
                        metadata={
                            "current_step": current_step,
                            "timestamp": datetime.utcnow().isoformat()
                        }
                    )
                    logger.info(f"‚úÖ Created PDF_EXTRACTED checkpoint for job {job_id}")

                # CHUNKS_CREATED checkpoint (after chunking - 40%)
                elif progress == 40 and 'Creating semantic chunks' in current_step:
                    await checkpoint_recovery_service.create_checkpoint(
                        job_id=job_id,
                        stage=ProcessingStage.CHUNKS_CREATED,
                        data={
                            "document_id": document_id,
                            "total_pages": details.get('total_pages', 0) if details else 0,
                            "images_extracted": details.get('images_extracted', 0) if details else 0
                        },
                        metadata={
                            "current_step": current_step,
                            "timestamp": datetime.utcnow().isoformat()
                        }
                    )
                    logger.info(f"‚úÖ Created CHUNKS_CREATED checkpoint for job {job_id}")

                # TEXT_EMBEDDINGS_GENERATED checkpoint (after embeddings - 60%)
                elif progress == 60 and 'Generating embeddings' in current_step:
                    await checkpoint_recovery_service.create_checkpoint(
                        job_id=job_id,
                        stage=ProcessingStage.TEXT_EMBEDDINGS_GENERATED,
                        data={
                            "document_id": document_id,
                            "chunks_created": details.get('chunks_created', 0) if details else 0,
                            "total_pages": details.get('total_pages', 0) if details else 0,
                            "images_extracted": details.get('images_extracted', 0) if details else 0
                        },
                        metadata={
                            "current_step": current_step,
                            "text_embeddings": details.get('text_embeddings', 0) if details else 0,
                            "openai_calls": details.get('openai_calls', 0) if details else 0,
                            "timestamp": datetime.utcnow().isoformat()
                        }
                    )
                    logger.info(f"‚úÖ Created TEXT_EMBEDDINGS_GENERATED checkpoint for job {job_id}")

                # IMAGES_EXTRACTED checkpoint (after image processing - 80%)
                elif progress == 80 and 'Processing images' in current_step:
                    await checkpoint_recovery_service.create_checkpoint(
                        job_id=job_id,
                        stage=ProcessingStage.IMAGES_EXTRACTED,
                        data={
                            "document_id": document_id,
                            "chunks_created": details.get('chunks_created', 0) if details else 0,
                            "images_extracted": details.get('images_extracted', 0) if details else 0
                        },
                        metadata={
                            "current_step": current_step,
                            "timestamp": datetime.utcnow().isoformat()
                        }
                    )
                    logger.info(f"‚úÖ Created IMAGES_EXTRACTED checkpoint for job {job_id}")

            logger.info("=" * 80)
            logger.info("üöÄ [STEP 4] CALLING LLAMAINDEX SERVICE index_document_content")
            logger.info("=" * 80)
            logger.info(f"üìÑ File content size: {len(file_content)} bytes")
            logger.info(f"üìù Document ID: {document_id}")
            logger.info(f"üìÇ Filename: {filename}")
            logger.info(f"üîß Chunk size: {chunk_size}, Overlap: {chunk_overlap}")
            logger.info("=" * 80)

            processing_result = await llamaindex_service.index_document_content(
                file_content=file_content,
                document_id=document_id,
                file_path=filename,
                metadata={
                    "workspace_id": "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",  # Default workspace UUID
                    "filename": filename,
                    "title": title or filename,
                    "description": description,
                    "tags": document_tags,
                    "source": "rag_upload_async"
                },
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                progress_callback=enhanced_progress_callback
            )

            logger.info("=" * 80)
            logger.info("‚úÖ [STEP 4] LLAMAINDEX SERVICE COMPLETED")
            logger.info("=" * 80)

            # Create IMAGE_EMBEDDINGS_GENERATED checkpoint after CLIP embeddings
            chunks_created = processing_result.get('statistics', {}).get('total_chunks', 0)
            images_extracted = processing_result.get('statistics', {}).get('images_extracted', 0)
            clip_embeddings = processing_result.get('statistics', {}).get('clip_embeddings_generated', 0)

            if clip_embeddings > 0:
                await checkpoint_recovery_service.create_checkpoint(
                    job_id=job_id,
                    stage=ProcessingStage.IMAGE_EMBEDDINGS_GENERATED,
                    data={
                        "document_id": document_id,
                        "chunks_created": chunks_created,
                        "images_extracted": images_extracted,
                        "clip_embeddings": clip_embeddings
                    },
                    metadata={
                        "current_step": "CLIP embeddings generated",
                        "timestamp": datetime.utcnow().isoformat()
                    }
                )
                logger.info(f"‚úÖ Created IMAGE_EMBEDDINGS_GENERATED checkpoint for job {job_id}")

            # Log the processing result status for debugging
            result_status = processing_result.get('status', 'unknown')
            logger.info(f"üìä Processing result status: {result_status}")
            logger.info(f"üìä Processing result keys: {list(processing_result.keys())}")
            logger.info(f"üìä Statistics: {processing_result.get('statistics', {})}")

            if result_status == 'success':
                # Create COMPLETED checkpoint with all processing data
                await checkpoint_recovery_service.create_checkpoint(
                    job_id=job_id,
                    stage=ProcessingStage.COMPLETED,
                    data={
                        "document_id": document_id,
                        "chunks_created": chunks_created,
                        "images_extracted": images_extracted,
                        "embeddings_generated": processing_result.get('statistics', {}).get('database_embeddings_stored', 0),
                        "clip_embeddings": clip_embeddings
                    },
                    metadata={
                        "processing_time": (datetime.utcnow() - start_time).total_seconds(),
                        "statistics": processing_result.get('statistics', {})
                    }
                )
                logger.info(f"‚úÖ Created COMPLETED checkpoint for job {job_id}")
            else:
                # Log why we're not creating COMPLETED checkpoint
                logger.error(f"‚ùå Processing did NOT return success status!")
                logger.error(f"   Status: {result_status}")
                logger.error(f"   Full result: {processing_result}")
                # Raise an exception so it gets caught and logged properly
                raise RuntimeError(f"Document processing returned status '{result_status}' instead of 'success'")
        else:
            # Resuming from checkpoint - retrieve data from last checkpoint
            logger.info(f"‚è≠Ô∏è Skipping document processing - resuming from {resume_from_stage}")
            checkpoint_data = last_checkpoint.get('checkpoint_data', {})
            processing_result = {
                'status': 'success',
                'statistics': {
                    'total_chunks': checkpoint_data.get('chunks_created', 0),
                    'images_extracted': checkpoint_data.get('images_extracted', 0),
                    'database_embeddings_stored': checkpoint_data.get('embeddings_generated', 0),
                    'clip_embeddings_generated': checkpoint_data.get('clip_embeddings', 0)
                }
            }

        processing_time = (datetime.utcnow() - start_time).total_seconds()

        # Check if processing succeeded
        result_status = processing_result.get('status', 'completed')
        chunks_created = processing_result.get('statistics', {}).get('total_chunks', 0)

        if result_status == 'error':
            error_message = processing_result.get('error', 'Unknown error during document processing')
            job_storage[job_id]["status"] = "failed"
            job_storage[job_id]["error"] = error_message
            job_storage[job_id]["progress"] = 100
        else:
            # ‚úÖ FIX 3: Run product creation in background to prevent timeout
            # Mark main job as 90% complete, product creation runs separately
            products_created = 0

            if chunks_created > 0:
                logger.info(f"üè≠ Scheduling background product creation for document {document_id}")
                # Start product creation in background (don't await)
                # Note: asyncio is already imported at the top of the file
                asyncio.create_task(create_products_background(
                    document_id=document_id,
                    workspace_id="ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
                    job_id=job_id
                ))
                logger.info("‚úÖ Product creation scheduled in background")
            else:
                logger.info("‚ö†Ô∏è No chunks created, skipping product creation")

            # ‚úÖ FIX 4: Start background image processing with sub-job tracking
            # Get images_extracted from database
            images_extracted_result = supabase.client.table("document_images").select("id", count="exact").eq("document_id", document_id).execute()
            images_extracted = images_extracted_result.count if images_extracted_result else 0
            if images_extracted > 0:
                logger.info(f"üñºÔ∏è Scheduling background image AI analysis for {images_extracted} images")
                asyncio.create_task(process_images_background(
                    document_id=document_id,
                    job_id=job_id,
                    images_count=images_extracted
                ))
                logger.info("‚úÖ Image AI analysis scheduled in background")
            else:
                logger.info("‚ö†Ô∏è No images extracted, skipping background image analysis")

            job_storage[job_id]["status"] = "completed"
            job_storage[job_id]["progress"] = 90  # Main processing complete, products/images running in background
            job_storage[job_id]["completed_at"] = datetime.utcnow().isoformat()
            job_storage[job_id]["result"] = {
                "document_id": document_id,
                "title": title or filename,
                "status": result_status,
                "chunks_created": chunks_created,
                "embeddings_generated": chunks_created > 0,
                "products_created": products_created,
                "images_extracted": images_extracted,  # Include images count
                "processing_time": processing_time,
                "message": f"Document processed successfully: {chunks_created} chunks created, {products_created} products created, {images_extracted} images extracted"
            }

            # Persist completion to database
            if job_recovery_service:
                await job_recovery_service.persist_job(
                    job_id=job_id,
                    document_id=document_id,
                    filename=filename,
                    status="completed",
                    progress=100,
                    metadata={
                        "chunks_created": chunks_created,
                        "products_created": products_created,
                        "images_extracted": images_extracted,  # Include images count
                        "processing_time": processing_time
                    }
                )

    except asyncio.CancelledError:
        # Job was cancelled (likely due to service shutdown)
        logger.error(f"üõë JOB INTERRUPTED: {job_id}")
        logger.error(f"   Document ID: {document_id}")
        logger.error(f"   Filename: {filename}")
        logger.error(f"   Reason: Service shutdown or task cancellation")
        logger.error(f"   Duration before interruption: {(datetime.utcnow() - start_time).total_seconds():.2f}s")

        job_storage[job_id]["status"] = "interrupted"
        job_storage[job_id]["error"] = "Job interrupted due to service shutdown"
        job_storage[job_id]["progress"] = job_storage[job_id].get("progress", 0)
        job_storage[job_id]["interrupted_at"] = datetime.utcnow().isoformat()

        # Persist interruption to database
        if job_recovery_service:
            await job_recovery_service.mark_job_interrupted(
                job_id=job_id,
                reason="Service shutdown or task cancellation"
            )

        # Re-raise to allow proper cleanup
        raise

    except Exception as e:
        logger.error(f"‚ùå BACKGROUND JOB FAILED: {job_id}")
        logger.error(f"   Document ID: {document_id}")
        logger.error(f"   Error: {e}", exc_info=True)
        logger.error(f"   Duration before failure: {(datetime.utcnow() - start_time).total_seconds():.2f}s")

        job_storage[job_id]["status"] = "failed"
        job_storage[job_id]["error"] = str(e)
        job_storage[job_id]["progress"] = 100
        job_storage[job_id]["failed_at"] = datetime.utcnow().isoformat()

        # Persist failure to database
        if job_recovery_service:
            await job_recovery_service.persist_job(
                job_id=job_id,
                document_id=document_id,
                filename=filename,
                status="failed",
                progress=100,
                error=str(e)
            )

    finally:
        end_time = datetime.utcnow()
        total_duration = (end_time - start_time).total_seconds()
        final_status = job_storage[job_id].get("status", "unknown")

        logger.info(f"üìã BACKGROUND JOB FINISHED: {job_id}")
        logger.info(f"   Final status: {final_status}")
        logger.info(f"   Total duration: {total_duration:.2f}s")
        logger.info(f"   Ended at: {end_time.isoformat()}")


async def process_document_with_discovery(
    job_id: str,
    document_id: str,
    file_content: bytes,
    filename: str,
    title: Optional[str],
    description: Optional[str],
    document_tags: List[str],
    discovery_model: str,
    focused_extraction: bool,
    extract_categories: List[str],
    chunk_size: int,
    chunk_overlap: int,
    workspace_id: str = "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
    agent_prompt: Optional[str] = None,
    enable_prompt_enhancement: bool = True
):
    """
    Background task to process document with intelligent product discovery.

    NEW ARCHITECTURE:
    Stage 0: Product Discovery (0-15%) - Analyze PDF with Claude/GPT, classify content by category
    Stage 1: Focused Extraction (15-30%) - Extract pages based on extract_categories
    Stage 2: Chunking (30-50%) - Create chunks for extracted content
    Stage 3: Image Processing (50-70%) - Extract images from specified categories only
    Stage 4: Product Creation (70-90%) - Create products from discovery
    Stage 5: Quality Enhancement (90-100%) - Claude validation (async)

    Args:
        focused_extraction: If True (default), only process pages/images from extract_categories.
                          If False, process entire PDF.
        extract_categories: List of categories to extract (e.g., ['products'], ['certificates', 'logos']).
                          Categories: 'products', 'certificates', 'logos', 'specifications', 'all'
    """
    start_time = datetime.utcnow()

    # Initialize lazy loading for this job
    from app.services.lazy_loader import get_component_manager
    component_manager = get_component_manager()

    # Track which components are loaded for cleanup
    loaded_components = []

    logger.info("=" * 80)
    logger.info(f"üîç [PRODUCT DISCOVERY] STARTING")
    logger.info("=" * 80)
    logger.info(f"üìã Job ID: {job_id}")
    logger.info(f"üìÑ Document ID: {document_id}")
    logger.info(f"ü§ñ Discovery Model: {discovery_model.upper()}")
    logger.info(f"üéØ Focused Extraction: {'ENABLED' if focused_extraction else 'DISABLED (Full PDF)'}")
    logger.info(f"üì¶ Extract Categories: {', '.join(extract_categories).upper()}")
    logger.info("=" * 80)

    try:
        # Initialize Progress Tracker
        from app.services.progress_tracker import ProgressTracker
        from app.schemas.jobs import ProcessingStage
        from app.services.checkpoint_recovery_service import checkpoint_recovery_service, ProcessingStage as CheckpointStage

        tracker = ProgressTracker(
            job_id=job_id,
            document_id=document_id,
            total_pages=0,  # Will update after PDF extraction
            job_storage=job_storage
        )
        await tracker.start_processing()

        # ü´Ä Start heartbeat monitoring (30s interval, 2min crash detection)
        await tracker.start_heartbeat(interval_seconds=30)

        # Create INITIALIZED checkpoint
        await checkpoint_recovery_service.create_checkpoint(
            job_id=job_id,
            stage=CheckpointStage.INITIALIZED,
            data={
                "document_id": document_id,
                "filename": filename,
                "file_size": len(file_content)
            },
            metadata={
                "title": title or filename,
                "description": description,
                "tags": document_tags,
                "discovery_model": discovery_model,
                "focused_extraction": focused_extraction
            }
        )
        logger.info(f"‚úÖ Created INITIALIZED checkpoint for job {job_id}")

        # Stage 0: Product Discovery (0-15%)
        logger.info("üîç [STAGE 0] Product Discovery - Starting...")
        await tracker.update_stage(ProcessingStage.INITIALIZING, stage_name="product_discovery")

        from app.services.product_discovery_service import ProductDiscoveryService
        from app.services.pdf_processor import PDFProcessor
        import tempfile
        import aiofiles

        # Save PDF to temporary file for two-stage discovery
        # This allows ProductDiscoveryService to extract specific page ranges
        temp_pdf_path = None
        try:
            # Create temp file that persists during processing
            temp_fd, temp_pdf_path = tempfile.mkstemp(suffix='.pdf', prefix=f'{document_id}_')
            os.close(temp_fd)  # Close file descriptor, we'll write with aiofiles

            # Write PDF bytes to temp file
            async with aiofiles.open(temp_pdf_path, 'wb') as f:
                await f.write(file_content)

            logger.info(f"üìÅ Saved PDF to temp file: {temp_pdf_path}")

            # üöÄ PROGRESSIVE TIMEOUT: Calculate timeout based on document size
            file_size_mb = len(file_content) / (1024 * 1024)

            # Quick page count check (fast, doesn't extract content)
            import fitz
            quick_doc = fitz.open(temp_pdf_path)
            page_count = len(quick_doc)
            quick_doc.close()

            # Calculate progressive timeout for PDF extraction
            pdf_extraction_timeout = ProgressiveTimeoutStrategy.calculate_pdf_extraction_timeout(
                page_count=page_count,
                file_size_mb=file_size_mb
            )
            logger.info(f"üìä Document: {page_count} pages, {file_size_mb:.1f} MB ‚Üí timeout: {pdf_extraction_timeout:.0f}s")

            # Extract PDF text first (with progressive timeout guard)
            pdf_processor = PDFProcessor()
            pdf_result = await with_timeout(
                pdf_processor.process_pdf_from_bytes(
                    pdf_bytes=file_content,
                    document_id=document_id,
                    processing_options={'extract_images': False, 'extract_tables': False}
                ),
                timeout_seconds=pdf_extraction_timeout,
                operation_name="PDF text extraction"
            )

            # Create processed_documents record IMMEDIATELY (required for job_progress foreign key)
            # This MUST happen BEFORE any _sync_to_database() calls
            supabase = get_supabase_client()
            try:
                supabase.client.table('processed_documents').upsert({
                    "id": document_id,
                    "workspace_id": "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
                    "pdf_document_id": document_id,
                    "content": pdf_result.markdown_content or "",
                    "processing_status": "processing",
                    "metadata": {
                        "filename": filename,
                        "file_size": len(file_content),
                        "page_count": pdf_result.page_count
                    }
                }).execute()
                logger.info(f"‚úÖ Created processed_documents record for {document_id}")
            except Exception as e:
                logger.error(f"‚ùå CRITICAL: Failed to create processed_documents record: {e}")
                raise  # Don't continue if this fails

            # Update tracker with total pages
            tracker.total_pages = pdf_result.page_count
            for page_num in range(1, pdf_result.page_count + 1):
                from app.schemas.jobs import PageProcessingStatus
                tracker.page_statuses[page_num] = PageProcessingStatus(
                    page_number=page_num,
                    stage=ProcessingStage.INITIALIZING,  # Changed from PENDING (doesn't exist)
                    status="pending"
                )

            # Run TWO-STAGE category-based discovery with prompt enhancement (with progressive timeout)
            # Stage 0A: Index scan (first 50-100 pages)
            # Stage 0B: Focused extraction (specific pages per product)

            # üöÄ PROGRESSIVE TIMEOUT: Calculate timeout based on pages and categories
            discovery_timeout = ProgressiveTimeoutStrategy.calculate_product_discovery_timeout(
                page_count=pdf_result.page_count,
                categories=extract_categories
            )
            logger.info(f"üìä Product discovery: {pdf_result.page_count} pages, {len(extract_categories)} categories ‚Üí timeout: {discovery_timeout:.0f}s")

            discovery_service = ProductDiscoveryService(model=discovery_model)
            catalog = await with_timeout(
                discovery_service.discover_products(
                    pdf_content=file_content,
                    pdf_text=pdf_result.markdown_content,
                    total_pages=pdf_result.page_count,
                    categories=extract_categories,
                    agent_prompt=agent_prompt,  # From unified_upload request
                    workspace_id=workspace_id,
                    enable_prompt_enhancement=enable_prompt_enhancement,
                    job_id=job_id,
                    pdf_path=temp_pdf_path  # ‚úÖ NEW: Enable two-stage discovery
                ),
                timeout_seconds=discovery_timeout,
                operation_name="Product discovery (Stage 0A + 0B)"
            )

        finally:
            # Clean up temp PDF file after discovery
            if temp_pdf_path and os.path.exists(temp_pdf_path):
                try:
                    os.unlink(temp_pdf_path)
                    logger.info(f"üóëÔ∏è Cleaned up temp PDF file: {temp_pdf_path}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to clean up temp PDF: {e}")

        logger.info(f"‚úÖ [STAGE 0] Discovery Complete:")
        logger.info(f"   Categories: {', '.join(extract_categories)}")
        logger.info(f"   Products: {len(catalog.products)}")
        if "certificates" in extract_categories:
            logger.info(f"   Certificates: {len(catalog.certificates)}")
        if "logos" in extract_categories:
            logger.info(f"   Logos: {len(catalog.logos)}")
        if "specifications" in extract_categories:
            logger.info(f"   Specifications: {len(catalog.specifications)}")
        logger.info(f"   Confidence: {catalog.confidence_score:.2f}")

        # Update tracker
        tracker.products_created = len(catalog.products)
        await tracker._sync_to_database(stage="product_discovery")

        # Create PRODUCTS_DETECTED checkpoint (now includes all categories)
        checkpoint_data = {
            "document_id": document_id,
            "categories": extract_categories,
            "products_detected": len(catalog.products),
            "product_names": [p.name for p in catalog.products],
            "total_pages": pdf_result.page_count
        }

        # Add other categories if discovered
        if "certificates" in extract_categories:
            checkpoint_data["certificates_detected"] = len(catalog.certificates)
            checkpoint_data["certificate_names"] = [c.name for c in catalog.certificates]
        if "logos" in extract_categories:
            checkpoint_data["logos_detected"] = len(catalog.logos)
            checkpoint_data["logo_names"] = [l.name for l in catalog.logos]
        if "specifications" in extract_categories:
            checkpoint_data["specifications_detected"] = len(catalog.specifications)
            checkpoint_data["specification_names"] = [s.name for s in catalog.specifications]

        await checkpoint_recovery_service.create_checkpoint(
            job_id=job_id,
            stage=CheckpointStage.PRODUCTS_DETECTED,
            data=checkpoint_data,
            metadata={
                "confidence_score": catalog.confidence_score,
                "discovery_model": discovery_model
            }
        )
        logger.info(f"‚úÖ Created PRODUCTS_DETECTED checkpoint for job {job_id}")

        # Stage 1: Focused Extraction (15-30%)
        logger.info("üéØ [STAGE 1] Focused Extraction - Starting...")
        await tracker.update_stage(ProcessingStage.EXTRACTING_TEXT, stage_name="focused_extraction")

        product_pages = set()
        if focused_extraction:
            logger.info(f"   ENABLED - Processing ONLY pages with {len(catalog.products)} products")
            invalid_pages_found = []
            for product in catalog.products:
                # ‚úÖ CRITICAL FIX: Validate page numbers against PDF page count before adding
                # Claude can hallucinate pages that don't exist (e.g., page 73 in a 71-page PDF)
                valid_pages = [p for p in product.page_range if 1 <= p <= pdf_result.page_count]
                invalid_pages = [p for p in product.page_range if p < 1 or p > pdf_result.page_count]

                if invalid_pages:
                    invalid_pages_found.extend(invalid_pages)
                    logger.warning(f"   ‚ö†Ô∏è Product '{product.name}' has invalid pages {invalid_pages} (PDF has {pdf_result.page_count} pages) - skipping these pages")

                product_pages.update(valid_pages)

            if invalid_pages_found:
                logger.warning(f"   ‚ö†Ô∏è Total invalid pages skipped: {sorted(set(invalid_pages_found))}")

            pages_to_skip = set(range(1, pdf_result.page_count + 1)) - product_pages
            for page_num in pages_to_skip:
                tracker.skip_page_processing(page_num, "Not a product page (focused extraction)")

            logger.info(f"   Product pages: {sorted(product_pages)}")
            logger.info(f"   Processing: {len(product_pages)} / {pdf_result.page_count} pages")
        else:
            logger.info(f"   DISABLED - Processing ALL {pdf_result.page_count} pages")
            product_pages = set(range(1, pdf_result.page_count + 1))

        await tracker._sync_to_database(stage="focused_extraction")

        # Stage 2: Chunking (30-50%)
        logger.info("üìù [STAGE 2] Chunking - Starting...")
        await tracker.update_stage(ProcessingStage.SAVING_TO_DATABASE, stage_name="chunking")

        from app.services.llamaindex_service import LlamaIndexService
        llamaindex_service = LlamaIndexService()

        # Create document in database
        doc_data = {
            'id': document_id,
            'workspace_id': "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",  # Default workspace
            'filename': filename,
            'content_type': 'application/pdf',
            'file_size': len(file_content),
            'file_path': f'pdf-documents/{document_id}/{filename}',
            'processing_status': 'processing',
            'metadata': {
                'title': title or filename,
                'description': description,
                'page_count': pdf_result.page_count,
                'tags': document_tags,
                'products_discovered': len(catalog.products),
                'product_names': [p.name for p in catalog.products],
                'focused_extraction': focused_extraction,
                'discovery_model': discovery_model
            }
        }
        supabase.client.table('documents').upsert(doc_data).execute()
        logger.info(f"‚úÖ Created documents table record for {document_id}")

        # Process chunks using LlamaIndex (with progressive timeout)
        logger.info(f"üìù Calling index_pdf_content with {len(file_content)} bytes, product_pages={sorted(product_pages)}")
        logger.info(f"üìù LlamaIndex service available: {llamaindex_service.available}")

        # üöÄ PROGRESSIVE TIMEOUT: Calculate timeout based on page count
        chunking_timeout = ProgressiveTimeoutStrategy.calculate_chunking_timeout(
            page_count=pdf_result.page_count,
            chunk_size=chunk_size
        )
        logger.info(f"üìä Chunking: {pdf_result.page_count} pages, chunk_size={chunk_size} ‚Üí timeout: {chunking_timeout:.0f}s")

        chunk_result = await with_timeout(
            llamaindex_service.index_pdf_content(
                pdf_content=file_content,
                document_id=document_id,
                metadata={
                    'filename': filename,
                    'title': title,
                    'page_count': pdf_result.page_count,
                    'product_pages': sorted(product_pages),
                    'chunk_size': chunk_size,
                    'chunk_overlap': chunk_overlap,
                    'workspace_id': workspace_id  # ‚úÖ FIX: Pass workspace_id to chunks
                }
            ),
            timeout_seconds=chunking_timeout,
            operation_name="Chunking operation"
        )

        logger.info(f"üìù index_pdf_content returned: {chunk_result}")

        tracker.chunks_created = chunk_result.get('chunks_created', 0)
        # NOTE: Don't pass chunks_created to update_database_stats because it increments!
        # We already set tracker.chunks_created directly above.
        await tracker.update_database_stats(
            kb_entries=tracker.chunks_created,
            sync_to_db=True
        )

        logger.info(f"‚úÖ [STAGE 2] Chunking Complete: {tracker.chunks_created} chunks created")

        # Create CHUNKS_CREATED checkpoint
        await checkpoint_recovery_service.create_checkpoint(
            job_id=job_id,
            stage=CheckpointStage.CHUNKS_CREATED,
            data={
                "document_id": document_id,
                "chunks_created": tracker.chunks_created,
                "chunk_ids": chunk_result.get('chunk_ids', [])
            },
            metadata={
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "product_pages": sorted(product_pages)
            }
        )
        logger.info(f"‚úÖ Created CHUNKS_CREATED checkpoint for job {job_id}")

        # Force garbage collection after chunking to free memory
        import gc
        gc.collect()
        logger.info("üßπ Memory cleanup after Stage 2 (Chunking)")

        # Stage 3: Image Processing (50-70%)
        logger.info("üñºÔ∏è [STAGE 3] Image Processing - Starting...")
        await tracker.update_stage(ProcessingStage.EXTRACTING_IMAGES, stage_name="image_processing")

        # LAZY LOADING: Load LlamaIndex service only for image processing
        logger.info("üì¶ Loading LlamaIndex service for image analysis...")
        try:
            llamaindex_service = await component_manager.load("llamaindex_service")
            loaded_components.append("llamaindex_service")
            logger.info("‚úÖ LlamaIndex service loaded for Stage 3")
        except Exception as e:
            logger.error(f"‚ùå Failed to load LlamaIndex service: {e}")
            raise

        # Re-extract PDF with images this time
        logger.info("üîÑ Starting image extraction from PDF...")
        logger.info(f"   PDF size: {len(file_content)} bytes")
        logger.info(f"   Document ID: {document_id}")
        logger.info(f"   Focused extraction: {focused_extraction}")
        logger.info(f"   Extract categories: {extract_categories}")

        try:
            # ‚úÖ OPTIMIZATION: Calculate estimated images based on focused extraction
            if focused_extraction and 'products' in extract_categories and product_pages:
                # Only extract images from product pages
                estimated_images = len(product_pages) * 2  # ~2 images per product page
                logger.info(f"   üéØ Focused extraction: Only extracting from {len(product_pages)} product pages")
                logger.info(f"   üìÑ Product pages: {sorted(product_pages)}")
            else:
                # Extract images from all pages
                estimated_images = page_count * 2
                logger.info(f"   üìÑ Full extraction: Extracting from all {page_count} pages")

            # üöÄ PROGRESSIVE TIMEOUT: Calculate timeout based on estimated images
            image_extraction_timeout = ProgressiveTimeoutStrategy.calculate_image_processing_timeout(
                image_count=estimated_images,
                concurrent_limit=1  # Extraction is sequential
            )
            logger.info(f"üìä Image extraction: ~{estimated_images} estimated images ‚Üí timeout: {image_extraction_timeout:.0f}s")

            # ‚úÖ OPTIMIZATION: Build processing options with page_list for focused extraction
            processing_options = {
                'extract_images': True,
                'extract_tables': False,
                'skip_upload': True  # Skip upload - will classify first, then upload only material images
            }

            # Add page_list for focused extraction (only extract images from product pages)
            if focused_extraction and 'products' in extract_categories and product_pages:
                processing_options['page_list'] = sorted(list(product_pages))  # Convert set to sorted list
                logger.info(f"   ‚úÖ Passing page_list to PyMuPDF: {len(processing_options['page_list'])} pages")

            # ‚è±Ô∏è PROGRESSIVE TIMEOUT GUARD: Prevent indefinite hangs during image extraction
            pdf_result_with_images = await with_timeout(
                pdf_processor.process_pdf_from_bytes(
                    pdf_bytes=file_content,
                    document_id=document_id,
                    processing_options=processing_options
                ),
                timeout_seconds=image_extraction_timeout,
                operation_name="PyMuPDF4LLM image extraction"
            )
            logger.info(f"‚úÖ Image extraction completed: {len(pdf_result_with_images.extracted_images)} images found")

            # AI-BASED IMAGE CLASSIFICATION: Filter out non-material images BEFORE processing
            # This prevents wasting money on AI processing for irrelevant images (faces, logos, etc.)
            logger.info(f"ü§ñ Starting AI-based image classification for {len(pdf_result_with_images.extracted_images)} images...")

            async def classify_image_as_material(image_path: str, page_num: int) -> Dict[str, Any]:
                """
                Classify an image as material-related or not using Claude/GPT-5.

                Returns:
                    {
                        'is_material': bool,
                        'classification': 'material_closeup' | 'material_in_situ' | 'non_material',
                        'confidence': float,
                        'reason': str
                    }
                """
                try:
                    # Read image and convert to base64
                    with open(image_path, 'rb') as f:
                        image_bytes = f.read()
                        image_base64 = base64.b64encode(image_bytes).decode('utf-8')

                    # Use Claude for classification (faster and cheaper than GPT-5 for this task)
                    classification_prompt = """Analyze this image and classify it into ONE of these categories:

            1. **material_closeup**: Close-up photo showing material texture, surface, pattern, or finish (tiles, wood, fabric, stone, metal, etc.)
            2. **material_in_situ**: Material shown in application/context (bathroom with tiles, furniture with fabric, room with flooring, etc.)
            3. **non_material**: NOT material-related (faces, logos, decorative graphics, charts, diagrams, text, random images, forests, abstract patterns that don't show actual material)

            Respond ONLY with this JSON format:
            {
                "classification": "material_closeup" | "material_in_situ" | "non_material",
                "confidence": 0.0-1.0,
                "reason": "brief explanation"
            }"""

                    response = await anthropic_service.analyze_image(
                        image_base64=image_base64,
                        prompt=classification_prompt,
                        model="claude-sonnet-4-20250514"  # Latest Claude model
                    )

                    # Parse response
                    import json
                    result = json.loads(response.get('analysis', '{}'))

                    is_material = result.get('classification') in ['material_closeup', 'material_in_situ']

                    return {
                        'is_material': is_material,
                        'classification': result.get('classification', 'non_material'),
                        'confidence': result.get('confidence', 0.0),
                        'reason': result.get('reason', 'Unknown')
                    }

                except Exception as e:
                    logger.error(f"‚ùå Image classification failed for {image_path}: {e}")
                    # Default to including the image if classification fails (fail-safe)
                    return {
                        'is_material': True,
                        'classification': 'unknown',
                        'confidence': 0.5,
                        'reason': f'Classification failed: {str(e)}'
                    }

            # Classify all images in parallel (with concurrency limit)
            from asyncio import Semaphore
            classification_semaphore = Semaphore(5)  # Classify 5 images at a time

            async def classify_with_semaphore(img_data):
                async with classification_semaphore:
                    image_path = img_data.get('path')
                    if not image_path or not os.path.exists(image_path):
                        return None

                    # Extract page number from filename
                    filename = img_data.get('filename', '')
                    page_num = None
                    if '-' in filename:
                        parts = filename.split('-')
                        if len(parts) >= 2:
                            try:
                                page_num = int(parts[-2]) + 1  # Convert 0-indexed to 1-indexed
                            except:
                                pass

                    classification = await classify_image_as_material(image_path, page_num)
                    img_data['ai_classification'] = classification
                    return img_data

            # Run classification for all images
            classification_tasks = [classify_with_semaphore(img_data) for img_data in pdf_result_with_images.extracted_images]
            classified_images = await asyncio.gather(*classification_tasks, return_exceptions=True)

            # Filter out non-material images
            material_images = []
            non_material_images = []

            for img_data in classified_images:
                if img_data is None or isinstance(img_data, Exception):
                    continue

                classification = img_data.get('ai_classification', {})
                if classification.get('is_material', False):
                    material_images.append(img_data)
                else:
                    non_material_images.append(img_data)
                    logger.info(f"   üö´ Filtered out: {img_data.get('filename')} - {classification.get('classification')} ({classification.get('reason')})")

            logger.info(f"‚úÖ AI classification complete:")
            logger.info(f"   Material images: {len(material_images)}")
            logger.info(f"   Non-material images filtered out: {len(non_material_images)}")
            logger.info(f"   Classification accuracy: {len(material_images) / (len(material_images) + len(non_material_images)) * 100:.1f}% kept")

            # Update tracker with filtered image count
            await tracker.update_database_stats(
                images_extracted=len(material_images),
                sync_to_db=True
            )

            # Upload only material images to Supabase Storage
            logger.info(f"üì§ Uploading {len(material_images)} material images to Supabase Storage...")

            async def upload_single_image(img_data):
                """Upload a single material image to Supabase Storage"""
                try:
                    image_path = img_data.get('path')
                    if not image_path or not os.path.exists(image_path):
                        logger.warning(f"Image file not found: {image_path}")
                        return None

                    # Upload to Supabase Storage
                    from app.services.pdf_processor import PDFProcessor
                    pdf_processor_temp = PDFProcessor()

                    upload_result = await pdf_processor_temp._upload_image_to_storage(
                        image_path,
                        document_id,
                        {
                            'filename': img_data.get('filename'),
                            'size_bytes': img_data.get('size_bytes'),
                            'format': img_data.get('format'),
                            'dimensions': img_data.get('dimensions'),
                            'width': img_data.get('width'),
                            'height': img_data.get('height')
                        },
                        None  # No enhanced path
                    )

                    if upload_result.get('success'):
                        # Update img_data with storage info
                        img_data['storage_url'] = upload_result.get('public_url')
                        img_data['storage_path'] = upload_result.get('storage_path')
                        img_data['storage_bucket'] = upload_result.get('storage_bucket')
                        logger.info(f"   ‚úÖ Uploaded: {img_data.get('filename')}")
                        return img_data
                    else:
                        logger.error(f"   ‚ùå Upload failed: {img_data.get('filename')} - {upload_result.get('error')}")
                        return None

                except Exception as e:
                    logger.error(f"   ‚ùå Upload error for {img_data.get('filename')}: {e}")
                    return None

            # Upload material images in parallel (5 at a time)
            upload_semaphore = Semaphore(5)

            async def upload_with_semaphore(img_data):
                async with upload_semaphore:
                    return await upload_single_image(img_data)

            upload_tasks = [upload_with_semaphore(img_data) for img_data in material_images]
            uploaded_images = await asyncio.gather(*upload_tasks, return_exceptions=True)

            # Filter out failed uploads
            material_images = [img for img in uploaded_images if img is not None and not isinstance(img, Exception)]

            logger.info(f"‚úÖ Upload complete: {len(material_images)} material images uploaded to storage")

            # Update pdf_result_with_images.extracted_images with uploaded material images
            pdf_result_with_images.extracted_images = material_images

            # üßπ CLEANUP: Delete ALL /tmp/ image files (material + non-material) after upload
            logger.info(f"üßπ Cleaning up temporary image files from /tmp/...")
            cleanup_count = 0
            cleanup_errors = 0

            # Combine all images for cleanup (material + non-material)
            all_images_for_cleanup = classified_images

            for img_data in all_images_for_cleanup:
                if img_data is None or isinstance(img_data, Exception):
                    continue

                image_path = img_data.get('path')
                if image_path and os.path.exists(image_path):
                    try:
                        os.remove(image_path)
                        cleanup_count += 1
                    except Exception as cleanup_error:
                        cleanup_errors += 1
                        logger.warning(f"   ‚ö†Ô∏è  Failed to delete {image_path}: {cleanup_error}")

            logger.info(f"‚úÖ Cleanup complete: {cleanup_count} files deleted, {cleanup_errors} errors")
        except Exception as extraction_error:
            logger.error(f"‚ùå CRITICAL: Image extraction failed: {extraction_error}")
            logger.error(f"   Error type: {type(extraction_error).__name__}")
            logger.error(f"   Error details: {str(extraction_error)}")
            import traceback
            logger.error(f"   Traceback: {traceback.format_exc()}")

            # üßπ CLEANUP ON ERROR: Delete any /tmp/ files that were created before the error
            try:
                if 'pdf_result_with_images' in locals() and hasattr(pdf_result_with_images, 'extracted_images'):
                    logger.info(f"üßπ Cleaning up {len(pdf_result_with_images.extracted_images)} temporary files after error...")
                    for img_data in pdf_result_with_images.extracted_images:
                        image_path = img_data.get('path')
                        if image_path and os.path.exists(image_path):
                            try:
                                os.remove(image_path)
                            except:
                                pass
            except Exception as cleanup_error:
                logger.warning(f"‚ö†Ô∏è  Cleanup after error failed: {cleanup_error}")

            # Update job status to failed
            await tracker.fail_job(error=Exception(f"Image extraction failed: {str(extraction_error)}"))
            raise

        # Group images by page number
        # Filename formats: "page_X_image_Y.png" OR "document_id.pdf-PAGE-IMAGE.jpg"
        images_by_page = {}
        for img_data in pdf_result_with_images.extracted_images:
            filename = img_data.get('filename', '')
            import re

            # Try format 1: page_X_image_Y.png
            match = re.search(r'page_(\d+)_', filename)
            if not match:
                # Try format 2: document_id.pdf-PAGE-IMAGE.jpg
                match = re.search(r'\.pdf-(\d+)-\d+\.', filename)

            if match:
                page_num = int(match.group(1))
                if page_num not in images_by_page:
                    images_by_page[page_num] = []
                images_by_page[page_num].append(img_data)
            else:
                logger.warning(f"Could not extract page number from filename: {filename}")

        # CRITICAL: Save images to database FIRST before AI processing
        # This ensures images are persisted even if AI processing fails
        # IMPORTANT: Respect focused_extraction flag and extract_categories
        logger.info(f"üíæ Saving images to database...")
        logger.info(f"   Focused extraction: {focused_extraction}")
        logger.info(f"   Extract categories: {', '.join(extract_categories)}")
        images_saved = 0
        images_skipped = 0

        # üöÄ DYNAMIC BATCH SIZE: Calculate optimal batch size for database inserts
        DEFAULT_INSERT_BATCH_SIZE = 100
        BATCH_INSERT_SIZE = memory_monitor.calculate_optimal_batch_size(
            default_batch_size=DEFAULT_INSERT_BATCH_SIZE,
            min_batch_size=10,  # At least 10 records per batch
            max_batch_size=200,  # Max 200 records per batch
            memory_per_item_mb=0.5  # Estimate 0.5MB per image record (metadata only)
        )
        logger.info(f"   üîß DYNAMIC DB BATCH SIZE: {BATCH_INSERT_SIZE} records per batch (adaptive)")

        image_records_batch = []

        for idx, img_data in enumerate(pdf_result_with_images.extracted_images):
            try:
                # Extract page number from filename
                filename = img_data.get('filename', '')
                match = re.search(r'page_(\d+)_', filename) or re.search(r'\.pdf-(\d+)-\d+\.', filename)
                page_num = int(match.group(1)) if match else None

                # DETAILED LOGGING: Log every image being processed
                logger.info(f"   [{idx+1}/{len(pdf_result_with_images.extracted_images)}] Processing image: {filename} (page {page_num})")
                logger.info(f"      storage_url: {img_data.get('storage_url')}")
                logger.info(f"      storage_path: {img_data.get('storage_path')}")
                logger.info(f"      storage_bucket: {img_data.get('storage_bucket')}")

                # Determine image category
                image_category = 'product' if (page_num and page_num in product_pages) else 'other'
                logger.info(f"      category: {image_category} (product_pages: {sorted(product_pages)[:5]}...)")

                # Skip images based on focused_extraction and extract_categories
                if focused_extraction and 'all' not in extract_categories:
                    # Only save images from categories specified in extract_categories
                    if 'products' in extract_categories and image_category != 'product':
                        logger.info(f"      ‚è≠Ô∏è  SKIPPED: Not a product image (focused_extraction=True, categories={extract_categories})")
                        images_skipped += 1
                        continue
                    # Note: Other categories (certificates, logos, specifications) are handled
                    # by the document_entities system, not the images table

                # Validate required fields
                if not img_data.get('storage_url'):
                    logger.warning(f"      ‚ö†Ô∏è  SKIPPED: Missing storage_url")
                    images_skipped += 1
                    continue

                # Prepare image record for database
                image_record = {
                    'document_id': document_id,
                    'workspace_id': workspace_id,
                    'image_url': img_data.get('storage_url'),
                    'image_type': 'extracted',
                    'caption': f"Image from page {page_num}" if page_num else "Extracted image",
                    'page_number': page_num,
                    'confidence': img_data.get('confidence_score', 0.5),
                    'processing_status': 'pending_analysis',
                    'metadata': {
                        'filename': filename,
                        'storage_path': img_data.get('storage_path'),
                        'storage_bucket': img_data.get('storage_bucket', 'pdf-tiles'),
                        'quality_score': img_data.get('quality_score', 0.5),
                        'extracted_at': datetime.utcnow().isoformat(),
                        'source': 'pdf_extraction',
                        'focused_extraction': focused_extraction,
                        'extract_categories': extract_categories,
                        'category': image_category,
                        'product_page': page_num in product_pages if page_num else False
                    }
                }

                # Add to batch instead of inserting immediately
                image_records_batch.append(image_record)
                images_saved += 1
                logger.info(f"      ‚úÖ Added to batch (total: {images_saved})")

                # ‚ö° OPTIMIZATION: Insert batch when it reaches BATCH_INSERT_SIZE
                if len(image_records_batch) >= BATCH_INSERT_SIZE:
                    try:
                        result = supabase.client.table('document_images').insert(image_records_batch).execute()
                        logger.info(f"   üíæ Batch inserted {len(image_records_batch)} images to database")

                        # üîë CRITICAL: Store returned IDs back into extracted_images for VECS lookup
                        if result.data:
                            for inserted_record in result.data:
                                # Find matching image in extracted_images and add the ID
                                for img in pdf_result_with_images.extracted_images:
                                    if img.get('filename') == inserted_record.get('metadata', {}).get('filename'):
                                        img['id'] = inserted_record['id']
                                        break

                        image_records_batch = []  # Clear batch
                    except Exception as batch_error:
                        logger.error(f"   ‚ùå Batch insert failed: {batch_error}")
                        # Fallback: Insert individually
                        for record in image_records_batch:
                            try:
                                result = supabase.client.table('document_images').insert(record).execute()
                                # Store ID back into extracted_images
                                if result.data and len(result.data) > 0:
                                    for img in pdf_result_with_images.extracted_images:
                                        if img.get('filename') == record.get('metadata', {}).get('filename'):
                                            img['id'] = result.data[0]['id']
                                            break
                            except Exception as individual_error:
                                logger.error(f"   ‚ùå Individual insert failed: {individual_error}")
                        image_records_batch = []

            except Exception as e:
                logger.error(f"      ‚ùå FAILED to prepare image {filename} for database: {e}")
                logger.error(f"         Error type: {type(e).__name__}")
                logger.error(f"         Error details: {str(e)}")
                import traceback
                logger.error(f"         Traceback: {traceback.format_exc()}")

        # ‚ö° OPTIMIZATION: Insert remaining images in final batch
        if image_records_batch:
            try:
                result = supabase.client.table('document_images').insert(image_records_batch).execute()
                logger.info(f"   üíæ Final batch inserted {len(image_records_batch)} images to database")

                # üîë CRITICAL: Store returned IDs back into extracted_images for VECS lookup
                if result.data:
                    for inserted_record in result.data:
                        # Find matching image in extracted_images and add the ID
                        for img in pdf_result_with_images.extracted_images:
                            if img.get('filename') == inserted_record.get('metadata', {}).get('filename'):
                                img['id'] = inserted_record['id']
                                break

            except Exception as batch_error:
                logger.error(f"   ‚ùå Final batch insert failed: {batch_error}")
                # Fallback: Insert individually
                for record in image_records_batch:
                    try:
                        result = supabase.client.table('document_images').insert(record).execute()
                        # Store ID back into extracted_images
                        if result.data and len(result.data) > 0:
                            for img in pdf_result_with_images.extracted_images:
                                if img.get('filename') == record.get('metadata', {}).get('filename'):
                                    img['id'] = result.data[0]['id']
                                    break
                    except Exception as individual_error:
                        logger.error(f"   ‚ùå Individual insert failed: {individual_error}")

        logger.info(f"‚úÖ Saved {images_saved}/{len(pdf_result_with_images.extracted_images)} images to database")
        if images_skipped > 0:
            logger.info(f"   Skipped {images_skipped} images (not in extract_categories: {', '.join(extract_categories)})")

        # Update tracker with images_extracted count
        tracker.images_extracted = images_saved
        await tracker.update_database_stats(
            images_stored=images_saved,
            sync_to_db=True
        )

        # Process images with Llama + CLIP (always extract images, even in focused mode)
        # OPTIMIZATION: Process in batches to reduce memory consumption
        images_processed = 0
        clip_embeddings_generated = 0
        specialized_embeddings_generated = 0  # ‚úÖ NEW: Track specialized embeddings

        # ‚úÖ OPTIMIZATION: Collect VECS records for batch upsert
        vecs_batch_records = []
        VECS_BATCH_SIZE = 50  # Batch upsert every 50 embeddings

        # üöÄ DYNAMIC BATCH SIZE: Calculate optimal batch size based on available memory
        DEFAULT_BATCH_SIZE = 10
        BATCH_SIZE = memory_monitor.calculate_optimal_batch_size(
            default_batch_size=DEFAULT_BATCH_SIZE,
            min_batch_size=2,  # At least 2 images per batch
            max_batch_size=20,  # Max 20 images per batch
            memory_per_item_mb=15.0  # Estimate 15MB per image (CLIP + Llama + image data)
        )

        logger.info(f"   Total images extracted from PDF: {len(pdf_result_with_images.extracted_images)}")
        logger.info(f"   Images grouped by page: {len(images_by_page)} pages with images")
        logger.info(f"   Product pages to process: {sorted(product_pages)}")
        logger.info(f"   üîß DYNAMIC BATCH PROCESSING: {BATCH_SIZE} images per batch (adaptive based on memory)")
        memory_monitor.log_memory_stats(prefix="   ")

        # ‚úÖ FIX: Filter images to process based on focused_extraction and product_pages
        # Only process images that were saved to database (i.e., product images when focused_extraction=True)
        all_images_to_process = []
        images_filtered_out = 0

        for page_num, images in images_by_page.items():
            for img_data in images:
                # Apply same filtering logic as database save
                if focused_extraction and 'all' not in extract_categories:
                    # Only process images from categories specified in extract_categories
                    image_category = 'product' if (page_num and page_num in product_pages) else 'other'

                    if 'products' in extract_categories and image_category != 'product':
                        # Skip non-product images when focused_extraction is enabled
                        images_filtered_out += 1
                        continue

                # Only process images that have storage_url (were successfully saved to database)
                if not img_data.get('storage_url'):
                    images_filtered_out += 1
                    continue

                all_images_to_process.append((page_num, img_data))

        total_images = len(all_images_to_process)
        logger.info(f"   üìä Total images to process for CLIP: {total_images} (filtered out: {images_filtered_out})")
        logger.info(f"   üéØ Focused extraction: {focused_extraction}, Categories: {extract_categories}")
        logger.info(f"   üìÑ Product pages: {sorted(product_pages)}")

        # ‚ö° OPTIMIZATION: Parallel image processing with concurrency limit
        # Process images in batches with parallel processing within each batch
        CONCURRENT_IMAGES = 5  # Process 5 images concurrently (5-10x faster)

        async def process_single_image(page_num, img_data, image_index, total_images):
            """Process a single image with CLIP + Llama Vision analysis"""
            nonlocal images_processed, clip_embeddings_generated, specialized_embeddings_generated, vecs_batch_records

            try:
                # Read image file and convert to base64
                image_path = img_data.get('path')
                if not image_path or not os.path.exists(image_path):
                    logger.warning(f"Image file not found: {image_path}")
                    return

                logger.info(f"üìñ [{image_index}/{total_images}] Reading image file: {image_path}")
                with open(image_path, 'rb') as f:
                    image_bytes = f.read()
                    image_base64 = base64.b64encode(image_bytes).decode('utf-8')
                logger.info(f"‚úÖ [{image_index}/{total_images}] Image read successfully: {len(image_bytes)} bytes")

                # CRITICAL: Generate CLIP embeddings for this image (with timeout guard + circuit breaker)
                logger.info(f"üé® [{image_index}/{total_images}] Generating CLIP embeddings for page {page_num}")
                try:
                    # Wrap with circuit breaker to fail fast on API outages
                    clip_result = await clip_breaker.call(
                        with_timeout,
                        llamaindex_service._generate_clip_embeddings(
                            image_base64=image_base64,
                            image_path=image_path
                        ),
                        timeout_seconds=TimeoutConstants.CLIP_EMBEDDING,
                        operation_name=f"CLIP embedding generation (image {image_index}/{total_images})"
                    )
                    logger.info(f"‚úÖ [{image_index}/{total_images}] CLIP embeddings generated successfully")
                except CircuitBreakerError as cb_error:
                    logger.warning(f"‚ö†Ô∏è [{image_index}/{total_images}] CLIP embedding skipped (circuit breaker open): {cb_error}")
                    clip_result = None
                except Exception as clip_error:
                    logger.error(f"‚ùå [{image_index}/{total_images}] CLIP embedding generation failed: {clip_error}")
                    logger.error(f"   Error type: {type(clip_error).__name__}")
                    clip_result = None

                # üîç Get image ID from the extracted_images list (already has UUID from batch insert)
                image_filename = os.path.basename(image_path)

                # Find image_id from the extracted_images list by matching filename
                image_id = None
                for img in pdf_result_with_images.extracted_images:
                    if img.get('filename') == image_filename:
                        image_id = img.get('id')
                        logger.debug(f"üìå [{image_index}/{total_images}] Found image ID from extracted list: {image_id}")
                        break

                if not image_id:
                    logger.warning(f"‚ö†Ô∏è [{image_index}/{total_images}] Could not find image ID for {image_filename} in extracted images list")

                # Analyze with Llama Vision (with timeout guard + circuit breaker)
                logger.info(f"üîç [{image_index}/{total_images}] Analyzing image with Llama Vision...")
                try:
                    # Wrap with circuit breaker to fail fast on API outages
                    analysis_result = await llama_breaker.call(
                        with_timeout,
                        llamaindex_service._analyze_image_material(
                            image_base64=image_base64,
                            image_path=image_path,
                            image_id=image_id,  # Pass actual image ID for Claude validation queue
                            document_id=document_id
                        ),
                        timeout_seconds=TimeoutConstants.LLAMA_VISION_CALL,
                        operation_name=f"Llama Vision analysis (image {image_index}/{total_images})"
                    )
                    logger.info(f"‚úÖ [{image_index}/{total_images}] Llama Vision analysis completed")
                except CircuitBreakerError as cb_error:
                    logger.warning(f"‚ö†Ô∏è [{image_index}/{total_images}] Llama Vision analysis skipped (circuit breaker open): {cb_error}")
                    analysis_result = {}
                except Exception as llama_error:
                    logger.error(f"‚ùå [{image_index}/{total_images}] Llama Vision analysis failed: {llama_error}")
                    logger.error(f"   Error type: {type(llama_error).__name__}")
                    analysis_result = {}

                # ‚úÖ OPTIMIZATION: Collect CLIP embeddings for batch upsert to VECS
                if clip_result and clip_result.get('embedding_512') and image_id:
                    # ‚úÖ FIX: Use storage_url from img_data instead of tmp image_path
                    storage_url = img_data.get('storage_url') or img_data.get('public_url')
                    storage_path = img_data.get('storage_path')

                    # Prepare metadata for VECS
                    vecs_metadata = {
                        'document_id': document_id,
                        'workspace_id': workspace_id,  # ‚úÖ ADD: Enable workspace filtering in VECS searches
                        'page_number': page_num,
                        'image_url': storage_url,  # ‚úÖ FIX: Use Supabase storage URL, not tmp path
                        'storage_path': storage_path,  # Include storage path for reference
                        'quality_score': analysis_result.get('quality_score'),
                        'confidence_score': analysis_result.get('confidence_score'),
                        'llama_analysis': analysis_result.get('llama_analysis'),
                        'material_properties': analysis_result.get('material_properties', {})
                    }

                    # Add to batch records for visual CLIP embeddings
                    vecs_batch_records.append((
                        image_id,
                        clip_result.get('embedding_512'),
                        vecs_metadata
                    ))

                    clip_embeddings_generated += 1
                    logger.debug(f"‚úÖ [{image_index}/{total_images}] Queued CLIP embedding for batch upsert (image {image_id})")

                    # ‚úÖ NEW: Save specialized CLIP embeddings (color, texture, style, material)
                    specialized_embeddings = {}
                    if clip_result.get('color_clip_512'):
                        specialized_embeddings['color'] = clip_result.get('color_clip_512')
                    if clip_result.get('texture_clip_512'):
                        specialized_embeddings['texture'] = clip_result.get('texture_clip_512')
                    if clip_result.get('style_clip_512'):
                        specialized_embeddings['style'] = clip_result.get('style_clip_512')
                    if clip_result.get('material_clip_512'):
                        specialized_embeddings['material'] = clip_result.get('material_clip_512')

                    if specialized_embeddings:
                        vecs_service = get_vecs_service()
                        await vecs_service.upsert_specialized_embeddings(
                            image_id=image_id,
                            embeddings=specialized_embeddings,
                            metadata=vecs_metadata
                        )
                        specialized_embeddings_generated += len(specialized_embeddings)  # ‚úÖ NEW: Track count
                        logger.debug(f"‚úÖ [{image_index}/{total_images}] Saved {len(specialized_embeddings)} specialized embeddings")

                    # ‚úÖ OPTIMIZATION: Batch upsert every VECS_BATCH_SIZE embeddings
                    if len(vecs_batch_records) >= VECS_BATCH_SIZE:
                        vecs_service = get_vecs_service()
                        batch_count = await vecs_service.batch_upsert_image_embeddings(vecs_batch_records)
                        logger.info(f"‚úÖ Batch upserted {batch_count} CLIP embeddings to VECS")
                        vecs_batch_records = []  # Clear batch

                elif not image_id:
                    logger.warning(f"‚ö†Ô∏è [{image_index}/{total_images}] Skipping VECS save - image ID not found for {image_filename}")

                images_processed += 1

                # üìä REAL-TIME PROGRESS: Update progress after each image (every 5 images to reduce DB load)
                if image_index % 5 == 0:
                    try:
                        await tracker.update_database_stats(
                            images_stored=images_processed,
                            sync_to_db=True
                        )
                        logger.debug(f"üìä Progress updated: {images_processed}/{total_images} images processed")
                    except Exception as progress_error:
                        logger.warning(f"‚ö†Ô∏è Failed to update progress: {progress_error}")

                # Clear image data from memory immediately after processing
                del image_bytes
                del image_base64
                if clip_result:
                    del clip_result
                if analysis_result:
                    del analysis_result

            except Exception as e:
                logger.error(f"‚ùå [{image_index}/{total_images}] Failed to process image on page {page_num}: {e}")
                logger.error(f"   Error type: {type(e).__name__}")
                logger.error(f"   Image path: {img_data.get('path')}")
                import traceback
                logger.error(f"   Traceback: {traceback.format_exc()}")
                # Continue processing other images even if one fails

        # Process images in batches with parallel processing
        for batch_start in range(0, total_images, BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, total_images)
            batch_images = all_images_to_process[batch_start:batch_end]
            batch_num = (batch_start // BATCH_SIZE) + 1
            total_batches = (total_images + BATCH_SIZE - 1) // BATCH_SIZE

            logger.info(f"   üîÑ Processing batch {batch_num}/{total_batches} ({len(batch_images)} images)")

            # üöÄ MEMORY PRESSURE MONITORING: Check memory before processing batch
            try:
                await memory_monitor.wait_for_memory_available(
                    required_mb=50,  # Need at least 50MB free for image processing
                    max_wait_seconds=120,  # Wait up to 2 minutes for memory
                    operation_name=f"batch {batch_num}/{total_batches}"
                )
            except MemoryError as mem_error:
                logger.error(f"‚ùå Insufficient memory for batch {batch_num}: {mem_error}")
                # Continue anyway, but log the warning
                memory_monitor.log_memory_stats(prefix=f"   [Batch {batch_num}] ")

            # Log memory before batch
            import psutil
            process = psutil.Process()
            mem_before = process.memory_info().rss / 1024 / 1024  # MB
            logger.info(f"   üíæ Memory before batch: {mem_before:.1f} MB")

            # ‚ö° PARALLEL PROCESSING: Process CONCURRENT_IMAGES images at a time
            from asyncio import Semaphore
            semaphore = Semaphore(CONCURRENT_IMAGES)

            async def process_with_semaphore(page_num, img_data, image_index):
                async with semaphore:
                    await process_single_image(page_num, img_data, image_index, total_images)

            # Create tasks for all images in this batch
            tasks = [
                process_with_semaphore(page_num, img_data, batch_start + idx + 1)
                for idx, (page_num, img_data) in enumerate(batch_images)
            ]

            # Execute all tasks in parallel (with concurrency limit)
            await asyncio.gather(*tasks, return_exceptions=True)

            # CRITICAL: Force garbage collection after each batch to free memory
            import gc
            gc.collect()

            # üöÄ MEMORY MONITORING: Log memory stats and check pressure after batch
            mem_after = process.memory_info().rss / 1024 / 1024  # MB
            mem_freed = mem_before - mem_after
            logger.info(f"   üíæ Memory after batch: {mem_after:.1f} MB (freed: {mem_freed:.1f} MB)")

            # Check memory pressure and trigger cleanup if needed
            mem_stats = await memory_monitor.check_memory_pressure()
            if mem_stats.is_high_pressure:
                logger.warning(f"   ‚ö†Ô∏è High memory pressure detected: {mem_stats.percent_used:.1f}%")

            logger.info(f"   ‚úÖ Batch {batch_num}/{total_batches} complete: {clip_embeddings_generated} CLIP embeddings generated so far")

            # Update progress after each batch
            await tracker.update_database_stats(
                images_stored=images_processed,
                sync_to_db=True
            )

        # ‚úÖ OPTIMIZATION: Final batch upsert for remaining VECS records
        if vecs_batch_records:
            vecs_service = get_vecs_service()
            batch_count = await vecs_service.batch_upsert_image_embeddings(vecs_batch_records)
            logger.info(f"‚úÖ Final batch upserted {batch_count} CLIP embeddings to VECS")
            vecs_batch_records = []

        # Don't overwrite tracker.images_stored - it was already set correctly from images_saved
        # tracker.images_stored is the count of images saved to database (line 2570)
        # images_processed is the count of images analyzed with Llama Vision (may be 0 if files don't exist)
        await tracker._sync_to_database(stage="image_processing")

        logger.info(f"‚úÖ [STAGE 3] Image Processing Complete: {images_processed} images processed, {clip_embeddings_generated} CLIP embeddings generated, {specialized_embeddings_generated} specialized embeddings generated (batch upserted to VECS)")

        # Create IMAGES_EXTRACTED checkpoint
        await checkpoint_recovery_service.create_checkpoint(
            job_id=job_id,
            stage=CheckpointStage.IMAGES_EXTRACTED,
            data={
                "document_id": document_id,
                "images_processed": images_processed,
                "clip_embeddings_generated": clip_embeddings_generated,
                "specialized_embeddings_generated": specialized_embeddings_generated,  # ‚úÖ NEW: Track specialized embeddings
                "images_by_page": {str(k): len(v) for k, v in images_by_page.items()}
            },
            metadata={
                "total_images": images_processed,
                "clip_embeddings": clip_embeddings_generated,
                "specialized_embeddings": specialized_embeddings_generated,  # ‚úÖ NEW: Add to metadata
                "product_pages_with_images": len(images_by_page)
            }
        )
        logger.info(f"‚úÖ Created IMAGES_EXTRACTED checkpoint for job {job_id}: {images_processed} images, {clip_embeddings_generated} CLIP embeddings, {specialized_embeddings_generated} specialized embeddings")

        # LAZY LOADING: Unload LlamaIndex service after image processing to free memory
        if "llamaindex_service" in loaded_components:
            logger.info("üßπ Unloading LlamaIndex service after Stage 3...")
            try:
                await component_manager.unload("llamaindex_service")
                loaded_components.remove("llamaindex_service")
                logger.info("‚úÖ LlamaIndex service unloaded, memory freed")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to unload LlamaIndex service: {e}")

        # Force garbage collection after image processing to free memory
        import gc
        gc.collect()
        logger.info("üßπ Memory cleanup after Stage 3 (Image Processing)")

        # Stage 4: Product Creation & Linking (70-90%)
        logger.info("üè≠ [STAGE 4] Product Creation & Linking - Starting...")
        await tracker.update_stage(ProcessingStage.FINALIZING, stage_name="product_creation")

        from app.services.entity_linking_service import EntityLinkingService
        from app.services.document_entity_service import DocumentEntityService

        # Create products in database (with metadata)
        products_created = 0
        product_id_map = {}  # Map product name to product ID for entity linking

        for product in catalog.products:
            try:
                # Use product.metadata field (new architecture - products + metadata inseparable)
                metadata = product.metadata or {}

                # Ensure page_range and confidence are in metadata
                if 'page_range' not in metadata:
                    metadata['page_range'] = product.page_range
                if 'confidence' not in metadata:
                    metadata['confidence'] = product.confidence

                product_data = {
                    'source_document_id': document_id,
                    'workspace_id': workspace_id,
                    'name': product.name,
                    'description': product.description or '',
                    'metadata': metadata  # ALL product metadata stored here
                }

                result = supabase.client.table('products').insert(product_data).execute()

                if result.data and len(result.data) > 0:
                    product_id = result.data[0]['id']
                    product_id_map[product.name] = product_id
                    products_created += 1
                    logger.info(f"   ‚úÖ Created product: {product.name} (ID: {product_id})")

            except Exception as e:
                logger.error(f"Failed to create product {product.name}: {e}")

        tracker.products_created = products_created
        logger.info(f"   Products created: {products_created}")

        # Save document entities (certificates, logos, specifications) if discovered
        document_entity_service = DocumentEntityService(supabase.client)
        entities_created = 0
        entity_id_map = {}  # Map entity name to entity ID

        # Combine all entities from catalog
        all_entities = []

        if "certificates" in extract_categories and catalog.certificates:
            from app.services.document_entity_service import DocumentEntity
            for cert in catalog.certificates:
                entity = DocumentEntity(
                    entity_type="certificate",
                    name=cert.name,
                    page_range=cert.page_range,
                    description=f"{cert.certificate_type or 'Certificate'} issued by {cert.issuer or 'Unknown'}",
                    metadata={
                        "certificate_type": cert.certificate_type,
                        "issuer": cert.issuer,
                        "issue_date": cert.issue_date,
                        "expiry_date": cert.expiry_date,
                        "standards": cert.standards or [],
                        "confidence": cert.confidence
                    }
                )
                all_entities.append(entity)

        if "logos" in extract_categories and catalog.logos:
            from app.services.document_entity_service import DocumentEntity
            for logo in catalog.logos:
                entity = DocumentEntity(
                    entity_type="logo",
                    name=logo.name,
                    page_range=logo.page_range,
                    description=logo.description,
                    metadata={
                        "logo_type": logo.logo_type,
                        "confidence": logo.confidence
                    }
                )
                all_entities.append(entity)

        if "specifications" in extract_categories and catalog.specifications:
            from app.services.document_entity_service import DocumentEntity
            for spec in catalog.specifications:
                entity = DocumentEntity(
                    entity_type="specification",
                    name=spec.name,
                    page_range=spec.page_range,
                    description=spec.description,
                    metadata={
                        "spec_type": spec.spec_type,
                        "confidence": spec.confidence
                    }
                )
                all_entities.append(entity)

        # Save all entities to database
        if all_entities:
            entity_ids = await document_entity_service.save_entities(
                entities=all_entities,
                source_document_id=document_id,
                workspace_id=workspace_id
            )
            entities_created = len(entity_ids)

            # Map entity names to IDs
            for entity, entity_id in zip(all_entities, entity_ids):
                entity_id_map[entity.name] = entity_id

            logger.info(f"   Document entities created: {entities_created}")
            logger.info(f"     - Certificates: {len([e for e in all_entities if e.entity_type == 'certificate'])}")
            logger.info(f"     - Logos: {len([e for e in all_entities if e.entity_type == 'logo'])}")
            logger.info(f"     - Specifications: {len([e for e in all_entities if e.entity_type == 'specification'])}")

        # Link entities (images, chunks, products)
        linking_service = EntityLinkingService()
        linking_results = await linking_service.link_all_entities(
            document_id=document_id,
            catalog=catalog
        )

        logger.info(f"   Entity linking complete:")
        logger.info(f"     - Image-to-product links: {linking_results['image_product_links']}")
        logger.info(f"     - Image-to-chunk links: {linking_results['image_chunk_links']}")

        await tracker._sync_to_database(stage="product_creation")

        logger.info(f"‚úÖ [STAGE 4] Product Creation & Linking Complete")

        # Create PRODUCTS_CREATED checkpoint
        checkpoint_metadata = {
            "entity_links": linking_results,
            "product_names": [p.name for p in catalog.products]
        }

        # Add document entity info if any were created
        if entities_created > 0:
            checkpoint_metadata["document_entities_created"] = entities_created
            checkpoint_metadata["entity_types"] = {
                "certificates": len([e for e in all_entities if e.entity_type == 'certificate']),
                "logos": len([e for e in all_entities if e.entity_type == 'logo']),
                "specifications": len([e for e in all_entities if e.entity_type == 'specification'])
            }

        await checkpoint_recovery_service.create_checkpoint(
            job_id=job_id,
            stage=CheckpointStage.PRODUCTS_CREATED,
            data={
                "document_id": document_id,
                "products_created": products_created,
                "document_entities_created": entities_created
            },
            metadata=checkpoint_metadata
        )
        logger.info(f"‚úÖ Created PRODUCTS_CREATED checkpoint for job {job_id}")

        # Force garbage collection after product creation to free memory
        import gc
        gc.collect()
        logger.info("üßπ Memory cleanup after Stage 4 (Product Creation)")

        # Stage 5: Quality Enhancement (90-100%) - ASYNC
        logger.info("‚ö° [STAGE 5] Quality Enhancement - Starting (Async)...")
        await tracker.update_stage(ProcessingStage.COMPLETED, stage_name="quality_enhancement")

        from app.services.claude_validation_service import ClaudeValidationService

        # Process Claude validation queue (for low-scoring images) with circuit breaker
        claude_service = ClaudeValidationService()
        try:
            validation_results = await claude_breaker.call(
                claude_service.process_validation_queue,
                document_id=document_id
            )
            logger.info(f"   Claude validation: {validation_results.get('validated', 0)} images validated")
            logger.info(f"   Average quality improvement: {validation_results.get('avg_improvement', 0):.2f}")
        except CircuitBreakerError as cb_error:
            logger.warning(f"‚ö†Ô∏è Claude validation skipped (circuit breaker open): {cb_error}")
            validation_results = {'validated': 0, 'avg_improvement': 0}

        await tracker._sync_to_database(stage="quality_enhancement")

        # Cleanup
        logger.info("üßπ Cleanup - Starting...")
        from app.services.cleanup_service import CleanupService

        cleanup_service = CleanupService()
        cleanup_results = await cleanup_service.cleanup_after_processing(
            document_id=document_id,
            job_id=job_id,
            temp_image_paths=[]  # Images already cleaned by PDF processor
        )

        logger.info(f"   Cleanup complete: {cleanup_results.get('images_deleted', 0)} images deleted, {cleanup_results.get('processes_killed', 0)} processes killed")

        # Cleanup temp PDF directory if it exists
        if hasattr(pdf_result, 'temp_dir') and pdf_result.temp_dir:
            try:
                pdf_processor = PDFProcessor()
                pdf_processor._cleanup_temp_files(pdf_result.temp_dir)
                logger.info(f"‚úÖ Cleaned up temp PDF directory: {pdf_result.temp_dir}")
            except Exception as cleanup_error:
                logger.warning(f"‚ö†Ô∏è Failed to cleanup temp PDF directory: {cleanup_error}")

        # Mark job as complete
        result = {
            "document_id": document_id,
            "products_discovered": len(catalog.products),
            "products_created": products_created,
            "product_names": [p.name for p in catalog.products],
            "chunks_created": tracker.chunks_created,
            "images_processed": images_processed,
            "claude_validations": validation_results.get('validated', 0),
            "focused_extraction": focused_extraction,
            "pages_processed": len(product_pages),
            "pages_skipped": pdf_result.page_count - len(product_pages),
            "confidence_score": catalog.confidence_score
        }

        await tracker.complete_job(result=result)

        # Create COMPLETED checkpoint
        await checkpoint_recovery_service.create_checkpoint(
            job_id=job_id,
            stage=CheckpointStage.COMPLETED,
            data={
                "document_id": document_id,
                "products_created": products_created,
                "chunks_created": tracker.chunks_created,
                "images_processed": images_processed
            },
            metadata={
                "processing_time": (datetime.utcnow() - start_time).total_seconds(),
                "confidence_score": catalog.confidence_score,
                "focused_extraction": focused_extraction,
                "pages_processed": len(product_pages)
            }
        )
        logger.info(f"‚úÖ Created COMPLETED checkpoint for job {job_id}")

        logger.info("=" * 80)
        logger.info(f"‚úÖ [PRODUCT DISCOVERY PIPELINE] COMPLETED")
        logger.info(f"   Products: {products_created}")
        logger.info(f"   Chunks: {tracker.chunks_created}")
        logger.info(f"   Images: {images_processed}")
        logger.info("=" * 80)

        # LAZY LOADING: Cleanup all loaded components after successful completion
        logger.info("üßπ Cleaning up all loaded components...")
        for component_name in loaded_components:
            try:
                await component_manager.unload(component_name)
                logger.info(f"‚úÖ Unloaded {component_name}")
            except Exception as cleanup_error:
                logger.warning(f"‚ö†Ô∏è Failed to unload {component_name}: {cleanup_error}")

        # Force garbage collection
        import gc
        gc.collect()
        logger.info("‚úÖ All components cleaned up, memory freed")

    except Exception as e:
        logger.error(f"‚ùå [PRODUCT DISCOVERY PIPELINE] FAILED: {e}", exc_info=True)

        # LAZY LOADING: Cleanup all loaded components on error
        logger.info("üßπ Cleaning up loaded components due to error...")
        for component_name in loaded_components:
            try:
                await component_manager.unload(component_name)
                logger.info(f"‚úÖ Unloaded {component_name}")
            except Exception as cleanup_error:
                logger.warning(f"‚ö†Ô∏è Failed to unload {component_name}: {cleanup_error}")

        # Cleanup temp PDF directory on error
        if 'pdf_result' in locals() and hasattr(pdf_result, 'temp_dir') and pdf_result.temp_dir:
            try:
                pdf_processor = PDFProcessor()
                pdf_processor._cleanup_temp_files(pdf_result.temp_dir)
                logger.info(f"‚úÖ Cleaned up temp PDF directory on error: {pdf_result.temp_dir}")
            except Exception as cleanup_error:
                logger.warning(f"‚ö†Ô∏è Failed to cleanup temp PDF directory on error: {cleanup_error}")

        # Force garbage collection
        import gc
        gc.collect()

        # Mark job as failed using tracker
        if 'tracker' in locals():
            await tracker.fail_job(error=e)
        else:
            # Fallback if tracker wasn't initialized
            job_storage[job_id]["status"] = "failed"
            job_storage[job_id]["error"] = str(e)
            job_storage[job_id]["progress"] = 100


@router.post("/query", response_model=QueryResponse)
async def query_documents(
    request: QueryRequest,
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    **ü§ñ CONSOLIDATED QUERY ENDPOINT - Text-Based RAG Query**

    This endpoint replaces:
    - `/api/documents/{id}/query` ‚Üí Use with `document_ids` filter
    - `/api/documents/{id}/summarize` ‚Üí Use with summarization prompt

    ## üéØ Query Capabilities

    ### Text Query (Implemented) ‚úÖ
    - Pure text-based RAG with advanced retrieval
    - Semantic search with reranking
    - Best for: Factual questions, information retrieval, summarization

    ## üìù Examples

    ### Text Query (Default)
    ```bash
    curl -X POST "/api/rag/query" \\
      -H "Content-Type: application/json" \\
      -d '{
        "query": "What are the dimensions of the NOVA product?",
        "top_k": 5
      }'
    ```

    ### Document-Specific Query
    ```bash
    curl -X POST "/api/rag/query" \\
      -H "Content-Type: application/json" \\
      -d '{
        "query": "Summarize this document",
        "document_ids": ["doc-123"],
        "top_k": 20
      }'
    ```

    ## üîÑ Migration from Old Endpoints

    **Old:** `POST /api/documents/{id}/query`
    **New:** `POST /api/rag/query` with `document_ids` filter

    **Old:** `POST /api/documents/{id}/summarize`
    **New:** `POST /api/rag/query` with summarization prompt
    """
    start_time = datetime.utcnow()

    try:
        # Standard text-based RAG query
        result = await llamaindex_service.advanced_rag_query(
            query=request.query,
            max_results=request.top_k,
            similarity_threshold=request.similarity_threshold,
            enable_reranking=request.enable_reranking,
            query_type="factual"
        )

        processing_time = (datetime.utcnow() - start_time).total_seconds()

        return QueryResponse(
            query=request.query,
            answer=result.get('answer', ''),
            sources=result.get('sources', []),
            confidence_score=result.get('confidence_score', 0.0),
            processing_time=processing_time,
            retrieved_chunks=len(result.get('sources', []))
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Query processing failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Query processing failed: {str(e)}"
        )

@router.post("/chat", response_model=ChatResponse)
async def chat_with_documents(
    request: ChatRequest,
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    Conversational interface for document Q&A.
    
    This endpoint maintains conversation context and provides
    contextual responses based on the document collection.
    """
    start_time = datetime.utcnow()
    
    try:
        # Generate conversation ID if not provided
        conversation_id = request.conversation_id or str(uuid4())
        
        # Process chat message using advanced_rag_query
        result = await llamaindex_service.advanced_rag_query(
            query=request.message,
            max_results=request.top_k,
            query_type="conversational"
        )
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        return ChatResponse(
            message=request.message,
            response=result.get('response', ''),
            conversation_id=conversation_id,
            sources=result.get('sources', []),
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Chat processing failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Chat processing failed: {str(e)}"
        )


async def _enhance_search_results(
    results: List[Dict[str, Any]],
    workspace_id: str,
    include_related_products: bool = True,
    related_products_limit: int = 3
) -> List[Dict[str, Any]]:
    """
    Enhance search results with related products and images.

    Args:
        results: Raw search results
        workspace_id: Workspace ID for scoped queries
        include_related_products: Whether to include related products
        related_products_limit: Max related products per result

    Returns:
        Enhanced results with related products and images
    """
    try:
        supabase_client = get_supabase_client()
        product_rel_service = ProductRelationshipService(supabase_client=supabase_client.client)

        enhanced = []

        for result in results:
            # Get product ID from result
            product_id = result.get('id')
            if not product_id:
                enhanced.append(result)
                continue

            # Fetch related images
            try:
                images_response = supabase_client.table('product_image_relationships').select(
                    'id, image_id, relationship_type, relevance_score, document_images(id, image_url, caption)'
                ).eq('product_id', product_id).order('relevance_score', desc=True).limit(10).execute()

                related_images = []
                for img_rel in images_response.data or []:
                    if img_rel.get('document_images'):
                        related_images.append({
                            'id': img_rel['document_images']['id'],
                            'url': img_rel['document_images']['image_url'],
                            'relationship_type': img_rel['relationship_type'],
                            'relevance_score': img_rel['relevance_score'],
                            'caption': img_rel['document_images'].get('caption')
                        })

                result['related_images'] = related_images
            except Exception as e:
                logger.warning(f"Failed to fetch related images for product {product_id}: {e}")
                result['related_images'] = []

            # Fetch related products
            if include_related_products:
                try:
                    related_products = await product_rel_service.find_related_products(
                        product_id=product_id,
                        workspace_id=workspace_id,
                        limit=related_products_limit
                    )
                    result['related_products'] = related_products
                except Exception as e:
                    logger.warning(f"Failed to fetch related products for product {product_id}: {e}")
                    result['related_products'] = []
            else:
                result['related_products'] = []

            # Ensure all metadata is included
            if 'metadata' not in result:
                result['metadata'] = {}

            enhanced.append(result)

        return enhanced

    except Exception as e:
        logger.error(f"Error enhancing search results: {e}", exc_info=True)
        # Return original results if enhancement fails
        return results


@router.post("/search", response_model=SearchResponse)
async def search_documents(
    request: SearchRequest,
    strategy: Optional[str] = Query(
        "multi_vector",
        description="Search strategy: 'multi_vector' (RECOMMENDED - default), 'semantic', 'vector', 'hybrid', 'material', 'image', 'color', 'texture', 'style', 'material_type', 'all'"
    ),
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    **üîç CONSOLIDATED SEARCH ENDPOINT - Single Entry Point for All Search Strategies**

    This endpoint replaces:
    - `/api/search/semantic` ‚Üí Use `strategy="semantic"`
    - `/api/search/similarity` ‚Üí Use `strategy="vector"`
    - `/api/unified-search` ‚Üí Use this endpoint

    ## üéØ Search Strategies (All Implemented ‚úÖ)

    ### Multi-Vector Search (`strategy="multi_vector"`) - ‚≠ê RECOMMENDED DEFAULT ‚úÖ
    - Combines 6 embedding types with intelligent weighted scoring:
      - text_embedding_1536 (20%)
      - visual_clip_embedding_512 (20%)
      - color_clip_embedding_512 (15%)
      - texture_clip_embedding_512 (15%)
      - style_clip_embedding_512 (15%)
      - material_clip_embedding_512 (15%)
    - Best accuracy and performance for general queries
    - Best for: Product discovery, material matching, general search

    ### Semantic Search (`strategy="semantic"`) ‚úÖ
    - Natural language understanding with MMR (Maximal Marginal Relevance)
    - Context-aware matching with diversity
    - Best for: Fast text queries, conceptual search, diverse results

    ### Vector Search (`strategy="vector"`) ‚úÖ
    - Pure vector similarity (cosine distance)
    - Fast and efficient, no diversity filtering
    - Best for: Finding most similar documents, precise matching

    ### Hybrid Search (`strategy="hybrid"`) ‚úÖ
    - Combines semantic (70%) + PostgreSQL full-text search (30%)
    - Best for: Balancing semantic understanding with keyword matching

    ### Material Property Search (`strategy="material"`) ‚úÖ
    - JSONB-based filtering with AND/OR logic
    - Requires `material_filters` in request body
    - Best for: Filtering by specific material properties

    ### Image Similarity Search (`strategy="image"`) ‚úÖ
    - Visual similarity using CLIP embeddings
    - Requires `image_url` or `image_base64` in request body
    - Best for: Finding visually similar products

    ### Specialized Visual Searches ‚úÖ NEW
    - **Color Search** (`strategy="color"`): Color palette matching using specialized CLIP embeddings
      - Best for: "Find materials with warm tones", "similar color palette"
    - **Texture Search** (`strategy="texture"`): Texture pattern matching using specialized CLIP embeddings
      - Best for: "Find rough textured materials", "similar texture pattern"
    - **Style Search** (`strategy="style"`): Design style matching using specialized CLIP embeddings
      - Best for: "Find modern style materials", "similar design aesthetic"
    - **Material Type Search** (`strategy="material_type"`): Material type matching using specialized CLIP embeddings
      - Best for: "Find similar material types", "materials like this"

    ### All Strategies (`strategy="all"`) ‚úÖ
    - **Parallel execution** of ALL 10 strategies using `asyncio.gather()`
    - ‚ö†Ô∏è **SLOWER and HIGHER COST** than multi_vector (10x more operations)
    - Intelligent result merging with weighted scoring
    - Graceful error handling (failed strategies don't block others)
    - Best for: Comprehensive search when user explicitly requests exhaustive results
    - **NOTE**: Use `multi_vector` instead for better performance and accuracy

    ## üìù Examples

    ### Multi-Vector Search (‚≠ê RECOMMENDED - Default)
    ```bash
    curl -X POST "/api/rag/search" \\
      -H "Content-Type: application/json" \\
      -d '{"query": "modern minimalist furniture", "workspace_id": "xxx", "top_k": 10}'
    ```

    ### Semantic Search (Fast text-only)
    ```bash
    curl -X POST "/api/rag/search?strategy=semantic" \\
      -H "Content-Type: application/json" \\
      -d '{"query": "oak wood flooring", "workspace_id": "xxx", "top_k": 5}'
    ```

    ### Specialized Color Search
    ```bash
    curl -X POST "/api/rag/search?strategy=color" \\
      -H "Content-Type: application/json" \\
      -d '{"query": "warm tones", "workspace_id": "xxx", "top_k": 10}'
    ```

    ### Material Property Search
    ```bash
    curl -X POST "/api/rag/search?strategy=material" \\
      -H "Content-Type: application/json" \\
      -d '{"workspace_id": "xxx", "material_filters": {"material_type": "fabric", "color": ["red", "blue"]}, "top_k": 10}'
    ```

    ### Image Similarity Search
    ```bash
    curl -X POST "/api/rag/search?strategy=image" \\
      -H "Content-Type: application/json" \\
      -d '{"workspace_id": "xxx", "image_url": "https://example.com/image.jpg", "top_k": 10}'
    ```

    ### All Strategies (Parallel Execution - 3-4x Faster!)
    ```bash
    curl -X POST "/api/rag/search?strategy=all" \\
      -H "Content-Type: application/json" \\
      -d '{"query": "modern oak furniture", "workspace_id": "xxx", "top_k": 10}'
    ```

    ## üìä Response Example (All Strategies)
    ```json
    {
      "query": "modern oak furniture",
      "enhanced_query": "modern oak furniture",
      "results": [
        {
          "id": "product_uuid_1",
          "name": "Modern Oak Dining Table",
          "description": "Contemporary oak furniture...",
          "score": 0.92,
          "final_score": 0.85,
          "strategy_count": 4,
          "strategies": ["semantic", "vector", "multi_vector", "hybrid"]
        }
      ],
      "total_results": 10,
      "search_type": "all",
      "processing_time": 0.223,
      "search_metadata": {
        "strategies_executed": 4,
        "strategies_successful": 4,
        "strategies_failed": 0,
        "strategy_breakdown": {
          "semantic": {"count": 3, "success": true},
          "vector": {"count": 2, "success": true},
          "multi_vector": {"count": 4, "success": true},
          "hybrid": {"count": 5, "success": true}
        },
        "parallel_execution": true,
        "parallel_processing_time": 0.017
      }
    }
    ```

    ## ‚ö° Performance Characteristics

    | Strategy | Typical Time | Max Time | Notes |
    |----------|-------------|----------|-------|
    | semantic | 100-150ms | 300ms | Indexed, MMR diversity |
    | vector | 50-100ms | 200ms | Fastest, pure similarity |
    | multi_vector | 200-300ms | 500ms | 3 embeddings, sequential scan for 2048-dim |
    | hybrid | 120-180ms | 350ms | Semantic + full-text search |
    | material | 30-50ms | 100ms | JSONB indexed |
    | image | 100-150ms | 300ms | CLIP indexed |
    | **all (parallel)** | **200-300ms** | **500ms** | **3-4x faster than sequential** |

    ## üîÑ Migration from Old Endpoints

    **Old:** `POST /api/search/semantic`
    **New:** `POST /api/rag/search?strategy=semantic`

    **Old:** `POST /api/search/similarity`
    **New:** `POST /api/rag/search?strategy=vector`

    **Old:** `POST /api/unified-search`
    **New:** `POST /api/rag/search` (same functionality, clearer naming)

    ## ‚ö†Ô∏è Error Codes

    - **400 Bad Request**: Invalid parameters (missing query, invalid strategy, etc.)
    - **401 Unauthorized**: Missing or invalid authentication
    - **404 Not Found**: Workspace not found
    - **500 Internal Server Error**: Search processing failed
    - **503 Service Unavailable**: LlamaIndex service not available

    ## üéØ Rate Limits

    - **60 requests/minute** per user
    - **1000 requests/hour** per workspace
    - Parallel execution (`strategy="all"`) counts as 1 request
    """
    start_time = datetime.utcnow()

    try:
        # Validate strategy
        valid_strategies = ['semantic', 'vector', 'multi_vector', 'hybrid', 'material', 'image', 'all']
        if strategy not in valid_strategies:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid strategy '{strategy}'. Valid strategies: {', '.join(valid_strategies)}"
            )

        # Initialize services
        supabase_client = get_supabase_client()
        search_prompt_service = SearchPromptService(supabase_client=supabase_client.client)
        product_rel_service = ProductRelationshipService(supabase_client=supabase_client.client)

        # Apply enhancement prompt to query if enabled
        query_to_use = request.query
        enhanced_query = None
        prompts_applied = []

        if request.use_search_prompts:
            enhancement_result = await search_prompt_service.enhance_query(
                query=request.query,
                workspace_id=request.workspace_id,
                custom_prompt=request.custom_formatting_prompt
            )
            if enhancement_result.get('enhancement_applied'):
                query_to_use = enhancement_result['enhanced_query']
                enhanced_query = query_to_use
                prompts_applied.extend(enhancement_result.get('prompts_applied', []))

        # Route to appropriate search method based on strategy
        if strategy == "semantic":
            # Semantic search using MMR (Maximal Marginal Relevance)
            # Balances relevance and diversity (lambda_mult=0.5)
            results = await llamaindex_service.semantic_search_with_mmr(
                query=query_to_use,
                k=request.top_k,
                lambda_mult=0.5
            )

        elif strategy == "vector":
            # Pure vector similarity search (cosine distance)
            # No diversity filtering (lambda_mult=1.0)
            results = await llamaindex_service.semantic_search_with_mmr(
                query=query_to_use,
                k=request.top_k,
                lambda_mult=1.0  # Pure similarity, no diversity
            )

        elif strategy == "multi_vector":
            # Multi-vector search combining 3 embedding types
            # text_embedding_1536 (40%), visual_clip_embedding_512 (30%), multimodal_fusion_embedding_2048 (30%)
            results = await llamaindex_service.multi_vector_search(
                query=query_to_use,
                workspace_id=request.workspace_id,
                top_k=request.top_k
            )

        elif strategy == "hybrid":
            # Hybrid search combining semantic (70%) + full-text keyword search (30%)
            results = await llamaindex_service.hybrid_search(
                query=query_to_use,
                workspace_id=request.workspace_id,
                top_k=request.top_k
            )

        elif strategy == "material":
            # Material property search using JSONB filtering
            # Requires material_filters in request
            material_filters = getattr(request, 'material_filters', {})
            if not material_filters:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="material_filters required for material property search"
                )
            results = await llamaindex_service.material_property_search(
                workspace_id=request.workspace_id,
                material_filters=material_filters,
                top_k=request.top_k
            )

        elif strategy == "image":
            # Image similarity search using CLIP embeddings
            # Requires image_url or image_base64 in request
            image_url = getattr(request, 'image_url', None)
            image_base64 = getattr(request, 'image_base64', None)
            if not image_url and not image_base64:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="image_url or image_base64 required for image similarity search"
                )
            results = await llamaindex_service.image_similarity_search(
                workspace_id=request.workspace_id,
                image_url=image_url,
                image_base64=image_base64,
                top_k=request.top_k
            )

        elif strategy == "all":
            # Run all strategies in parallel for 3-4x performance improvement
            # Sequential: ~800ms, Parallel: ~200-300ms
            material_filters = getattr(request, 'material_filters', None)
            image_url = getattr(request, 'image_url', None)
            image_base64 = getattr(request, 'image_base64', None)

            results = await llamaindex_service.search_all_strategies(
                query=query_to_use,
                workspace_id=request.workspace_id,
                top_k=request.top_k,
                material_filters=material_filters,
                image_url=image_url,
                image_base64=image_base64
            )

        # Get raw results
        raw_results = results.get('results', [])

        # Apply formatting, filtering, enrichment prompts if enabled
        processed_results = raw_results
        if request.use_search_prompts:
            processed_results = await search_prompt_service.format_results(
                raw_results, request.workspace_id, request.custom_formatting_prompt
            )
            processed_results = await search_prompt_service.filter_results(
                processed_results, request.workspace_id
            )
            processed_results = await search_prompt_service.enrich_results(
                processed_results, request.workspace_id
            )

        # Enhance results with related products and images
        enhanced_results = await _enhance_search_results(
            processed_results,
            request.workspace_id,
            request.include_related_products,
            request.related_products_limit
        )

        processing_time = (datetime.utcnow() - start_time).total_seconds()

        # Build search metadata
        search_metadata = {
            'prompts_applied': prompts_applied,
            'prompts_enabled': request.use_search_prompts,
            'related_products_included': request.include_related_products
        }

        # Add parallel execution metadata for 'all' strategy
        if strategy == "all":
            search_metadata.update({
                'strategies_executed': results.get('strategies_executed', 0),
                'strategies_successful': results.get('strategies_successful', 0),
                'strategies_failed': results.get('strategies_failed', 0),
                'strategy_breakdown': results.get('strategy_breakdown', {}),
                'parallel_execution': True,
                'parallel_processing_time': results.get('processing_time', 0)
            })

        return SearchResponse(
            query=request.query,
            enhanced_query=enhanced_query,
            results=enhanced_results,
            total_results=results.get('total_results', 0),
            search_type=strategy,
            processing_time=processing_time,
            search_metadata=search_metadata
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Search processing failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Search processing failed: {str(e)}"
        )

@router.get("/documents/documents/{document_id}/content")
async def get_document_content(
    document_id: str,
    include_chunks: bool = Query(True, description="Include document chunks"),
    include_images: bool = Query(True, description="Include document images"),
    include_products: bool = Query(False, description="Include products created from document")
):
    """
    Get complete document content with all AI analysis results.

    Returns comprehensive document data including:
    - Document metadata
    - All chunks with embeddings
    - All images with AI analysis (CLIP, Llama, Claude)
    - All products created from the document
    - Complete AI model usage statistics
    """
    try:
        logger.info(f"üìä Fetching complete content for document {document_id}")
        supabase_client = get_supabase_client()

        # Get document metadata
        doc_response = supabase_client.client.table('documents').select('*').eq('id', document_id).execute()
        if not doc_response.data or len(doc_response.data) == 0:
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found")

        document = doc_response.data[0]
        result = {
            "id": document['id'],
            "created_at": document['created_at'],
            "metadata": document.get('metadata', {}),
            "chunks": [],
            "images": [],
            "products": [],
            "statistics": {}
        }

        # Get chunks with embeddings
        if include_chunks:
            logger.info(f"üìÑ Fetching chunks for document {document_id}")
            chunks_response = supabase_client.client.table('document_chunks').select('*').eq('document_id', document_id).execute()
            chunks = chunks_response.data or []

            # Get embeddings for each chunk
            for chunk in chunks:
                embeddings_response = supabase_client.client.table('embeddings').select('*').eq('chunk_id', chunk['id']).execute()
                chunk['embeddings'] = embeddings_response.data or []

            result['chunks'] = chunks
            logger.info(f"‚úÖ Fetched {len(chunks)} chunks")

        # Get images with AI analysis
        if include_images:
            logger.info(f"üñºÔ∏è Fetching images for document {document_id}")
            images_response = supabase_client.client.table('document_images').select('*').eq('document_id', document_id).execute()
            result['images'] = images_response.data or []
            logger.info(f"‚úÖ Fetched {len(result['images'])} images")

        # Get products
        if include_products:
            logger.info(f"üè≠ Fetching products for document {document_id}")
            products_response = supabase_client.client.table('products').select('*').eq('source_document_id', document_id).execute()
            result['products'] = products_response.data or []
            logger.info(f"‚úÖ Fetched {len(result['products'])} products")

        # Calculate statistics
        chunks_count = len(result['chunks'])
        images_count = len(result['images'])
        products_count = len(result['products'])

        # Count embeddings
        text_embeddings = sum(1 for chunk in result['chunks'] if chunk.get('embeddings'))
        clip_embeddings = sum(1 for img in result['images'] if img.get('visual_clip_embedding_512'))
        llama_analysis = sum(1 for img in result['images'] if img.get('llama_analysis'))
        claude_validation = sum(1 for img in result['images'] if img.get('claude_validation'))
        color_embeddings = sum(1 for img in result['images'] if img.get('color_embedding_256'))
        texture_embeddings = sum(1 for img in result['images'] if img.get('texture_embedding_256'))
        application_embeddings = sum(1 for img in result['images'] if img.get('application_embedding_512'))

        result['statistics'] = {
            "chunks_count": chunks_count,
            "images_count": images_count,
            "products_count": products_count,
            "ai_usage": {
                "openai_calls": text_embeddings,
                "llama_calls": llama_analysis,
                "claude_calls": claude_validation,
                "clip_embeddings": clip_embeddings
            },
            "embeddings_generated": {
                "text": text_embeddings,
                "visual": clip_embeddings,
                "color": color_embeddings,
                "texture": texture_embeddings,
                "application": application_embeddings,
                "total": text_embeddings + clip_embeddings + color_embeddings + texture_embeddings + application_embeddings
            },
            "completion_rates": {
                "text_embeddings": f"{(text_embeddings / chunks_count * 100) if chunks_count > 0 else 0:.1f}%",
                "image_analysis": f"{(clip_embeddings / images_count * 100) if images_count > 0 else 0:.1f}%"
            }
        }

        logger.info(f"‚úÖ Document content fetched successfully: {chunks_count} chunks, {images_count} images, {products_count} products")
        return JSONResponse(content=result)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error fetching document content: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error fetching document content: {str(e)}")


@router.get("/documents", response_model=DocumentListResponse)
async def list_documents(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Page size"),
    search: Optional[str] = Query(None, description="Search term for filtering"),
    tags: Optional[str] = Query(None, description="Comma-separated tags for filtering"),
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    List and filter documents in the collection.

    This endpoint provides paginated access to the document collection
    with optional filtering by search terms and tags.
    """
    try:
        # Parse tags filter
        tag_filter = []
        if tags:
            tag_filter = [tag.strip() for tag in tags.split(',')]
        
        # Get documents using list_indexed_documents
        result = await llamaindex_service.list_indexed_documents()
        
        return DocumentListResponse(
            documents=result.get('documents', []),
            total_count=result.get('total_count', 0),
            page=page,
            page_size=page_size
        )
        
    except Exception as e:
        logger.error(f"Document listing failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Document listing failed: {str(e)}"
        )

@router.delete("/documents/{document_id}")
async def delete_document(
    document_id: str,
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    Delete a document and its associated embeddings.
    
    This endpoint removes a document from the collection and
    cleans up all associated data including embeddings and chunks.
    """
    try:
        # Delete document
        result = await llamaindex_service.delete_document(document_id)
        
        if not result.get('success', False):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found"
            )
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "Document deleted successfully",
                "document_id": document_id,
                "chunks_deleted": result.get('chunks_deleted', 0)
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document deletion failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Document deletion failed: {str(e)}"
        )

@router.get("/health", response_model=HealthCheckResponse)
async def rag_health_check(
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    Health check for RAG services.
    
    This endpoint checks the health of all RAG-related services
    including LlamaIndex, embedding service, and vector store.
    """
    try:
        # Check LlamaIndex service health
        llamaindex_health = await llamaindex_service.health_check()

        # Try to check embedding service health (optional)
        embedding_health = {"status": "unknown", "message": "Embedding service not available"}
        try:
            embedding_service = await get_embedding_service()
            embedding_health = await embedding_service.health_check()
        except Exception as e:
            logger.warning(f"Embedding service health check failed: {e}")
            embedding_health = {"status": "error", "error": str(e)}

        # Determine overall status
        overall_status = "healthy"
        if llamaindex_health.get("status") != "healthy":
            overall_status = "degraded"

        return HealthCheckResponse(
            status=overall_status,
            services={
                "llamaindex": llamaindex_health,
                "embedding": embedding_health
            },
            timestamp=datetime.utcnow().isoformat()
        )
        
    except Exception as e:
        logger.error(f"RAG health check failed: {e}", exc_info=True)
        return HealthCheckResponse(
            status="unhealthy",
            services={
                "llamaindex": {"status": "error", "error": str(e)},
                "embedding": {"status": "unknown"}
            },
            timestamp=datetime.utcnow().isoformat()
        )

@router.get("/stats")
async def get_rag_statistics(
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    Get RAG system statistics.

    This endpoint provides statistics about the RAG system including
    document counts, embedding statistics, and performance metrics.
    """
    try:
        # Get available statistics from the service
        memory_stats = llamaindex_service.get_memory_stats()
        health_check = await llamaindex_service.health_check()

        # Combine statistics
        stats = {
            "memory": memory_stats,
            "health": health_check,
            "indices_count": len(llamaindex_service.indices),
            "storage_dir": llamaindex_service.storage_dir,
            "embedding_model": llamaindex_service.embedding_model,
            "llm_model": llamaindex_service.llm_model
        }

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "status": "success",
                "statistics": stats,
                "timestamp": datetime.utcnow().isoformat()
            }
        )

    except Exception as e:
        logger.error(f"Statistics retrieval failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Statistics retrieval failed: {str(e)}"
        )

@router.get("/workspace-stats")
async def get_workspace_statistics(
    workspace_id: str,
    supabase: SupabaseClient = Depends(get_supabase_client)
):
    """
    Get comprehensive workspace statistics including VECS embedding counts.

    Returns counts for:
    - Products
    - Chunks
    - Images
    - Text embeddings (from embeddings table)
    - Image embeddings (from VECS)
    - Total embeddings (text + image)
    """
    try:
        from app.services.vecs_service import get_vecs_service

        # Query Supabase tables for counts
        products_response = supabase.client.table('products').select('id', count='exact').eq('workspace_id', workspace_id).execute()
        chunks_response = supabase.client.table('document_chunks').select('id', count='exact').eq('workspace_id', workspace_id).execute()
        images_response = supabase.client.table('document_images').select('id', count='exact').eq('workspace_id', workspace_id).execute()
        text_embeddings_response = supabase.client.table('embeddings').select('id', count='exact').eq('workspace_id', workspace_id).execute()

        # Get VECS image embeddings count using SQL function (bypasses VECS connection issues)
        try:
            vecs_count_result = supabase.client.rpc('count_vecs_embeddings', {'p_workspace_id': workspace_id}).execute()
            image_embeddings_count = vecs_count_result.data if isinstance(vecs_count_result.data, int) else 0
        except Exception as vecs_error:
            logger.warning(f"‚ö†Ô∏è VECS count failed: {vecs_error}")
            image_embeddings_count = 0

        # Calculate totals
        products_count = products_response.count or 0
        chunks_count = chunks_response.count or 0
        images_count = images_response.count or 0
        text_embeddings_count = text_embeddings_response.count or 0
        total_embeddings = text_embeddings_count + image_embeddings_count

        stats = {
            "workspace_id": workspace_id,
            "products": products_count,
            "chunks": chunks_count,
            "images": images_count,
            "embeddings": {
                "text": text_embeddings_count,
                "images": image_embeddings_count,
                "total": total_embeddings
            }
        }

        logger.info(f"‚úÖ Workspace stats: {stats}")

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "status": "success",
                "statistics": stats,
                "timestamp": datetime.utcnow().isoformat()
            }
        )

    except Exception as e:
        logger.error(f"Workspace statistics retrieval failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Workspace statistics retrieval failed: {str(e)}"
        )

# Advanced Search Endpoints for Phase 7 Features

@router.post("/search/mmr", response_model=MMRSearchResponse)
async def mmr_search(
    request: MMRSearchRequest,
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    Perform MMR (Maximal Marginal Relevance) search for diverse results.
    
    This endpoint implements MMR search to provide diverse, non-redundant results
    by balancing relevance and diversity using the lambda parameter.
    """
    try:
        start_time = datetime.utcnow()
        
        # Call the MMR search method from LlamaIndex service
        results = await llamaindex_service.semantic_search_with_mmr(
            query=request.query,
            top_k=request.top_k,
            diversity_threshold=request.diversity_threshold,
            lambda_param=request.lambda_param,
            document_ids=request.document_ids,
            include_metadata=request.include_metadata
        )
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        return MMRSearchResponse(
            query=request.query,
            results=results.get('results', []),
            total_results=results.get('total_results', 0),
            diversity_score=results.get('diversity_score', 0.0),
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"MMR search failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"MMR search failed: {str(e)}"
        )

@router.post("/search/advanced", response_model=AdvancedQueryResponse)
async def advanced_query_search(
    request: AdvancedQueryRequest,
    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
):
    """
    Perform advanced query search with optimization and expansion.
    
    This endpoint provides advanced query processing including query expansion,
    rewriting, and optimization based on query type and search parameters.
    """
    try:
        start_time = datetime.utcnow()
        
        # Convert string enums to proper enum types
        query_type = QueryType(request.query_type.upper())
        search_operator = SearchOperator(request.search_operator.upper())
        
        # Call the advanced query method from LlamaIndex service
        results = await llamaindex_service.advanced_query_with_optimization(
            query=request.query,
            query_type=query_type,
            top_k=request.top_k,
            enable_expansion=request.enable_expansion,
            enable_rewriting=request.enable_rewriting,
            similarity_threshold=request.similarity_threshold,
            document_ids=request.document_ids,
            metadata_filters=request.metadata_filters,
            search_operator=search_operator
        )

        processing_time = (datetime.utcnow() - start_time).total_seconds()

        return AdvancedQueryResponse(
            query=request.query,
            results=results.get('results', []),
            total_results=results.get('total_results', 0),
            processing_time=processing_time,
            query_type=request.query_type,
            optimizations_applied=results.get('optimizations_applied', [])
        )

    except Exception as e:
        logger.error(f"Advanced query search failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Advanced query search failed: {str(e)}"
        )


@router.get("/job/{job_id}/ai-tracking")
async def get_job_ai_tracking(job_id: str):
    """
    Get detailed AI model tracking information for a job.

    Returns comprehensive metrics on:
    - Which AI models were used (LLAMA, Anthropic, CLIP, OpenAI)
    - Confidence scores and results
    - Token usage and processing time
    - Success/failure rates
    - Per-stage breakdown
    """
    try:
        if job_id not in job_storage:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} not found"
            )

        job_info = job_storage[job_id]
        ai_tracker = job_info.get("ai_tracker")

        if not ai_tracker:
            return {
                "job_id": job_id,
                "message": "No AI tracking data available for this job",
                "status": job_info.get("status", "unknown")
            }

        # Get comprehensive summary
        summary = ai_tracker.get_job_summary()

        return {
            "job_id": job_id,
            "status": job_info.get("status", "processing"),
            "progress": job_info.get("progress", 0),
            "ai_tracking": summary,
            "metadata": job_info.get("metadata", {})
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get AI tracking for job {job_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get AI tracking: {str(e)}"
        )


@router.get("/job/{job_id}/ai-tracking/stage/{stage}")
async def get_job_ai_tracking_by_stage(job_id: str, stage: str):
    """
    Get AI model tracking information for a specific processing stage.

    Args:
        job_id: Job identifier
        stage: Processing stage (classification, boundary_detection, embedding, etc.)

    Returns:
        Detailed metrics for the specified stage
    """
    try:
        if job_id not in job_storage:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} not found"
            )

        job_info = job_storage[job_id]
        ai_tracker = job_info.get("ai_tracker")

        if not ai_tracker:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No AI tracking data available for this job"
            )

        stage_details = ai_tracker.get_stage_details(stage)

        if not stage_details:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No tracking data for stage: {stage}"
            )

        return stage_details

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get AI tracking for job {job_id} stage {stage}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get AI tracking: {str(e)}"
        )


@router.get("/job/{job_id}/ai-tracking/model/{model_name}")
async def get_job_ai_tracking_by_model(job_id: str, model_name: str):
    """
    Get AI model tracking information for a specific AI model.

    Args:
        job_id: Job identifier
        model_name: AI model name (LLAMA, Anthropic, CLIP, OpenAI)

    Returns:
        Statistics for the specified AI model
    """
    try:
        if job_id not in job_storage:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} not found"
            )

        job_info = job_storage[job_id]
        ai_tracker = job_info.get("ai_tracker")

        if not ai_tracker:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No AI tracking data available for this job"
            )

        model_stats = ai_tracker.get_model_stats(model_name)

        if not model_stats:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No tracking data for model: {model_name}"
            )

        return model_stats

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get AI tracking for job {job_id} model {model_name}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get AI tracking: {str(e)}"
        )
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        return AdvancedQueryResponse(
            original_query=request.query,
            optimized_query=results.get('optimized_query', request.query),
            query_type=request.query_type,
            results=results.get('results', []),
            total_results=results.get('total_results', 0),
            expansion_terms=results.get('expansion_terms', []),
            processing_time=processing_time,
            confidence_score=results.get('confidence_score', 0.0)
        )
        
    except ValueError as e:
        logger.error(f"Invalid query parameters: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid query parameters: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Advanced query search failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Advanced query search failed: {str(e)}"
        )


@router.get("/admin/stuck-jobs/analyze/{job_id}")
async def analyze_stuck_job(job_id: str):
    """
    Analyze a stuck job to determine root cause and get recommendations.

    Returns detailed analysis including:
    - Root cause identification
    - Bottleneck stage
    - Stage-by-stage timing analysis
    - Recovery options
    - Optimization recommendations
    """
    try:
        analysis = await stuck_job_analyzer.analyze_stuck_job(job_id)
        return JSONResponse(content=analysis)
    except Exception as e:
        logger.error(f"Failed to analyze stuck job {job_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to analyze stuck job: {str(e)}"
        )


@router.get("/admin/stuck-jobs/statistics")
async def get_stuck_job_statistics():
    """
    Get overall statistics about stuck jobs.

    Returns:
    - Total stuck jobs
    - Stage breakdown (which stages jobs get stuck at)
    - Most common stuck stage
    - Historical patterns
    """
    try:
        stats = await stuck_job_analyzer.get_stuck_job_statistics()
        return JSONResponse(content=stats)
    except Exception as e:
        logger.error(f"Failed to get stuck job statistics: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get stuck job statistics: {str(e)}"
        )