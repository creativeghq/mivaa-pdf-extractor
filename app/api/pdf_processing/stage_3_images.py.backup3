"""
Stage 3: Image Processing

Consolidated image processing pipeline that performs:
1. Image extraction from PDF
2. AI classification (material vs non-material)
3. Upload to Supabase Storage
4. Save to database
5. CLIP embeddings generation (SigLIP primary, CLIP fallback)
6. Llama Vision analysis for quality scoring

All operations are performed in batches with memory monitoring and cleanup.
"""

import logging
import os
import base64
import gc
from typing import Dict, List, Any, Optional, Set

from app.services.supabase_client import get_supabase_client
from app.services.vecs_service import VecsService
from app.services.real_embeddings_service import RealEmbeddingsService
from app.services.pdf_processor import PDFProcessor
from app.utils.timeout_guard import with_timeout, TimeoutConstants, ProgressiveTimeoutStrategy
from app.utils.circuit_breaker import CircuitBreaker, CircuitBreakerError
from app.utils.memory_monitor import global_memory_monitor as memory_monitor
from app.services.checkpoint_recovery_service import ProcessingStage as CheckpointStage

logger = logging.getLogger(__name__)


async def process_stage_3_images(
    file_content: bytes,
    document_id: str,
    workspace_id: str,
    job_id: str,
    page_count: int,
    product_pages: Set[int],
    focused_extraction: bool,
    extract_categories: List[str],
    image_analysis_model: str,
    component_manager: Any,
    loaded_components: List[str],
    tracker: Any,
    checkpoint_recovery_service: Any,
    logger: Any,
    pdf_result_with_images: Any = None
) -> Dict[str, Any]:
    """
    Stage 3: Consolidated Image Processing
    
    Performs all image operations in a single batch-processed flow:
    - Extract images from PDF (with optional page filtering)
    - AI classification (Llama/Claude)
    - Upload to Supabase Storage
    - Save to database
    - Generate CLIP embeddings (SigLIP/CLIP)
    - Llama Vision analysis for quality scoring
    
    Args:
        file_content: PDF file bytes
        document_id: Document UUID
        workspace_id: Workspace UUID
        job_id: Processing job UUID
        page_count: Total pages in PDF
        product_pages: Set of page numbers containing products
        focused_extraction: Whether to extract only from specific pages
        extract_categories: Categories to extract (e.g., ['products'])
        component_manager: Lazy loading component manager
        loaded_components: List to track loaded components
        tracker: Progress tracker
        checkpoint_recovery_service: Checkpoint service
        resource_manager: Resource cleanup manager
        pdf_result_with_images: Optional pre-extracted PDF result
        
    Returns:
        Dict with processing results and statistics
    """
    logger.info("üñºÔ∏è [STAGE 3] Image Processing - Starting...")
    await tracker.update_stage(CheckpointStage.IMAGES_EXTRACTED, stage_name="image_processing")
    
    # Initialize services
    supabase_client = get_supabase_client()
    vecs_service = VecsService()
    embedding_service = RealEmbeddingsService()
    pdf_processor = PDFProcessor()
    
    # Load LlamaIndex service for Llama Vision analysis
    logger.info("üì¶ Loading LlamaIndex service for image analysis...")
    try:
        llamaindex_service = await component_manager.load("llamaindex_service")
        loaded_components.append("llamaindex_service")
        logger.info("‚úÖ LlamaIndex service loaded")
    except Exception as e:
        logger.error(f"‚ùå Failed to load LlamaIndex service: {e}")
        raise
    
    # Counters for tracking
    images_saved_count = 0
    clip_embeddings_generated = 0
    specialized_embeddings_generated = 0
    images_processed = 0  # Images analyzed with Llama Vision
    vecs_batch_records = []
    VECS_BATCH_SIZE = 50
    
    # Circuit breakers for API calls
    clip_breaker = CircuitBreaker(failure_threshold=5, timeout_seconds=60, name="CLIP")
    llama_breaker = CircuitBreaker(failure_threshold=5, timeout_seconds=60, name="Llama")
    
    # Dynamic batch size calculation
    DEFAULT_BATCH_SIZE = 15
    BATCH_SIZE = memory_monitor.calculate_optimal_batch_size(
        default_batch_size=DEFAULT_BATCH_SIZE,
        min_batch_size=5,
        max_batch_size=20,
        memory_per_item_mb=20.0  # Estimate 20MB per image
    )
    
    # Dynamic concurrency control
    mem_stats = memory_monitor.get_memory_stats()
    if mem_stats.percent_used < 50:
        CONCURRENT_IMAGES = 8
    elif mem_stats.percent_used < 70:
        CONCURRENT_IMAGES = 5
    else:
        CONCURRENT_IMAGES = 3
    
    logger.info(f"   üîß DYNAMIC BATCH PROCESSING: {BATCH_SIZE} images per batch")
    logger.info(f"   üöÄ Concurrency level: {CONCURRENT_IMAGES} images (memory: {mem_stats.percent_used:.1f}%)")
    memory_monitor.log_memory_stats(prefix="   ")

    # Step 1: Extract images from PDF (if not already provided)
    if pdf_result_with_images is None:
        logger.info("üîÑ Starting image extraction from PDF...")
        logger.info(f"   PDF size: {len(file_content)} bytes")
        logger.info(f"   Focused extraction: {focused_extraction}")
        logger.info(f"   Extract categories: {extract_categories}")

        # Calculate estimated images for timeout
        if focused_extraction and 'products' in extract_categories and product_pages:
            estimated_images = len(product_pages) * 2
            logger.info(f"   üéØ Focused extraction: Only extracting from {len(product_pages)} product pages")
            logger.info(f"   üìÑ Product pages: {sorted(product_pages)}")
        else:
            estimated_images = page_count * 2
            logger.info(f"   üìÑ Full extraction: Extracting from all {page_count} pages")

        # Progressive timeout calculation
        image_extraction_timeout = ProgressiveTimeoutStrategy.calculate_image_processing_timeout(
            image_count=estimated_images,
            concurrent_limit=1
        )
        logger.info(f"üìä Image extraction: ~{estimated_images} estimated images ‚Üí timeout: {image_extraction_timeout:.0f}s")

        # Build processing options
        processing_options = {
            'extract_images': True,
            'extract_tables': False,
            'skip_upload': True  # Skip upload - will classify first, then upload only material images
        }

        # Add page_list for focused extraction
        if focused_extraction and 'products' in extract_categories and product_pages:
            processing_options['page_list'] = sorted(list(product_pages))
            logger.info(f"   ‚úÖ Passing page_list to PyMuPDF: {len(processing_options['page_list'])} pages")

        # Extract images with timeout
        try:
            pdf_result_with_images = await with_timeout(
                pdf_processor.process_pdf_from_bytes(
                    pdf_bytes=file_content,
                    document_id=document_id,
                    processing_options=processing_options
                ),
                timeout_seconds=image_extraction_timeout,
                operation_name="Image extraction from PDF"
            )
            logger.info(f"‚úÖ Image extraction complete: {len(pdf_result_with_images.extracted_images)} images extracted")
        except Exception as e:
            logger.error(f"‚ùå Image extraction failed: {e}")
            raise
    else:
        logger.info(f"‚úÖ Using pre-extracted images: {len(pdf_result_with_images.extracted_images)} images")

    # Get extracted images
    all_images = pdf_result_with_images.extracted_images
    if not all_images:
        logger.warning("‚ö†Ô∏è No images extracted from PDF")
        return {
            "status": "completed",
            "pdf_result_with_images": pdf_result_with_images,
            "material_images": [],
            "images_extracted": 0,
            "images_processed": 0,
            "clip_embeddings_generated": 0,
            "clip_embeddings_expected": 0,
            "clip_completion_rate": 0,
            "specialized_embeddings": 0,
            "images_analyzed": 0,
            "total_images_extracted": 0,
            "non_material_images": 0,
            "quality_flags": {
                "clip_embeddings_complete": True,  # No images = nothing to fail
                "all_images_analyzed": True,
                "specialized_embeddings_complete": True
            }
        }

    logger.info(f"üìä Total images to process: {len(all_images)}")

    # Step 2: AI Classification (Llama/Claude) to filter material images
    logger.info("ü§ñ Starting AI classification to identify material images...")

    import asyncio
    from asyncio import Semaphore

    # Semaphores for concurrency control
    llama_semaphore = Semaphore(10)  # Max 10 concurrent Llama calls
    claude_semaphore = Semaphore(3)  # Max 3 concurrent Claude calls

    material_images = []
    non_material_count = 0
    classification_errors = 0

    async def classify_single_image(img_data, index):
        """Classify a single image as material or non-material"""
        nonlocal non_material_count, classification_errors

        try:
            image_path = img_data.get('path')
            if not image_path or not os.path.exists(image_path):
                logger.warning(f"   ‚ö†Ô∏è [{index}/{len(all_images)}] Image file not found: {image_path}")
                classification_errors += 1
                return None

            # Read image file
            with open(image_path, 'rb') as f:
                image_bytes = f.read()
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')

            # Call Llama Vision for classification
            async with llama_semaphore:
                classification = await llamaindex_service._classify_image_material(
                    image_base64=image_base64,
                    confidence_threshold=0.6  # Lowered from 0.7 to reduce false negatives
                )

            if classification and classification.get('is_material', False):
                logger.info(f"   ‚úÖ [{index}/{len(all_images)}] Material image (confidence: {classification.get('confidence', 0):.2f})")
                img_data['classification'] = classification
                return img_data
            else:
                logger.info(f"   ‚è≠Ô∏è  [{index}/{len(all_images)}] Non-material image (skipped)")
                non_material_count += 1
                return None

        except Exception as e:
            logger.error(f"   ‚ùå [{index}/{len(all_images)}] Classification failed: {e}")
            classification_errors += 1
            return None

    # Process all images in parallel with semaphore control
    classification_tasks = [
        classify_single_image(img_data, idx + 1)
        for idx, img_data in enumerate(all_images)
    ]

    classification_results = await asyncio.gather(*classification_tasks, return_exceptions=True)

    # Filter out None results and exceptions
    material_images = [
        result for result in classification_results
        if result is not None and not isinstance(result, Exception)
    ]

    logger.info(f"‚úÖ AI Classification Complete:")
    logger.info(f"   Material images: {len(material_images)}")
    logger.info(f"   Non-material images: {non_material_count}")
    logger.info(f"   Classification errors: {classification_errors}")

    if not material_images:
        logger.warning("‚ö†Ô∏è No material images identified")
        return {
            "status": "completed",
            "pdf_result_with_images": pdf_result_with_images,
            "material_images": [],
            "images_extracted": 0,
            "images_processed": 0,
            "clip_embeddings_generated": 0,
            "clip_embeddings_expected": 0,
            "clip_completion_rate": 0,
            "specialized_embeddings": 0,
            "images_analyzed": 0,
            "total_images_extracted": len(all_images),
            "non_material_images": len(all_images),
            "quality_flags": {
                "clip_embeddings_complete": True,  # No material images = nothing to fail
                "all_images_analyzed": True,
                "specialized_embeddings_complete": True
            }
        }

    # Step 3: Consolidated batch processing (Upload ‚Üí Save ‚Üí CLIP ‚Üí Llama Vision)
    logger.info(f"üöÄ Starting consolidated batch processing for {len(material_images)} material images...")
    logger.info(f"   Operations: Upload ‚Üí Save to DB ‚Üí CLIP Embeddings ‚Üí Llama Vision Analysis")

    async def process_single_image_complete(img_data, image_index, total_images):
        """Complete processing for a single image: Upload ‚Üí Save ‚Üí CLIP ‚Üí Llama Vision"""
        nonlocal images_saved_count, clip_embeddings_generated, specialized_embeddings_generated, images_processed, vecs_batch_records

        try:
            image_path = img_data.get('path')
            if not image_path or not os.path.exists(image_path):
                logger.error(f"   ‚ùå [{image_index}/{total_images}] Image file not found: {image_path}")
                return None

            # STEP 1: Upload to Supabase Storage
            logger.info(f"   üì§ [{image_index}/{total_images}] Uploading: {img_data.get('filename')}")

            upload_result = await pdf_processor._upload_image_to_storage(
                image_path,
                document_id,
                {
                    'filename': img_data.get('filename'),
                    'size_bytes': img_data.get('size_bytes'),
                    'format': img_data.get('format'),
                    'dimensions': img_data.get('dimensions'),
                    'width': img_data.get('width'),
                    'height': img_data.get('height')
                },
                None
            )

            if not upload_result.get('success'):
                logger.error(f"   ‚ùå [{image_index}/{total_images}] Upload failed: {upload_result.get('error')}")
                return None

            # Update img_data with storage info
            img_data['storage_url'] = upload_result.get('public_url')
            img_data['storage_path'] = upload_result.get('storage_path')
            img_data['storage_bucket'] = upload_result.get('storage_bucket')

            # STEP 2: Save to database
            image_id = await supabase_client.save_single_image(
                image_info=img_data,
                document_id=document_id,
                workspace_id=workspace_id,
                image_index=image_index - 1
            )

            if not image_id:
                logger.error(f"   ‚ùå [{image_index}/{total_images}] Failed to save to DB")
                return None

            img_data['id'] = image_id
            images_saved_count += 1
            logger.info(f"   ‚úÖ [{image_index}/{total_images}] Saved to DB: {image_id}")

            # STEP 3: Read image file for AI processing
            with open(image_path, 'rb') as f:
                image_bytes = f.read()
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            image_base64_with_prefix = f"data:image/jpeg;base64,{image_base64}"

            # STEP 4: Generate CLIP embeddings (SigLIP primary, CLIP fallback)
            logger.info(f"   üé® [{image_index}/{total_images}] Generating CLIP embeddings...")
            try:
                clip_result = await clip_breaker.call(
                    with_timeout,
                    embedding_service.generate_all_embeddings(
                        entity_id=image_id,
                        entity_type="image",
                        text_content="",
                        image_data=image_base64_with_prefix,
                        material_properties={}
                    ),
                    timeout_seconds=TimeoutConstants.CLIP_EMBEDDING,
                    operation_name=f"CLIP embedding (image {image_index}/{total_images})"
                )

                if clip_result and clip_result.get('success'):
                    embeddings = clip_result.get('embeddings', {})

                    # Save visual CLIP embedding to VECS (batch)
                    visual_embedding = embeddings.get('visual_512')
                    if visual_embedding:
                        vecs_batch_records.append((
                            image_id,
                            visual_embedding,
                            {
                                'document_id': document_id,
                                'workspace_id': workspace_id,
                                'page_number': img_data.get('page_number', 1),
                                'image_url': img_data.get('storage_url'),
                                'storage_path': img_data.get('storage_path')
                            }
                        ))
                        clip_embeddings_generated += 1

                    # Save specialized embeddings
                    specialized_embeddings = {}
                    for emb_type in ['color_512', 'texture_512', 'style_512', 'material_512']:
                        if embeddings.get(emb_type):
                            key = emb_type.replace('_512', '')
                            specialized_embeddings[key] = embeddings.get(emb_type)

                    if specialized_embeddings:
                        await vecs_service.upsert_specialized_embeddings(
                            image_id=image_id,
                            embeddings=specialized_embeddings,
                            metadata={'document_id': document_id, 'page_number': img_data.get('page_number', 1)}
                        )
                        specialized_embeddings_generated += len(specialized_embeddings)

                    logger.info(f"   ‚úÖ [{image_index}/{total_images}] Generated {1 + len(specialized_embeddings)} CLIP embeddings")

                    # Batch upsert VECS records
                    if len(vecs_batch_records) >= VECS_BATCH_SIZE:
                        batch_count = await vecs_service.batch_upsert_image_embeddings(vecs_batch_records)
                        logger.info(f"   üíæ Batch upserted {batch_count} CLIP embeddings to VECS")
                        vecs_batch_records.clear()

            except CircuitBreakerError as cb_error:
                logger.warning(f"   ‚ö†Ô∏è [{image_index}/{total_images}] CLIP skipped (circuit breaker): {cb_error}")
            except Exception as clip_error:
                logger.error(f"   ‚ùå [{image_index}/{total_images}] CLIP failed: {clip_error}")

            # STEP 5: Llama Vision analysis for quality scoring
            logger.info(f"   üîç [{image_index}/{total_images}] Analyzing with Llama Vision...")
            try:
                analysis_result = await llama_breaker.call(
                    with_timeout,
                    llamaindex_service._analyze_image_material(
                        image_base64=image_base64,
                        image_path=image_path,
                        image_id=image_id,
                        document_id=document_id
                    ),
                    timeout_seconds=TimeoutConstants.LLAMA_VISION_CALL,
                    operation_name=f"Llama Vision (image {image_index}/{total_images})"
                )

                if analysis_result:
                    img_data['quality_score'] = analysis_result.get('quality_score', 0.5)
                    img_data['confidence_score'] = analysis_result.get('confidence_score', 0.5)
                    img_data['material_properties'] = analysis_result.get('material_properties', {})
                    images_processed += 1
                    logger.info(f"   ‚úÖ [{image_index}/{total_images}] Llama Vision complete (quality: {img_data['quality_score']:.2f})")

            except CircuitBreakerError as cb_error:
                logger.warning(f"   ‚ö†Ô∏è [{image_index}/{total_images}] Llama Vision skipped (circuit breaker): {cb_error}")
            except Exception as llama_error:
                logger.error(f"   ‚ùå [{image_index}/{total_images}] Llama Vision failed: {llama_error}")

            # Memory optimization: Clear image data immediately
            del image_bytes
            del image_base64
            del image_base64_with_prefix

            return img_data

        except Exception as e:
            logger.error(f"   ‚ùå [{image_index}/{total_images}] Failed to process image: {e}")
            import traceback
            logger.error(f"   Traceback: {traceback.format_exc()}")
            return None

    # Process images in batches
    total_batches = (len(material_images) + BATCH_SIZE - 1) // BATCH_SIZE
    logger.info(f"üì¶ Processing {len(material_images)} images in {total_batches} batches of {BATCH_SIZE}")

    processing_semaphore = Semaphore(CONCURRENT_IMAGES)

    for batch_num in range(total_batches):
        start_idx = batch_num * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(material_images))
        batch_images = material_images[start_idx:end_idx]

        logger.info(f"üîÑ Processing batch {batch_num + 1}/{total_batches} ({len(batch_images)} images)...")

        # Check memory pressure before processing batch
        mem_stats = global_memory_monitor.get_memory_stats()
        if mem_stats.is_critical_pressure:
            logger.error(f"üî¥ CRITICAL memory pressure: {mem_stats.percent_used:.1f}%")
            global_memory_monitor.send_memory_alert(mem_stats, "critical")
            global_memory_monitor.trigger_emergency_cleanup()
            # Re-check after cleanup
            mem_stats = global_memory_monitor.get_memory_stats()
        elif mem_stats.is_high_pressure:
            logger.warning(f"‚ö†Ô∏è High memory pressure: {mem_stats.percent_used:.1f}%")
            global_memory_monitor.send_memory_alert(mem_stats, "warning")
        
        # Check memory before batch
        mem_stats = memory_monitor.get_memory_stats()
        logger.info(f"   üíæ Memory before batch: {mem_stats.used_mb:.1f} MB ({mem_stats.percent_used:.1f}%)")

        # Process batch in parallel with semaphore control
        async def process_with_semaphore(img_data, idx):
            async with processing_semaphore:
                return await process_single_image_complete(img_data, start_idx + idx + 1, len(material_images))

        batch_tasks = [
            process_with_semaphore(img_data, idx)
            for idx, img_data in enumerate(batch_images)
        ]

        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

        # Garbage collection after batch
        gc.collect()
        
        # Unload CLIP model to free memory (will reload on next use)
        try:
            embedding_service.unload_clip_model()
        except Exception as e:
            logger.warning(f"Failed to unload CLIP model: {e}")
        
        mem_after = memory_monitor.get_memory_stats()
        mem_freed = mem_stats.used_mb - mem_after.used_mb
        logger.info(f"   üíæ Memory after batch: {mem_after.used_mb:.1f} MB (freed: {mem_freed:.1f} MB)")

        # Check memory pressure
        if mem_after.is_high_pressure:
            logger.warning(f"   ‚ö†Ô∏è High memory pressure detected: {mem_after.percent_used:.1f}%")

        logger.info(f"   ‚úÖ Batch {batch_num + 1}/{total_batches} complete: {clip_embeddings_generated} CLIP embeddings generated so far")

        # Update progress after each batch
        await tracker.update_database_stats(
            images_stored=images_saved_count,
            clip_embeddings_generated=clip_embeddings_generated
        )
        await tracker._sync_to_database(stage="image_processing")

    # Final VECS batch upsert for remaining records
    if vecs_batch_records:
        batch_count = await vecs_service.batch_upsert_image_embeddings(vecs_batch_records)
        logger.info(f"üíæ Final batch upserted {batch_count} CLIP embeddings to VECS")
        vecs_batch_records.clear()

    # Register files for cleanup (but don't release yet - will be cleaned up at end of pipeline)
    for img_data in material_images:
        image_path = img_data.get('path')
        if image_path:
            await resource_manager.register_resource(
                resource_id=f"image_{os.path.basename(image_path)}",
                resource_path=image_path,
                resource_type="temp_file",
                job_id=job_id
            )

    logger.info(f"‚úÖ [STAGE 3] Image Processing Complete:")
    logger.info(f"   Images saved to DB: {images_saved_count}")
    logger.info(f"   CLIP embeddings generated: {clip_embeddings_generated}")
    logger.info(f"   Specialized embeddings: {specialized_embeddings_generated}")
    logger.info(f"   Llama Vision analyzed: {images_processed}")

    # Sync tracker to database
    await tracker._sync_to_database(stage="image_processing")

    # Create IMAGES_EXTRACTED checkpoint
    await checkpoint_recovery_service.create_checkpoint(
        job_id=job_id,
        stage=CheckpointStage.IMAGES_EXTRACTED,
        data={
            "document_id": document_id,
            "images_saved": images_saved_count,
            "clip_embeddings": clip_embeddings_generated,
            "specialized_embeddings": specialized_embeddings_generated,
            "images_analyzed": images_processed
        },
        metadata={
            "total_images_extracted": len(all_images),
            "material_images": len(material_images),
            "non_material_images": non_material_count,
            "classification_errors": classification_errors
        }
    )

    # Force garbage collection
    gc.collect()
    logger.info("üíæ Memory freed after Stage 3 (Image Processing)")

    # Calculate quality metrics
    expected_clip_embeddings = images_saved_count * 5  # 5 types per image
    clip_completion_rate = (clip_embeddings_generated / expected_clip_embeddings) if expected_clip_embeddings > 0 else 0
    
    quality_flags = {
        "clip_embeddings_complete": clip_completion_rate >= 0.9,  # 90% threshold
        "all_images_analyzed": images_processed == images_saved_count,
        "specialized_embeddings_complete": specialized_embeddings_generated >= (images_saved_count * 4)  # 4 specialized types
    }
    
    logger.info(f"üìä Quality Metrics:")
    logger.info(f"   CLIP Completion Rate: {clip_completion_rate:.1%} ({clip_embeddings_generated}/{expected_clip_embeddings})")
    logger.info(f"   Images Analyzed: {images_processed}/{images_saved_count}")
    logger.info(f"   Quality Flags: {quality_flags}")
    
    return {
        "status": "completed",
        "pdf_result_with_images": pdf_result_with_images,
        "material_images": material_images,
        "images_extracted": images_saved_count,
        "images_processed": images_processed,
        "clip_embeddings_generated": clip_embeddings_generated,
        "clip_embeddings_expected": expected_clip_embeddings,
        "clip_completion_rate": clip_completion_rate,
        "specialized_embeddings": specialized_embeddings_generated,
        "images_analyzed": images_processed,
        "total_images_extracted": len(all_images),
        "non_material_images": non_material_count,
        "quality_flags": quality_flags
    }

