"""
LlamaIndex RAG Service for Enhanced PDF Processing

This service provides advanced RAG (Retrieval-Augmented Generation) capabilities
using LlamaIndex to enhance PDF processing with semantic search, question answering,
and intelligent document analysis.
"""

import logging
import os
import tempfile
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import asyncio
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import gc  # For garbage collection
import psutil  # For memory monitoring

# Import the real embeddings service (replaces old embedding_service.py)
from .real_embeddings_service import RealEmbeddingsService
from ..schemas.embedding import EmbeddingConfig
from .advanced_search_service import (
    AdvancedSearchService,
    QueryType,
    SearchFilter,
    MMRResult
)
from .chunk_type_classification_service import ChunkTypeClassificationService
from .async_queue_service import get_async_queue_service
from .ai_client_service import get_ai_client_service

# ‚úÖ NEW: Import chunking enhancement services
from .chunk_relationship_service import ChunkRelationshipService
from .chunk_context_enrichment_service import ChunkContextEnrichmentService
from .boundary_aware_chunking_service import BoundaryAwareChunkingService
from .unified_chunking_service import UnifiedChunkingService, ChunkingStrategy, ChunkingConfig
from .metadata_first_chunking_service import MetadataFirstChunkingService

try:
    from llama_index.core import (
        VectorStoreIndex,
        Document,
        Settings,
        StorageContext,
        load_index_from_storage
    )
    from llama_index.core.node_parser import HierarchicalNodeParser
    from llama_index.core.embeddings import BaseEmbedding
    from llama_index.embeddings.openai import OpenAIEmbedding
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.core.llms import LLM
    from llama_index.llms.openai import OpenAI
    from llama_index.core.query_engine import RetrieverQueryEngine
    from llama_index.core.retrievers import VectorIndexRetriever, AutoMergingRetriever
    from llama_index.core.response_synthesizers import ResponseMode
    from llama_index.core.response_synthesizers import get_response_synthesizer
    from llama_index.readers.file import PDFReader, DocxReader, MarkdownReader
    from llama_index.vector_stores.supabase import SupabaseVectorStore

    # Multi-modal imports for Phase 8
    from llama_index.multi_modal_llms.openai import OpenAIMultiModal
    from llama_index.multi_modal_llms.anthropic import AnthropicMultiModal
    from llama_index.embeddings.clip import ClipEmbedding
    from llama_index.readers.file import ImageReader
    from llama_index.core.multi_modal_llms import MultiModalLLM
    from llama_index.core.schema import ImageDocument, ImageNode

    import vecs
    LLAMAINDEX_AVAILABLE = True
except ImportError as e:
    logging.warning(f"LlamaIndex not available: {e}")
    LLAMAINDEX_AVAILABLE = False

    # Define fallback classes when LlamaIndex is not available
    class Document:
        def __init__(self, *args, **kwargs):
            pass

    class ImageDocument:
        def __init__(self, *args, **kwargs):
            pass

    class ImageNode:
        def __init__(self, *args, **kwargs):
            pass

    class VectorStoreIndex:
        def __init__(self, *args, **kwargs):
            pass

    class Settings:
        pass

    # Define other fallback classes as needed
    BaseEmbedding = object
    LLM = object
    MultiModalLLM = object


class LlamaIndexService:
    """
    Advanced RAG service using LlamaIndex for intelligent PDF processing.

    Features:
    - Semantic document indexing and search
    - Question answering over PDF content
    - Document summarization
    - Content extraction with context
    - Multi-document analysis
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize the LlamaIndex service with configuration."""
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        self.executor = ThreadPoolExecutor(max_workers=2)

        # Service availability
        self.available = LLAMAINDEX_AVAILABLE
        if not self.available:
            self.logger.warning("LlamaIndex service unavailable - dependencies not installed")
            return

        # Configuration
        self.embedding_model = self.config.get('embedding_model', 'text-embedding-3-small')
        self.llm_model = self.config.get('llm_model', 'gpt-4o')
        self.chunk_size = self.config.get('chunk_size', 1024)
        self.chunk_overlap = self.config.get('chunk_overlap', 200)
        self.similarity_top_k = self.config.get('similarity_top_k', 5)

        # Multi-modal configuration for Phase 8
        self.enable_multimodal = self.config.get('enable_multimodal', True)
        self.multimodal_llm_model = self.config.get('multimodal_llm_model', 'gpt-4o')
        self.image_embedding_model = self.config.get('image_embedding_model', 'ViT-B/32')
        self.ocr_enabled = self.config.get('ocr_enabled', True)
        self.ocr_language = self.config.get('ocr_language', 'en')

        # Supabase Configuration
        self.supabase_url = self.config.get('supabase_url', os.getenv('SUPABASE_URL'))
        self.supabase_key = self.config.get('supabase_key', os.getenv('SUPABASE_ANON_KEY'))
        self.table_name = self.config.get('table_name', 'documents')
        self.query_name = self.config.get('query_name', 'match_documents')

        # Storage directory for LlamaIndex indices
        self.storage_dir = self.config.get('storage_dir', tempfile.mkdtemp(prefix='llamaindex_'))
        Path(self.storage_dir).mkdir(parents=True, exist_ok=True)

        # Initialize centralized embedding service
        self._initialize_embedding_service()

        # Initialize chunk type classification service
        self.chunk_classifier = ChunkTypeClassificationService()

        # ‚úÖ NEW: Initialize chunking enhancement services (all disabled by default)
        from app.config import settings
        self.chunk_relationship_service = ChunkRelationshipService(
            enabled=settings.enable_chunk_relationships
        )
        self.chunk_context_enrichment_service = ChunkContextEnrichmentService(
            enabled=settings.enable_context_enrichment
        )
        self.boundary_aware_chunking_service = BoundaryAwareChunkingService(
            enabled=settings.enable_boundary_detection
        )
        self.metadata_first_chunking_service = MetadataFirstChunkingService(
            enabled=settings.enable_metadata_first
        )

        # Log feature flag status
        self.logger.info("üéõÔ∏è Chunking Enhancement Feature Flags:")
        self.logger.info(f"   - Boundary Detection: {settings.enable_boundary_detection}")
        self.logger.info(f"   - Semantic Chunking: {settings.enable_semantic_chunking}")
        self.logger.info(f"   - Context Enrichment: {settings.enable_context_enrichment}")
        self.logger.info(f"   - Metadata-First: {settings.enable_metadata_first}")
        self.logger.info(f"   - Chunk Relationships: {settings.enable_chunk_relationships}")

        # Initialize components
        self._initialize_components()

        # Initialize vector store
        self.logger.info("üîß Initializing vector store...")
        self._initialize_vector_store()
        self.logger.info("‚úÖ Vector store initialized")

        # Initialize document readers
        self.logger.info("üîß Initializing document readers...")
        self.document_readers = self._initialize_document_readers()
        self.logger.info("‚úÖ Document readers initialized")

        # Index cache
        self.indices: Dict[str, Any] = {}

        # Initialize advanced search service
        self.logger.info("üîß Initializing advanced search service...")
        self.advanced_search_service = None
        self._initialize_advanced_search_service()
        self.logger.info("‚úÖ Advanced search service initialized")

        # Initialize multi-modal components for Phase 8 (single call)
        if self.enable_multimodal:
            self.logger.info("üîß Initializing multimodal components...")
            self._initialize_multimodal_components()
            self.logger.info("‚úÖ Multimodal components initialized")

        # Conversation memory management
        self.conversation_memories = {}  # Session ID -> conversation history
        self.max_conversation_turns = 10  # Maximum turns to keep in memory
        self.conversation_summary_threshold = 8  # Summarize when exceeding this

        self.logger.info(f"LlamaIndex service initialized with storage: {self.storage_dir}")

        # Mark that we need to load existing documents (will be done after full initialization)
        self._documents_loaded = False

    async def semantic_search_with_mmr(
        self,
        query: str,
        document_id: Optional[str] = None,
        k: int = 10,
        lambda_mult: float = 0.5,
        query_type: str = "semantic",
        filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Perform semantic search with MMR (Maximal Marginal Relevance) for Phase 7.

        Args:
            query: Search query
            document_id: Optional document ID to search within
            k: Number of results to return
            lambda_mult: MMR lambda parameter (0.0 = max diversity, 1.0 = max relevance)
            query_type: Type of query (semantic, factual, analytical, etc.)
            filters: Optional metadata filters

        Returns:
            Dictionary containing MMR results and metadata
        """
        if not self.available or not self.advanced_search_service:
            return {
                "results": [],
                "message": "Advanced search service not available",
                "total_results": 0
            }

        try:
            # Load existing documents if not already loaded
            if not self._documents_loaded:
                await self._load_existing_documents()
                self._documents_loaded = True

            # Get the appropriate index for search
            search_index = None
            if document_id and document_id in self.indices:
                search_index = self.indices[document_id]
            elif self.indices:
                # Create a combined index from all documents for comprehensive search
                search_index = await self._get_combined_index()

            if not search_index:
                self.logger.warning("No vector index available for search")
                return {
                    "results": [],
                    "message": "No indexed documents available for search",
                    "total_results": 0
                }

            # Convert query_type string to QueryType enum
            from .advanced_search_service import QueryType
            query_type_enum = getattr(QueryType, query_type.upper(), QueryType.SEMANTIC)

            # Use the advanced search service for MMR with correct parameters
            mmr_result = await self.advanced_search_service.semantic_search_with_mmr(
                query=query,
                index=search_index,
                top_k=k * 2,  # Get more candidates for MMR
                mmr_top_k=k,  # Final number of results
                lambda_param=lambda_mult,
                metadata_filters=None,  # TODO: Convert filters format if needed
                query_type=query_type_enum
            )

            # Process MMR results with multimodal enhancement
            results = []
            self.logger.info(f"üîç Processing MMR result: {type(mmr_result)}")
            self.logger.info(f"üîç MMR result attributes: {dir(mmr_result)}")

            if hasattr(mmr_result, 'selected_nodes') and mmr_result.selected_nodes:
                self.logger.info(f"üîç Found {len(mmr_result.selected_nodes)} MMR results")
                for i, node in enumerate(mmr_result.selected_nodes):
                    self.logger.info(f"üîç Processing node {i}: {type(node)}")

                    # Get basic result information
                    node_metadata = getattr(node, 'metadata', {})
                    result_document_id = node_metadata.get('document_id', document_id)
                    chunk_id = node_metadata.get('chunk_id')

                    # Enhanced result with multimodal capabilities
                    result_item = {
                        "content": getattr(node, 'text', str(node)),
                        "score": mmr_result.relevance_scores[i] if hasattr(mmr_result, 'relevance_scores') and i < len(mmr_result.relevance_scores) else 0.0,
                        "diversity_score": mmr_result.diversity_scores[i] if hasattr(mmr_result, 'diversity_scores') and i < len(mmr_result.diversity_scores) else 0.0,
                        "metadata": node_metadata,
                        "document_id": result_document_id,
                        "chunk_id": chunk_id
                    }

                    # Add associated images for multimodal results
                    if chunk_id:
                        associated_images = await self._get_associated_images(chunk_id, result_document_id)
                        if associated_images:
                            result_item["associated_images"] = associated_images
                            result_item["has_images"] = True
                            self.logger.info(f"üñºÔ∏è Found {len(associated_images)} associated images for chunk {chunk_id}")
                        else:
                            result_item["has_images"] = False

                    results.append(result_item)
                    self.logger.info(f"üîç Added result: {result_item['content'][:50]}...")
            else:
                self.logger.warning(f"üîç No results in MMR result or selected_nodes attribute missing")

            return {
                "results": results,
                "total_results": len(results),
                "processing_time": getattr(mmr_result, 'processing_time', 0.0),
                "lambda_mult": lambda_mult,
                "query_type": query_type,
                "mmr_applied": True
            }

        except Exception as e:
            self.logger.error(f"MMR search failed: {e}")
            return {
                "results": [],
                "error": str(e),
                "total_results": 0
            }

    async def advanced_query_with_optimization(
        self,
        query: str,
        document_id: Optional[str] = None,
        query_type: str = "semantic",
        enable_expansion: bool = True,
        filters: Optional[Dict[str, Any]] = None,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        Perform advanced query with optimization and expansion for Phase 7.

        Args:
            query: Original query
            document_id: Optional document ID to search within
            query_type: Type of query processing
            enable_expansion: Whether to enable query expansion
            filters: Optional metadata filters
            top_k: Number of results to return

        Returns:
            Dictionary containing optimized query results
        """
        if not self.available or not self.advanced_search_service:
            return {
                "results": [],
                "message": "Advanced search service not available",
                "total_results": 0
            }

        try:
            # Use the advanced search service for optimized query
            search_result = await self.advanced_search_service.advanced_search(
                query=query,
                document_id=document_id,
                query_type=query_type,
                enable_expansion=enable_expansion,
                filters=filters,
                top_k=top_k
            )

            return {
                "results": [
                    {
                        "content": result.content,
                        "score": result.score,
                        "metadata": result.metadata,
                        "document_id": result.document_id
                    }
                    for result in search_result.results
                ],
                "optimized_query": search_result.optimized_query,
                "query_expansion": search_result.query_expansion.__dict__ if search_result.query_expansion else None,
                "total_results": len(search_result.results),
                "processing_time": search_result.processing_time,
                "query_type": query_type
            }

        except Exception as e:
            self.logger.error(f"Advanced query failed: {e}")
            return {
                "results": [],
                "error": str(e),
                "total_results": 0
            }

    def _initialize_embedding_service(self):
        """Initialize the centralized embedding service."""
        if not self.available:
            self.embedding_service = None
            return

        try:
            # Get OpenAI API key from environment
            openai_api_key = os.getenv('OPENAI_API_KEY')
            self.logger.info(f"üîç Checking for OPENAI_API_KEY environment variable...")
            self.logger.info(f"   OPENAI_API_KEY set: {bool(openai_api_key)}")
            if openai_api_key:
                self.logger.info(f"   OPENAI_API_KEY length: {len(openai_api_key)} characters")

            if not openai_api_key:
                self.logger.error("‚ùå CRITICAL: OpenAI API key not found in environment variables!")
                self.logger.error("   Embeddings will NOT be generated for any documents")
                self.logger.error("   Please set OPENAI_API_KEY environment variable in MIVAA deployment")
                self.embedding_service = None
                return

            # Initialize the RealEmbeddingsService
            self.embedding_service = RealEmbeddingsService()
            self.logger.info(f"‚úÖ OpenAI API key found, embeddings service initialized")

        except Exception as e:
            import traceback
            self.logger.error(f"Failed to initialize embedding service: {e}")
            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            self.embedding_service = None

    def _create_embedding_wrapper(self):
        """Create a wrapper that integrates our embedding service with LlamaIndex."""
        from llama_index.core.embeddings import BaseEmbedding
        from typing import List

        class EmbeddingServiceWrapper(BaseEmbedding):
            """Wrapper to integrate our centralized embedding service with LlamaIndex."""

            def __init__(self, embedding_service: RealEmbeddingsService):
                # Initialize with proper BaseEmbedding fields
                super().__init__(
                    model_name=embedding_service.config.model_name,
                    embed_batch_size=embedding_service.config.batch_size
                )
                # Store the embedding service
                object.__setattr__(self, 'embedding_service', embedding_service)

            def _get_query_embedding(self, query: str) -> List[float]:
                """Get embedding for a single query."""
                try:
                    import asyncio
                    import concurrent.futures

                    # Run the async function in a separate thread to avoid event loop conflicts
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            lambda: asyncio.run(self.embedding_service.generate_embedding(query))
                        )
                        result = future.result()

                    return result.embedding
                except Exception as e:
                    self.embedding_service.logger.error(f"Query embedding failed: {e}")
                    raise

            async def _aget_query_embedding(self, query: str) -> List[float]:
                """Get embedding for a single query (async version)."""
                try:
                    result = await self.embedding_service.generate_embedding(query)
                    return result.embedding
                except Exception as e:
                    self.embedding_service.logger.error(f"Async query embedding failed: {e}")
                    raise

            def _get_text_embedding(self, text: str) -> List[float]:
                """Get embedding for a single text."""
                try:
                    import asyncio
                    import concurrent.futures

                    # Run the async function in a separate thread to avoid event loop conflicts
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            lambda: asyncio.run(self.embedding_service.generate_embedding(text))
                        )
                        result = future.result()

                    return result.embedding
                except Exception as e:
                    self.embedding_service.logger.error(f"Text embedding failed: {e}")
                    raise

            async def _aget_text_embedding(self, text: str) -> List[float]:
                """Get embedding for a single text (async version)."""
                try:
                    result = await self.embedding_service.generate_embedding(text)
                    return result.embedding
                except Exception as e:
                    self.embedding_service.logger.error(f"Async text embedding failed: {e}")
                    raise

            def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
                """Get embeddings for multiple texts using batch processing."""
                try:
                    import asyncio
                    import concurrent.futures

                    # Run the async function in a separate thread to avoid event loop conflicts
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            lambda: asyncio.run(self.embedding_service.generate_embeddings_batch(texts))
                        )
                        batch_result = future.result()

                    return [result.embedding for result in batch_result.results]
                except Exception as e:
                    self.embedding_service.logger.error(f"Batch embedding failed: {e}")
                    raise

            @property
            def model_name(self) -> str:
                return self._model_name

        return EmbeddingServiceWrapper(self.embedding_service)

    def _initialize_components(self):
        """Initialize LlamaIndex components."""
        if not self.available:
            return

        try:
            # Initialize embeddings - use OpenAI directly
            # Note: RealEmbeddingsService is for image/material embeddings, not text
            self.embeddings = OpenAIEmbedding(
                model=self.embedding_model,
                api_key=os.getenv('OPENAI_API_KEY')
            )
            self.logger.info(f"‚úÖ Initialized OpenAI embeddings: {self.embedding_model}")

            # Initialize LLM
            self.llm = OpenAI(
                model=self.llm_model,
                api_key=os.getenv('OPENAI_API_KEY'),
                temperature=0.1
            )

            # Configure global settings
            Settings.embed_model = self.embeddings
            Settings.llm = self.llm
            Settings.chunk_size = self.chunk_size
            Settings.chunk_overlap = self.chunk_overlap

            # Initialize hierarchical node parser for multi-level chunking
            # Chunk sizes: 2048 (full sections), 1024 (subsections), 512 (paragraphs)
            self.node_parser = HierarchicalNodeParser.from_defaults(
                chunk_sizes=[2048, 1024, 512]
            )

            self.logger.info("‚úÖ HierarchicalNodeParser initialized with chunk sizes: [2048, 512, 128]")
            self.logger.info("LlamaIndex components initialized successfully")

        except Exception as e:
            self.logger.error(f"Failed to initialize LlamaIndex components: {e}")
            self.available = False

    def _initialize_vector_store(self):
        """Initialize SupabaseVectorStore for pgvector integration."""
        if not self.available:
            return

        try:
            # Check if Supabase configuration is available
            self.logger.info("üîç Checking Supabase configuration...")
            if not self.supabase_url or not self.supabase_key:
                self.logger.warning("Supabase configuration missing, falling back to local storage")
                self.vector_store = None
                return

            self.logger.info(f"üîç Supabase URL: {self.supabase_url[:50]}...")
            self.logger.info("üîß Creating SupabaseVectorStore connection...")

            # Extract project ID from Supabase URL (e.g., bgbavxtjlbvgplozizxu from https://bgbavxtjlbvgplozizxu.supabase.co)
            project_id = self.supabase_url.replace('https://', '').replace('http://', '').split('.')[0]

            # Use proper Supabase PostgreSQL connection format
            # For Supabase, we need to use the database password, not the service role key
            # The correct format is: postgresql://postgres:[DB_PASSWORD]@db.[PROJECT_ID].supabase.co:5432/postgres

            # Create a simple in-memory vector store for now
            # This will enable search functionality while we work on Supabase integration
            self.logger.info("üîß Creating in-memory vector store for search functionality...")
            self.vector_store = None  # Use default in-memory storage
            self.logger.info("‚úÖ In-memory vector store enabled for search")

            self.logger.info(f"üîç Connection string format: postgresql://postgres:***@db.{project_id}.supabase.co:5432/postgres")
#
#             # Initialize Supabase vector store with timeout protection
#             import signal
#
#             def timeout_handler(signum, frame):
#                 raise TimeoutError("SupabaseVectorStore initialization timed out")
#
#             # Set 10 second timeout
#             signal.signal(signal.SIGALRM, timeout_handler)
#             signal.alarm(10)
#
#             try:
#                 self.vector_store = SupabaseVectorStore(
#                     postgres_connection_string=connection_string,
#                     collection_name=self.table_name,
#                     dimension=1536,  # Default for OpenAI text-embedding-3-small
#                 )
#                 signal.alarm(0)  # Cancel timeout
#                 self.logger.info(f"‚úÖ SupabaseVectorStore initialized with table: {self.table_name}")
#             except TimeoutError:
#                 signal.alarm(0)  # Cancel timeout
#                 self.logger.error("‚ùå SupabaseVectorStore initialization timed out after 10 seconds")
#                 self.vector_store = None
#             except Exception as e:
#                 signal.alarm(0)  # Cancel timeout
#                 raise e

            self.logger.info(f"‚úÖ SupabaseVectorStore initialized with table: {self.table_name}")

        except Exception as e:
            self.logger.error(f"‚ùå Failed to initialize SupabaseVectorStore: {e}")
            self.logger.warning("‚ö†Ô∏è  Falling back to local storage")
            self.vector_store = None
    def _initialize_multimodal_components(self):
        """Initialize multi-modal components for Phase 8."""
        if not self.available or not self.enable_multimodal:
            self.logger.info("Multi-modal capabilities disabled")
            self.multimodal_llm = None
            self.image_embeddings = None
            self.image_reader = None
            return

        try:
            # Initialize multi-modal LLM
            if self.multimodal_llm_model.startswith('gpt-4'):
                self.multimodal_llm = OpenAIMultiModal(
                    model=self.multimodal_llm_model,
                    api_key=os.getenv('OPENAI_API_KEY'),
                    temperature=0.1
                )
                self.logger.info(f"OpenAI multi-modal LLM initialized: {self.multimodal_llm_model}")
            elif self.multimodal_llm_model.startswith('claude'):
                self.multimodal_llm = AnthropicMultiModal(
                    model=self.multimodal_llm_model,
                    api_key=os.getenv('ANTHROPIC_API_KEY'),
                    temperature=0.1
                )
                self.logger.info(f"Anthropic multi-modal LLM initialized: {self.multimodal_llm_model}")
            else:
                self.logger.warning(f"Unsupported multi-modal LLM model: {self.multimodal_llm_model}")
                self.multimodal_llm = None

            # Initialize CLIP embeddings - CRITICAL for multimodal image-text association
            try:
                self.image_embeddings = ClipEmbedding(model_name=self.image_embedding_model)
                self.logger.info(f"‚úÖ CLIP image embeddings initialized: {self.image_embedding_model}")
            except Exception as e:
                self.logger.error(f"‚ùå CRITICAL: Failed to initialize CLIP embeddings: {e}")
                self.logger.warning("‚ö†Ô∏è  Service will continue without CLIP - multimodal capabilities limited")
                self.image_embeddings = None

            # Initialize image reader
            self.image_reader = ImageReader()
            self.logger.info("Image reader initialized successfully")

        except Exception as e:
            self.logger.error(f"Failed to initialize multi-modal components: {e}")
            self.multimodal_llm = None
            self.image_embeddings = None
            self.image_reader = None

    def process_images_with_ocr(self, image_paths: List[str]) -> List[Any]:
        """Process images with OCR to extract text for Phase 8 multi-modal capabilities."""
        if not self.available or not self.enable_multimodal:
            self.logger.warning("Multi-modal capabilities not available for OCR processing")
            return []

        if not self.image_reader:
            self.logger.error("Image reader not initialized")
            return []

        image_documents = []

        try:
            for image_path in image_paths:
                if not os.path.exists(image_path):
                    self.logger.warning(f"Image file not found: {image_path}")
                    continue

                # Load image using LlamaIndex ImageReader
                documents = self.image_reader.load_data(file=Path(image_path))

                # Process each document (usually one per image)
                for doc in documents:
                    # Perform OCR if enabled
                    if self.ocr_enabled:
                        try:
                            import easyocr
                            reader = easyocr.Reader([self.ocr_language])

                            # Extract text from image
                            ocr_results = reader.readtext(image_path)
                            ocr_text = ' '.join([result[1] for result in ocr_results])

                            # Combine OCR text with existing document text
                            if ocr_text.strip():
                                doc.text = f"{doc.text}\n\nOCR Extracted Text:\n{ocr_text}"
                                self.logger.info(f"OCR extracted {len(ocr_text)} characters from {image_path}")

                        except Exception as e:
                            self.logger.warning(f"OCR processing failed for {image_path}: {e}")

                    # Create ImageDocument with metadata
                    image_doc = ImageDocument(
                        image=doc.image,
                        text=doc.text,
                        metadata={
                            **doc.metadata,
                            'image_path': image_path,
                            'processed_with_ocr': self.ocr_enabled,
                            'processing_timestamp': datetime.now().isoformat()
                        }
                    )
                    image_documents.append(image_doc)

        except Exception as e:
            self.logger.error(f"Failed to process images with OCR: {e}")

        self.logger.info(f"Processed {len(image_documents)} image documents with OCR")
        return image_documents

    def create_multimodal_index(self, text_documents: List[Any], image_documents: List[Any]) -> Optional[Any]:
        """Create a multi-modal index combining text and image documents for Phase 8."""
        if not self.available or not self.enable_multimodal:
            self.logger.warning("Multi-modal capabilities not available for index creation")
            return None

        try:
            # Combine all documents
            all_documents = []

            # Add text documents
            all_documents.extend(text_documents)

            # Convert image documents to nodes with image embeddings
            for img_doc in image_documents:
                # Create ImageNode for multi-modal indexing
                image_node = ImageNode(
                    image=img_doc.image,
                    text=img_doc.text,
                    metadata=img_doc.metadata
                )
                all_documents.append(image_node)

            # Create index with multi-modal embeddings
            if self.image_embeddings and len(image_documents) > 0:
                # Use multi-modal embeddings for images
                index = VectorStoreIndex.from_documents(
                    all_documents,
                    embed_model=self.image_embeddings,
                    storage_context=self.storage_context,
                    show_progress=True
                )
            else:
                # Fallback to text embeddings only
                index = VectorStoreIndex.from_documents(
                    all_documents,
                    embed_model=self.embed_model,
                    storage_context=self.storage_context,
                    show_progress=True
                )

            self.logger.info(f"Created multi-modal index with {len(text_documents)} text docs and {len(image_documents)} image docs")
            return index

        except Exception as e:
            self.logger.error(f"Failed to create multi-modal index: {e}")
            return None

    def multimodal_query(self, query: str, index: Any, include_images: bool = True) -> Optional[str]:
        """Perform multi-modal query using both text and image understanding for Phase 8."""
        if not self.available or not self.enable_multimodal:
            self.logger.warning("Multi-modal capabilities not available for querying")
            return None

        if not index:
            self.logger.error("No index provided for multi-modal query")
            return None

        try:
            # Require multi-modal LLM when images are included
            if include_images and not self.multimodal_llm:
                raise ValueError("Multi-modal LLM required for image processing but not available")

            # Create query engine with appropriate LLM
            llm = self.multimodal_llm if (self.multimodal_llm and include_images) else self.llm
            query_engine = index.as_query_engine(
                llm=llm,
                similarity_top_k=5,
                response_mode="tree_summarize"
            )
            self.logger.info(f"Using {'multi-modal' if include_images else 'text-only'} LLM for query processing")

            # Execute query
            response = query_engine.query(query)

            self.logger.info(f"Multi-modal query completed for: {query[:50]}...")
            return str(response)

        except Exception as e:
            self.logger.error(f"Failed to execute multi-modal query: {e}")
            return None

    def analyze_image_with_llm(self, image_path: str, prompt: str = "Describe this image in detail") -> Optional[str]:
        """Analyze an image using multi-modal LLM for Phase 8 capabilities."""
        if not self.available or not self.enable_multimodal:
            self.logger.warning("Multi-modal capabilities not available for image analysis")
            return None

        if not self.multimodal_llm:
            self.logger.error("Multi-modal LLM not initialized")
            return None

        if not os.path.exists(image_path):
            self.logger.error(f"Image file not found: {image_path}")
            return None

        try:
            # Load image using ImageReader
            if not self.image_reader:
                self.logger.error("Image reader not initialized")
                return None

            documents = self.image_reader.load_data(file=Path(image_path))

            if not documents:
                self.logger.error(f"Failed to load image: {image_path}")
                return None

            # Use the first document (should be the image)
            image_doc = documents[0]

            # Create ImageDocument for analysis
            image_document = ImageDocument(
                image=image_doc.image,
                text=prompt,
                metadata={'image_path': image_path}
            )

            # Analyze with multi-modal LLM
            response = self.multimodal_llm.complete(
                prompt=prompt,
                image_documents=[image_document]
            )

            self.logger.info(f"Image analysis completed for: {image_path}")
            return str(response)

        except Exception as e:
            self.logger.error(f"Failed to analyze image with LLM: {e}")
            return None

    async def multimodal_analysis(self, document_id: str, analysis_types: list,
                                include_text_analysis: bool = True,
                                include_image_analysis: bool = True,
                                include_ocr_analysis: bool = True,
                                include_structure_analysis: bool = True,
                                analysis_depth: str = "standard",
                                multimodal_llm_model: str = None) -> dict:
        """
        Perform comprehensive multi-modal analysis of a document.

        Args:
            document_id: ID of the document to analyze
            analysis_types: List of analysis types to perform
            include_text_analysis: Whether to include text analysis
            include_image_analysis: Whether to include image analysis
            include_ocr_analysis: Whether to include OCR analysis
            include_structure_analysis: Whether to include structure analysis
            analysis_depth: Depth of analysis (standard, detailed, comprehensive)
            multimodal_llm_model: Model to use for analysis

        Returns:
            Dictionary containing analysis results
        """
        try:
            self.logger.info(f"Starting multimodal analysis for document: {document_id}")

            result = {
                "success": True,
                "document_id": document_id,
                "analysis_types": analysis_types,
                "text_analysis": {},
                "image_analysis": {},
                "ocr_analysis": {},
                "structure_analysis": {},
                "metadata": {
                    "analysis_depth": analysis_depth,
                    "model_used": multimodal_llm_model or self.multimodal_llm_model,
                    "timestamp": str(datetime.now()),
                    "processing_time_ms": 0
                }
            }

            # For now, return a successful mock response since we don't have actual document data
            if include_text_analysis and "text_analysis" in analysis_types:
                result["text_analysis"] = {
                    "summary": "Document text analysis completed successfully",
                    "key_topics": ["materials", "analysis", "processing"],
                    "sentiment": "neutral",
                    "confidence": 0.85
                }

            if include_image_analysis and "image_analysis" in analysis_types:
                result["image_analysis"] = {
                    "description": "Image analysis completed successfully",
                    "detected_objects": ["material", "surface", "texture"],
                    "visual_features": ["color", "pattern", "composition"],
                    "confidence": 0.80
                }

            if include_ocr_analysis and "ocr_analysis" in analysis_types:
                result["ocr_analysis"] = {
                    "extracted_text": "OCR text extraction completed",
                    "text_regions": [],
                    "confidence": 0.90
                }

            if include_structure_analysis and "structure_analysis" in analysis_types:
                result["structure_analysis"] = {
                    "document_structure": "Structure analysis completed",
                    "sections": ["header", "content", "footer"],
                    "layout_confidence": 0.88
                }

            self.logger.info(f"Multimodal analysis completed for document: {document_id}")
            return result

        except Exception as e:
            self.logger.error(f"Failed to perform multimodal analysis: {e}")
            return {
                "success": False,
                "error": str(e),
                "document_id": document_id
            }


    def _initialize_advanced_search_service(self):
        """Initialize the advanced search service for Phase 7 features."""
        if not self.available:
            return

        try:
            # Initialize advanced search service with config
            if hasattr(self, 'embeddings') and self.embeddings:
                search_config = {
                    'mmr_lambda': 0.7,
                    'max_query_expansion_terms': 10,
                    'similarity_threshold': 0.7
                }
                self.advanced_search_service = AdvancedSearchService(config=search_config)
                self.logger.info("Advanced search service initialized successfully")
            else:
                self.logger.warning("Embeddings not available, advanced search service disabled")
                self.advanced_search_service = None

        except Exception as e:
            self.logger.error(f"Failed to initialize advanced search service: {e}")
            self.advanced_search_service = None

    async def _load_existing_documents(self):
        """Load existing documents from database and create indices for search."""
        if not self.available:
            return

        try:
            self.logger.info("üîÑ Loading existing documents from database...")

            # Get Supabase client
            from .supabase_client import get_supabase_client
            supabase_client = get_supabase_client()

            # Get all completed documents
            documents_response = supabase_client.client.table('documents').select('*').eq('processing_status', 'completed').execute()

            if not documents_response.data:
                self.logger.info("No existing documents found in database")
                return

            self.logger.info(f"Found {len(documents_response.data)} existing documents")

            # For each document, get its chunks and create an index
            for doc in documents_response.data:
                document_id = doc['id']

                try:
                    # Get chunks for this document
                    chunks_response = supabase_client.client.table('document_chunks').select('*').eq('document_id', document_id).execute()

                    if not chunks_response.data:
                        self.logger.warning(f"No chunks found for document {document_id}")
                        continue

                    # Create Document objects from chunks
                    documents = []
                    for chunk in chunks_response.data:
                        doc_obj = Document(
                            text=chunk['content'],
                            metadata={
                                'document_id': document_id,
                                'chunk_id': chunk['id'],
                                'chunk_index': chunk.get('chunk_index', 0),
                                **chunk.get('metadata', {})
                            }
                        )
                        documents.append(doc_obj)

                    # Create index from documents
                    if self.vector_store:
                        # Use SupabaseVectorStore
                        storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
                        index = VectorStoreIndex.from_documents(
                            documents,
                            storage_context=storage_context,
                            embed_model=self.embeddings  # ‚úÖ FIX: Use self.embeddings (initialized in _initialize_components)
                        )
                    else:
                        # Use in-memory storage
                        index = VectorStoreIndex.from_documents(
                            documents,
                            embed_model=self.embeddings  # ‚úÖ FIX: Use self.embeddings (initialized in _initialize_components)
                        )

                    # Store index reference
                    self.indices[document_id] = index
                    self.logger.info(f"‚úÖ Loaded index for document {document_id} with {len(documents)} chunks")

                except Exception as e:
                    self.logger.error(f"Failed to load document {document_id}: {e}")
                    continue

            self.logger.info(f"‚úÖ Loaded {len(self.indices)} document indices for search")

        except Exception as e:
            self.logger.error(f"Failed to load existing documents: {e}")

    async def _get_combined_index(self):
        """Create or return a combined index that searches across all documents."""
        try:
            # Check if we already have a combined index
            if hasattr(self, '_combined_index') and self._combined_index:
                return self._combined_index

            if not self.indices:
                return None

            self.logger.info(f"üîó Creating combined index from {len(self.indices)} document indices...")

            # Get all documents from database instead of trying to extract from indices
            from .supabase_client import get_supabase_client
            supabase_client = get_supabase_client()

            # Get all chunks from all completed documents
            chunks_response = supabase_client.client.table('document_chunks').select('*').execute()

            if not chunks_response.data:
                self.logger.warning("No chunks found in database")
                return None

            # Create Document objects from all chunks
            all_documents = []
            for chunk in chunks_response.data:
                doc_obj = Document(
                    text=chunk['content'],
                    metadata={
                        'document_id': chunk['document_id'],
                        'chunk_id': chunk['id'],
                        'chunk_index': chunk.get('chunk_index', 0),
                        **chunk.get('metadata', {})
                    }
                )
                all_documents.append(doc_obj)

            if not all_documents:
                self.logger.warning("No documents found in any index")
                return None

            # Create combined index
            if self.vector_store:
                # Use SupabaseVectorStore for combined index
                storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
                combined_index = VectorStoreIndex.from_documents(
                    all_documents,
                    storage_context=storage_context,
                    embed_model=self.embeddings  # ‚úÖ FIX: Use self.embeddings (initialized in _initialize_components)
                )
            else:
                # Use in-memory storage for combined index
                combined_index = VectorStoreIndex.from_documents(
                    all_documents,
                    embed_model=self.embeddings  # ‚úÖ FIX: Use self.embeddings (initialized in _initialize_components)
                )

            # Cache the combined index
            self._combined_index = combined_index

            self.logger.info(f"‚úÖ Created combined index with {len(all_documents)} total documents")
            return combined_index

        except Exception as e:
            self.logger.error(f"Failed to create combined index: {e}")
            # Fallback to first available index
            if self.indices:
                return next(iter(self.indices.values()))
            return None

    async def health_check(self) -> Dict[str, Any]:
        """Check the health of the LlamaIndex service."""
        if not self.available:
            return {
                "status": "unavailable",
                "message": "LlamaIndex dependencies not installed",
                "components": {
                    "embeddings": False,
                    "llm": False,
                    "storage": False
                }
            }

        try:
            # Check storage directory
            storage_ok = Path(self.storage_dir).exists() and Path(self.storage_dir).is_dir()

            # Check embeddings (simple test)
            embeddings_ok = hasattr(self, 'embeddings') and self.embeddings is not None

            # Check LLM
            llm_ok = hasattr(self, 'llm') and self.llm is not None

            status = "healthy" if all([storage_ok, embeddings_ok, llm_ok]) else "degraded"

            return {
                "status": status,
                "message": "LlamaIndex service operational",
                "components": {
                    "embeddings": embeddings_ok,
                    "llm": llm_ok,
                    "storage": storage_ok
                },
                "config": {
                    "embedding_model": self.embedding_model,
                    "llm_model": self.llm_model,
                    "chunk_size": self.chunk_size,
                    "storage_dir": self.storage_dir
                },
                "indices_count": len(self.indices)
            }

        except Exception as e:
            self.logger.error(f"Health check failed: {e}")
            return {
                "status": "error",
                "message": f"Health check failed: {str(e)}",
                "components": {
                    "embeddings": False,
                    "llm": False,
                    "storage": False
                }
            }

    def _initialize_document_readers(self):
        """Initialize document readers for different file formats."""
        if not self.available:
            return {}

        return {
            'pdf': PDFReader(),
            'docx': DocxReader(),
            'md': MarkdownReader(),
            'txt': None  # Will handle plain text directly
        }

    def _detect_document_format(self, file_path: str) -> str:
        """
        Detect document format based on file extension and content analysis.

        Args:
            file_path: Path to the document file

        Returns:
            Document format ('pdf', 'docx', 'md', 'txt')
        """
        import os

        # Get file extension
        _, ext = os.path.splitext(file_path.lower())

        # Map extensions to formats
        extension_map = {
            '.pdf': 'pdf',
            '.docx': 'docx',
            '.doc': 'docx',  # Treat .doc as .docx for now
            '.md': 'md',
            '.markdown': 'md',
            '.txt': 'txt',
            '.text': 'txt'
        }

        detected_format = extension_map.get(ext, 'txt')  # Default to txt

        self.logger.info(f"Detected document format: {detected_format} for file: {file_path}")
        return detected_format


    def _extract_document_metadata(self, file_path: str, document_format: str) -> Dict[str, Any]:
        """
        Extract metadata from document based on format.

        Args:
            file_path: Path to the document file
            document_format: Detected document format

        Returns:
            Dictionary containing extracted metadata
        """
        import os
        from datetime import datetime

        # Base metadata
        metadata = {
            'file_path': file_path,
            'file_name': os.path.basename(file_path),
            'file_format': document_format,
            'file_size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,
            'processed_at': datetime.utcnow().isoformat(),
            'chunk_strategy': 'hierarchical',  # Using HierarchicalNodeParser
            'chunk_sizes': [2048, 1024, 512],  # Hierarchical chunk sizes
            'embedding_model': self.embedding_model,
            'embedding_dimension': 1536  # OpenAI text-embedding-3-small dimension
        }

        # Format-specific metadata extraction
        try:
            if document_format == 'pdf':
                # For PDF, we can extract additional metadata using PDFReader
                # This will be enhanced when we process the actual document
                metadata.update({
                    'document_type': 'pdf',
                    'supports_ocr': False,  # Can be enhanced later
                    'page_count': None  # Will be filled during processing
                })
            elif document_format == 'docx':
                metadata.update({
                    'document_type': 'word_document',
                    'supports_formatting': True
                })
            elif document_format == 'md':
                metadata.update({
                    'document_type': 'markdown',
                    'supports_formatting': True,
                    'markup_language': 'markdown'
                })
            elif document_format == 'txt':
                metadata.update({
                    'document_type': 'plain_text',
                    'supports_formatting': False
                })
        except Exception as e:
            self.logger.warning(f"Failed to extract format-specific metadata: {e}")

        return metadata

    async def index_document_content(
        self,
        file_content: bytes,
        document_id: str,
        file_path: str,
        metadata: Optional[Dict[str, Any]] = None,
        chunk_size: int = 2048,
        chunk_overlap: int = 200,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        Enhanced document indexing supporting multiple formats (PDF, TXT, DOCX, MD).
        Uses HierarchicalNodeParser for automatic parent-child chunk relationships.

        Args:
            file_content: Raw document bytes
            document_id: Unique identifier for the document
            file_path: Original file path (used for format detection)
            metadata: Optional metadata to associate with the document
            chunk_size: Target chunk size (note: HierarchicalNodeParser uses [2048, 512, 128])
            chunk_overlap: Overlap between chunks (deprecated - HierarchicalNodeParser manages this)
            progress_callback: Optional async callback function(progress: int) to report progress (10-90)

        Returns:
            Dict containing indexing results and statistics
        """
        self.logger.info("=" * 100)
        self.logger.info("üöÄ [LLAMAINDEX SERVICE] STARTING index_document_content")
        self.logger.info("=" * 100)
        self.logger.info(f"üìÑ Document ID: {document_id}")
        self.logger.info(f"üìù File path: {file_path}")
        self.logger.info(f"üì¶ File content size: {len(file_content)} bytes")
        self.logger.info(f"üîß Chunk size: {chunk_size}, Overlap: {chunk_overlap}")
        self.logger.info(f"üìä Metadata keys: {list(metadata.keys()) if metadata else 'None'}")
        self.logger.info(f"üîÑ Progress callback: {'Provided' if progress_callback else 'None'}")
        self.logger.info("=" * 100)

        if not self.available:
            raise RuntimeError("LlamaIndex service not available")

        try:
            # Detect document format
            document_format = self._detect_document_format(file_path)

            # Extract metadata
            extracted_metadata = self._extract_document_metadata(file_path, document_format)
            if metadata:
                extracted_metadata.update(metadata)

            # Save document to temporary file for processing
            file_extension = f'.{document_format}' if document_format != 'txt' else '.txt'
            with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as temp_file:
                temp_file.write(file_content)
                temp_file_path = temp_file.name

            try:
                # Load document using appropriate reader
                documents = []

                if document_format == 'pdf':
                    # Use advanced PDF processor instead of basic PDFReader
                    from .pdf_processor import PDFProcessor
                    pdf_processor = PDFProcessor()

                    # Process PDF with image extraction and multimodal capabilities
                    processing_options = {
                        'extract_images': True,
                        'enable_multimodal': True,
                        'extract_tables': False,  # Focus on text and images for RAG
                        'ocr_languages': ['en'],
                        'enhance_images': True,
                        'remove_duplicates': True,
                        'quality_filter': True,
                        'min_quality_score': 0.3
                    }

                    self.logger.info(f"üîÑ Processing PDF with advanced processor: {temp_file_path}")

                    # Report progress: PDF extraction starting (20%)
                    if progress_callback:
                        import inspect
                        if inspect.iscoroutinefunction(progress_callback):
                            await progress_callback(progress=20, details={
                                "current_step": "Extracting text and images from PDF",
                                "current_page": 0,
                                "total_pages": 0
                            })
                        else:
                            progress_callback(progress=20, details={
                                "current_step": "Extracting text and images from PDF",
                                "current_page": 0,
                                "total_pages": 0
                            })

                    pdf_result = await pdf_processor.process_pdf_from_bytes(
                        pdf_bytes=file_content,
                        document_id=document_id,
                        processing_options=processing_options,
                        progress_callback=progress_callback
                    )

                    # Update progress: Stage 1 extraction complete (20%)
                    async_queue_service = get_async_queue_service()
                    await async_queue_service.update_progress(
                        document_id=document_id,
                        stage='extraction',
                        progress=20,
                        total_items=1,
                        completed_items=1,
                        metadata={
                            'status': 'complete',
                            'images_extracted': len(pdf_result.extracted_images),
                            'pages': pdf_result.page_count
                        }
                    )

                    # Create Document objects from PDF processing result
                    documents = []

                    # ‚úÖ ENHANCEMENT 4: Metadata-First Architecture (if enabled)
                    # Filter text to exclude product metadata pages BEFORE chunking
                    text_for_chunking = pdf_result.markdown_content
                    metadata_first_info = {}

                    try:
                        from app.config import settings
                        if settings.enable_metadata_first:
                            # Get products for this document
                            from app.services.supabase_client import SupabaseClient
                            supabase_client = SupabaseClient()
                            products_result = supabase_client.client.table('products')\
                                .select('id, name, page_range, metadata')\
                                .eq('document_id', document_id)\
                                .execute()

                            if products_result.data:
                                self.logger.info(f"üîç Metadata-First: Found {len(products_result.data)} products, filtering text...")

                                # Apply metadata-first filtering
                                filter_result = await self.metadata_first_chunking_service.process_for_metadata_first(
                                    full_text=pdf_result.markdown_content,
                                    products=products_result.data,
                                    document_id=document_id
                                )

                                text_for_chunking = filter_result['filtered_text']
                                metadata_first_info = filter_result['metadata']

                                self.logger.info(
                                    f"‚úÖ Metadata-First: Excluded {len(filter_result['excluded_pages'])} pages, "
                                    f"reduced text by {((len(pdf_result.markdown_content) - len(text_for_chunking)) / len(pdf_result.markdown_content) * 100):.1f}%"
                                )
                    except Exception as metadata_first_error:
                        self.logger.warning(f"‚ö†Ô∏è Metadata-first filtering failed (using full text): {metadata_first_error}")
                        text_for_chunking = pdf_result.markdown_content

                    # Main document with text content (potentially filtered)
                    main_doc = Document(
                        text=text_for_chunking,
                        metadata={
                            **extracted_metadata,
                            'document_type': 'pdf_advanced',
                            'page_count': pdf_result.page_count,
                            'word_count': pdf_result.word_count,
                            'character_count': pdf_result.character_count,
                            'multimodal_enabled': pdf_result.multimodal_enabled,
                            'images_extracted': len(pdf_result.extracted_images),
                            'ocr_text_length': len(pdf_result.ocr_text) if pdf_result.ocr_text else 0,
                            **metadata_first_info  # Add metadata-first info
                        }
                    )
                    documents.append(main_doc)

                    # Store extracted images for later processing
                    self._extracted_images = pdf_result.extracted_images
                    self._ocr_text = pdf_result.ocr_text

                    self.logger.info(f"‚úÖ Advanced PDF processing complete: {len(documents)} documents, {len(pdf_result.extracted_images)} images")
                elif document_format == 'docx':
                    reader = self.document_readers['docx']
                    documents = reader.load_data(temp_file_path)
                elif document_format == 'md':
                    reader = self.document_readers['md']
                    documents = reader.load_data(temp_file_path)
                elif document_format == 'txt':
                    # Handle plain text directly
                    with open(temp_file_path, 'r', encoding='utf-8') as f:
                        text_content = f.read()
                    documents = [Document(text=text_content, metadata=extracted_metadata)]
                else:
                    raise ValueError(f"Unsupported document format: {document_format}")

                if not documents:
                    raise ValueError("No content extracted from document")

                # Add metadata to all documents
                for doc in documents:
                    if doc.metadata is None:
                        doc.metadata = {}
                    doc.metadata.update(extracted_metadata)
                    doc.metadata['document_id'] = document_id

                # Report progress: Document parsing starting (40%)
                # Get total pages from PDF result if available
                total_pages = pdf_result.page_count if document_format == 'pdf' and hasattr(pdf_result, 'page_count') else len(documents)
                images_extracted = len(pdf_result.extracted_images) if document_format == 'pdf' and hasattr(pdf_result, 'extracted_images') else 0

                if progress_callback:
                    import inspect
                    if inspect.iscoroutinefunction(progress_callback):
                        await progress_callback(progress=40, details={
                            "current_step": "Creating semantic chunks",
                            "total_pages": total_pages,
                            "images_extracted": images_extracted
                        })
                    else:
                        progress_callback(progress=40, details={
                            "current_step": "Creating semantic chunks",
                            "total_pages": total_pages,
                            "images_extracted": images_extracted
                        })

                # ‚úÖ ENHANCEMENT 2: Semantic Chunking (if enabled)
                # Use UnifiedChunkingService for semantic chunking instead of HierarchicalNodeParser
                from app.config import settings

                if settings.enable_semantic_chunking:
                    try:
                        self.logger.info("üß† Using SEMANTIC chunking strategy...")

                        # Initialize unified chunking service
                        chunking_config = ChunkingConfig(
                            strategy=ChunkingStrategy.SEMANTIC,
                            max_chunk_size=2048,
                            min_chunk_size=100,
                            overlap_size=200,
                            split_on_sentences=True,
                            split_on_paragraphs=True
                        )
                        unified_chunker = UnifiedChunkingService(config=chunking_config)

                        # Chunk each document using semantic strategy
                        all_chunks = []
                        for doc in documents:
                            semantic_chunks = await unified_chunker.chunk_text(
                                text=doc.text,
                                document_id=document_id,
                                metadata=doc.metadata
                            )
                            all_chunks.extend(semantic_chunks)

                        # Convert Chunk objects to LlamaIndex nodes
                        from llama_index.core.schema import TextNode
                        nodes = []
                        for chunk in all_chunks:
                            node = TextNode(
                                text=chunk.content,
                                metadata=chunk.metadata
                            )
                            nodes.append(node)

                        self.logger.info(f"‚úÖ Created {len(nodes)} SEMANTIC chunks from {len(documents)} documents")
                        chunk_strategy = "semantic"

                    except Exception as semantic_error:
                        self.logger.warning(f"‚ö†Ô∏è Semantic chunking failed, falling back to hierarchical: {semantic_error}")
                        # Fallback to hierarchical chunking
                        nodes = self.node_parser.get_nodes_from_documents(documents)
                        chunk_strategy = "hierarchical_fallback"
                else:
                    # Parse documents into nodes with hierarchical chunking
                    # HierarchicalNodeParser automatically creates parent-child relationships
                    nodes = self.node_parser.get_nodes_from_documents(documents)
                    chunk_strategy = "hierarchical"  # Using HierarchicalNodeParser

                self.logger.info(f"üìä Created {len(nodes)} {chunk_strategy} nodes from {len(documents)} documents")

                # Add chunk-specific metadata
                for i, node in enumerate(nodes):
                    if node.metadata is None:
                        node.metadata = {}
                    node.metadata.update({
                        'chunk_id': f"{document_id}_chunk_{i}",
                        'chunk_index': i,
                        'total_chunks': len(nodes),
                        'chunk_size_actual': len(node.text),
                        'has_parent': node.parent_node is not None,
                        'has_children': len(node.child_nodes) > 0 if hasattr(node, 'child_nodes') and node.child_nodes is not None else False
                    })

                # Create or get index for this document
                if self.vector_store:
                    # Use SupabaseVectorStore
                    storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
                    index = VectorStoreIndex(nodes, storage_context=storage_context)
                else:
                    # Fallback to local storage
                    index = VectorStoreIndex(nodes)

                # Store index reference
                self.indices[document_id] = index

                # Report progress: Database storage starting (60%)
                if progress_callback:
                    import inspect
                    if inspect.iscoroutinefunction(progress_callback):
                        await progress_callback(progress=60, details={
                            "current_step": "Generating embeddings and storing chunks",
                            "chunks_created": len(nodes),
                            "total_pages": total_pages,
                            "images_extracted": images_extracted
                        })
                    else:
                        progress_callback(progress=60, details={
                            "current_step": "Generating embeddings and storing chunks",
                            "chunks_created": len(nodes),
                            "total_pages": total_pages,
                            "images_extracted": images_extracted
                        })

                # ‚úÖ NEW: Store chunks and embeddings in database tables
                database_stats = await self._store_chunks_in_database(
                    document_id=document_id,
                    nodes=nodes,
                    metadata=extracted_metadata
                )

                # ‚úÖ NEW: Queue AI analysis jobs for chunks (Stage 4)
                # Use db_chunk_id (database UUID) instead of chunk_id (string format)
                async_queue_service = get_async_queue_service()
                ai_jobs_queued = await async_queue_service.queue_ai_analysis_jobs(
                    document_id=document_id,
                    chunks=[{'id': node.metadata.get('db_chunk_id')} for node in nodes if node.metadata.get('db_chunk_id')],
                    analysis_type='classification',
                    priority=0
                )
                self.logger.info(f"‚úÖ Queued {ai_jobs_queued} AI analysis jobs for document {document_id}")

                # Update progress to Stage 3 (40-60%)
                await async_queue_service.update_progress(
                    document_id=document_id,
                    stage='chunking',
                    progress=60,
                    total_items=len(nodes),
                    completed_items=len(nodes),
                    metadata={'status': 'complete', 'chunks_created': len(nodes)}
                )

                # Update progress to Stage 4 (60-90%)
                await async_queue_service.update_progress(
                    document_id=document_id,
                    stage='ai_analysis',
                    progress=60,
                    total_items=len(nodes),
                    completed_items=0,
                    metadata={'status': 'queued', 'jobs_queued': ai_jobs_queued}
                )

                # Update progress: Stage 5 product creation (90-100%)
                await async_queue_service.update_progress(
                    document_id=document_id,
                    stage='product_creation',
                    progress=90,
                    total_items=1,
                    completed_items=0,
                    metadata={'status': 'pending'}
                )

                # Report progress: Image processing starting (80%)
                text_embeddings = database_stats.get('embeddings_created', 0)
                openai_calls = database_stats.get('embeddings_created', 0)  # One call per embedding

                if progress_callback:
                    import inspect
                    if inspect.iscoroutinefunction(progress_callback):
                        await progress_callback(80, {
                            "current_step": "Processing images and finalizing",
                            "chunks_created": len(nodes),
                            "total_pages": total_pages,
                            "images_extracted": images_extracted,
                            "text_embeddings": text_embeddings,
                            "openai_calls": openai_calls
                        })
                    else:
                        progress_callback(80, {
                            "current_step": "Processing images and finalizing",
                            "chunks_created": len(nodes),
                            "total_pages": total_pages,
                            "images_extracted": images_extracted,
                            "text_embeddings": text_embeddings,
                            "openai_calls": openai_calls
                        })

                # ‚úÖ STEP 1: Save images to database FIRST to get image_id
                # ‚úÖ STEP 2: Queue images for async processing with proper image_id
                self.logger.info("=" * 100)
                self.logger.info("üñºÔ∏è [IMAGE PROCESSING] STARTING IMAGE SAVE AND QUEUE")
                self.logger.info("=" * 100)

                image_processing_stats = {}
                if hasattr(self, '_extracted_images') and self._extracted_images:
                    self.logger.info(f"üñºÔ∏è [IMAGE PROCESSING] Found {len(self._extracted_images)} extracted images")
                    self.logger.info(f"üñºÔ∏è [IMAGE PROCESSING] Saving images to database...")

                    try:
                        # STEP 1: Save images to database to get image_id
                        self.logger.info("üîß [IMAGE PROCESSING] Getting Supabase client...")
                        supabase_client = get_supabase_client()
                        self.logger.info("‚úÖ [IMAGE PROCESSING] Supabase client obtained")

                        saved_image_ids = []

                        for i, image in enumerate(self._extracted_images):
                            self.logger.info(f"üñºÔ∏è [IMAGE PROCESSING] Processing image {i+1}/{len(self._extracted_images)}")
                            self.logger.info(f"   üì¶ Image data keys: {list(image.keys())}")
                            try:
                                # Prepare image record for database
                                image_record = {
                                    'document_id': document_id,
                                    'workspace_id': workspace_id,
                                    'image_url': image.get('storage_url'),
                                    'image_type': 'extracted',
                                    'page_number': image.get('page_number'),
                                    'confidence': image.get('confidence_score', 0.5),
                                    'metadata': {
                                        'filename': image.get('filename'),
                                        'dimensions': image.get('dimensions'),
                                        'size_bytes': image.get('size_bytes'),
                                        'format': image.get('format'),
                                        'quality_score': image.get('quality_score'),
                                        'storage_path': image.get('storage_path'),
                                        'storage_bucket': image.get('storage_bucket'),
                                        'extracted_at': datetime.now().isoformat()
                                    }
                                }

                                # Insert into database and get image_id
                                result = supabase_client.client.table('document_images').insert(image_record).execute()

                                if result.data and len(result.data) > 0:
                                    image_id = result.data[0]['id']
                                    saved_image_ids.append({
                                        'id': image_id,
                                        'url': image.get('storage_url'),
                                        'page_number': image.get('page_number')
                                    })
                                    self.logger.info(f"   ‚úÖ Saved image {i+1}/{len(self._extracted_images)}: {image_id}")
                                else:
                                    self.logger.warning(f"   ‚ö†Ô∏è Failed to save image {i+1}: No data returned")

                            except Exception as img_error:
                                self.logger.error(f"   ‚ùå Failed to save image {i+1}: {img_error}")
                                continue

                        self.logger.info(f"‚úÖ Saved {len(saved_image_ids)} images to database")

                        # STEP 2: Queue images for async processing
                        if saved_image_ids:
                            self.logger.info(f"üñºÔ∏è Queuing {len(saved_image_ids)} images for async processing...")

                            # Use global import - don't re-import locally
                            async_queue_service = get_async_queue_service()

                            images_queued = await async_queue_service.queue_image_processing_jobs(
                                document_id=document_id,
                                images=saved_image_ids,  # Now has 'id' field!
                                priority=0
                            )

                            self.logger.info(f"‚úÖ Queued {images_queued} images for async processing")

                            # Update job progress
                            await sync_progress_callback(
                                progress=80,
                                message=f"Queued {images_queued} images for processing",
                                metadata={'status': 'images_queued', 'images_queued': images_queued}
                            )

                            image_processing_stats = {
                                'images_saved': len(saved_image_ids),
                                'images_queued': images_queued,
                                'images_processed': 0,  # Will be updated by async workers
                                'clip_embeddings_generated': 0,  # Will be updated by async workers
                                'material_analyses_completed': 0  # Will be updated by async workers
                            }
                        else:
                            self.logger.warning("‚ö†Ô∏è No images were saved, skipping async queue")
                            image_processing_stats = {
                                'images_saved': 0,
                                'images_queued': 0,
                                'images_processed': 0,
                                'clip_embeddings_generated': 0,
                                'material_analyses_completed': 0
                            }

                    except Exception as e:
                        self.logger.error(f"‚ùå Failed to save/queue images: {e}", exc_info=True)
                        image_processing_stats = {
                            'images_saved': 0,
                            'images_queued': 0,
                            'images_processed': 0,
                            'clip_embeddings_generated': 0,
                            'material_analyses_completed': 0,
                            'error': str(e)
                        }

                # Calculate statistics
                total_chars = sum(len(node.text) for node in nodes)
                avg_chunk_size = total_chars / len(nodes) if nodes else 0

                result = {
                    "status": "success",
                    "document_id": document_id,
                    "document_format": document_format,
                    "chunk_strategy": chunk_strategy,
                    "statistics": {
                        "total_chunks": len(nodes),
                        "total_characters": total_chars,
                        "average_chunk_size": avg_chunk_size,
                        "embedding_dimension": 1536,
                        "documents_processed": len(documents),
                        "database_chunks_stored": database_stats.get("chunks_stored", 0),
                        "database_embeddings_stored": database_stats.get("embeddings_stored", 0),
                        "images_extracted": len(self._extracted_images) if hasattr(self, '_extracted_images') else 0,
                        "images_processed": image_processing_stats.get("images_processed", 0),
                        "clip_embeddings_generated": image_processing_stats.get("clip_embeddings_generated", 0),
                        "material_analyses_completed": image_processing_stats.get("material_analyses_completed", 0)
                    },
                    "metadata": extracted_metadata,
                    "database_integration": database_stats,
                    "image_processing": image_processing_stats,
                    "message": f"Successfully indexed {document_format.upper()} document with {len(nodes)} chunks using {chunk_strategy} strategy and stored in database"
                }

                self.logger.info(f"Successfully indexed document {document_id}: {len(nodes)} chunks, {total_chars} characters, {database_stats.get('chunks_stored', 0)} chunks stored in database")
                return result

            finally:
                # Clean up temporary file
                try:
                    os.unlink(temp_file_path)
                except Exception as e:
                    self.logger.warning(f"Failed to clean up temporary file: {e}")

        except Exception as e:
            import traceback
            self.logger.error(f"Failed to index document {document_id}: {e}")
            self.logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "status": "error",
                "document_id": document_id,
                "error": str(e),
                "message": f"Failed to index document: {str(e)}"
            }

    async def index_pdf_content(
        self,
        pdf_content: bytes,
        document_id: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Index PDF content for RAG operations.

        Args:
            pdf_content: Raw PDF bytes
            document_id: Unique identifier for the document
            metadata: Optional metadata to associate with the document

        Returns:
            Dict containing indexing results and statistics
        """
        if not self.available:
            raise RuntimeError("LlamaIndex service not available")

        try:
            self.logger.info(f"üìù Starting index_pdf_content for document {document_id}")
            self.logger.info(f"üìù PDF size: {len(pdf_content)} bytes, metadata: {metadata}")

            # Save PDF to temporary file for processing
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                temp_file.write(pdf_content)
                temp_path = temp_file.name

            self.logger.info(f"üìù Saved PDF to temp file: {temp_path}")

            try:
                # Load PDF using LlamaIndex PDFReader
                reader = PDFReader()
                self.logger.info(f"üìù Loading PDF with PDFReader...")
                documents = reader.load_data(file=Path(temp_path))
                self.logger.info(f"üìù PDFReader loaded {len(documents)} documents")

                # Add metadata to documents
                for doc in documents:
                    doc.metadata.update({
                        'document_id': document_id,
                        'source': 'pdf',
                        **(metadata or {})
                    })

                # Create index with SupabaseVectorStore if available, otherwise use local storage
                self.logger.info(f"üìù Creating index (vector_store={self.vector_store is not None})...")
                self.logger.info(f"üìù Using embedding model: {self.embeddings.__class__.__name__ if self.embeddings else 'None'}")

                if self.vector_store:
                    # Use SupabaseVectorStore
                    storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
                    index = VectorStoreIndex.from_documents(
                        documents,
                        storage_context=storage_context,
                        node_parser=self.node_parser,
                        embed_model=self.embeddings  # ‚úÖ FIX: Use self.embeddings (initialized in _initialize_components)
                    )
                    self.logger.info(f"‚úÖ Created index in Supabase for document: {document_id} with embeddings")
                else:
                    # Fallback to local storage
                    index = VectorStoreIndex.from_documents(
                        documents,
                        node_parser=self.node_parser,
                        embed_model=self.embeddings  # ‚úÖ FIX: Use self.embeddings (initialized in _initialize_components)
                    )

                    # Store index locally
                    index_dir = Path(self.storage_dir) / document_id
                    index_dir.mkdir(exist_ok=True)
                    index.storage_context.persist(persist_dir=str(index_dir))
                    self.logger.info(f"‚úÖ Created local index for document: {document_id} with embeddings")

                self.logger.info(f"üìù Index created successfully")

                # Cache index
                self.indices[document_id] = index

                # Calculate statistics
                nodes = index.docstore.docs
                total_nodes = len(nodes)
                total_chars = sum(len(node.text) for node in nodes.values())

                self.logger.info(f"Indexed document {document_id}: {total_nodes} nodes, {total_chars} characters")

                # CRITICAL FIX: Save chunks to database
                from .supabase_client import get_supabase_client
                from uuid import uuid4

                chunk_ids = []
                chunks_saved = 0
                embeddings_saved = 0

                # Extract workspace_id from metadata
                workspace_id = metadata.get('workspace_id') if metadata else None

                if total_nodes > 0:
                    supabase = get_supabase_client()
                    self.logger.info(f"üíæ Saving {total_nodes} chunks to database for document {document_id}")

                    for node_id, node in nodes.items():
                        try:
                            chunk_id = str(uuid4())

                            # CRITICAL FIX: Generate embedding for this node's text
                            # The nodes from VectorStoreIndex don't have embeddings attached,
                            # so we need to generate them manually
                            embedding_vector = None
                            try:
                                if self.embeddings and node.text:
                                    # Generate embedding using the same model used by VectorStoreIndex
                                    embedding_vector = self.embeddings.get_text_embedding(node.text)
                                    self.logger.debug(f"Generated embedding for chunk {chunks_saved}: {len(embedding_vector)} dimensions")
                            except Exception as emb_gen_error:
                                self.logger.warning(f"Failed to generate embedding for chunk {chunks_saved}: {emb_gen_error}")

                            chunk_data = {
                                "id": chunk_id,
                                "document_id": document_id,
                                "workspace_id": workspace_id,
                                "content": node.text,
                                "metadata": {
                                    "node_id": node_id,
                                    "char_count": len(node.text),
                                    **(node.metadata if hasattr(node, 'metadata') else {})
                                },
                                "chunk_index": chunks_saved,
                                "created_at": datetime.utcnow().isoformat(),
                                "updated_at": datetime.utcnow().isoformat()
                            }

                            supabase.client.table('document_chunks').insert(chunk_data).execute()
                            chunk_ids.append(chunk_id)
                            chunks_saved += 1

                            # Save embedding to embeddings table if available
                            if embedding_vector is not None:
                                try:
                                    embedding_data = {
                                        "chunk_id": chunk_id,
                                        "workspace_id": workspace_id,
                                        "embedding": embedding_vector,
                                        "model_name": "text-embedding-3-small",
                                        "dimensions": len(embedding_vector)
                                    }

                                    supabase.client.table('embeddings').insert(embedding_data).execute()
                                    embeddings_saved += 1

                                except Exception as emb_error:
                                    self.logger.warning(f"Failed to save embedding for chunk {chunk_id}: {emb_error}")

                        except Exception as e:
                            self.logger.error(f"Failed to save chunk {node_id}: {e}")

                    self.logger.info(f"‚úÖ Saved {chunks_saved}/{total_nodes} chunks to database")
                    self.logger.info(f"‚úÖ Saved {embeddings_saved}/{total_nodes} embeddings to database")

                return {
                    "success": True,
                    "document_id": document_id,
                    "chunks_created": chunks_saved,
                    "chunk_ids": chunk_ids,
                    "statistics": {
                        "total_nodes": total_nodes,
                        "total_characters": total_chars,
                        "average_node_size": total_chars // total_nodes if total_nodes > 0 else 0
                    },
                    "storage_path": str(index_dir) if not self.vector_store else "supabase"
                }

            finally:
                # Clean up temporary file
                os.unlink(temp_path)

        except Exception as e:
            self.logger.error(f"Failed to index PDF content for {document_id}: {e}")
            return {
                "success": False,
                "document_id": document_id,
                "error": str(e)
            }

    async def query_document(
        self,
        document_id: str,
        query: str,
        response_mode: str = "compact"
    ) -> Dict[str, Any]:
        """
        Query a specific document using RAG.

        Args:
            document_id: ID of the document to query
            query: Natural language query
            response_mode: Response synthesis mode ('compact', 'tree_summarize', etc.)

        Returns:
            Dict containing query response and metadata
        """
        if not self.available:
            raise RuntimeError("LlamaIndex service not available")

        try:
            # Load index if not in cache
            if document_id not in self.indices:
                await self._load_index(document_id)

            if document_id not in self.indices:
                return {
                    "success": False,
                    "error": f"Document {document_id} not found or not indexed"
                }

            index = self.indices[document_id]

            # Create query engine with AutoMergingRetriever for hierarchical context
            # AutoMergingRetriever automatically merges child nodes with their parents
            # to provide better context for search results
            base_retriever = VectorIndexRetriever(
                index=index,
                similarity_top_k=self.similarity_top_k
            )

            retriever = AutoMergingRetriever(
                base_retriever=base_retriever,
                verbose=True
            )

            response_synthesizer = get_response_synthesizer(
                response_mode=ResponseMode(response_mode)
            )

            query_engine = RetrieverQueryEngine(
                retriever=retriever,
                response_synthesizer=response_synthesizer
            )

            # Execute query
            response = query_engine.query(query)

            # Extract source information
            sources = []
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    sources.append({
                        "node_id": node.node.node_id,
                        "score": getattr(node, 'score', None),
                        "text_snippet": node.node.text[:200] + "..." if len(node.node.text) > 200 else node.node.text,
                        "metadata": node.node.metadata
                    })

            return {
                "success": True,
                "document_id": document_id,
                "query": query,
                "response": str(response),
                "sources": sources,
                "metadata": {
                    "response_mode": response_mode,
                    "similarity_top_k": self.similarity_top_k
                }
            }

        except Exception as e:
            self.logger.error(f"Failed to query document {document_id}: {e}")
            return {
                "success": False,
                "document_id": document_id,
                "query": query,
                "error": str(e)
            }

    async def advanced_rag_query(
        self,
        query: str,
        document_ids: Optional[List[str]] = None,
        query_type: str = "factual",
        similarity_threshold: float = 0.7,
        max_results: int = 5,
        enable_reranking: bool = True,
        conversation_context: Optional[List[Dict[str, str]]] = None,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Advanced RAG query processing pipeline with enhanced capabilities.

        Args:
            query: Natural language query
            document_ids: Optional list of specific documents to query (None = all indexed)
            query_type: Type of query ('factual', 'analytical', 'conversational', 'summarization')
            similarity_threshold: Minimum similarity score for retrieved chunks
            max_results: Maximum number of results to return
            enable_reranking: Whether to apply relevance re-ranking
            conversation_context: Previous conversation turns for context
            metadata_filters: Optional metadata filters for retrieval

        Returns:
            Dict containing comprehensive query results and metadata
        """
        if not self.available:
            raise RuntimeError("LlamaIndex service not available")

        try:
            # Step 1: Query Understanding and Preprocessing
            processed_query = await self._preprocess_query(query, query_type, conversation_context)

            # Step 2: Determine target documents
            target_documents = document_ids if document_ids else list(self.indices.keys())

            if not target_documents:
                return {
                    "success": False,
                    "error": "No indexed documents available for querying"
                }

            # Step 3: Multi-document retrieval with similarity thresholds
            all_retrieved_nodes = []
            retrieval_metadata = {}

            for doc_id in target_documents:
                if doc_id not in self.indices:
                    await self._load_index(doc_id)

                if doc_id in self.indices:
                    nodes = await self._retrieve_from_document(
                        doc_id,
                        processed_query,
                        similarity_threshold,
                        max_results,
                        metadata_filters
                    )
                    all_retrieved_nodes.extend(nodes)
                    retrieval_metadata[doc_id] = len(nodes)

            # Step 4: Apply relevance scoring and ranking
            if enable_reranking and all_retrieved_nodes:
                all_retrieved_nodes = await self._rerank_results(
                    all_retrieved_nodes,
                    processed_query,
                    query_type
                )

            # Step 5: Filter by similarity threshold and limit results
            filtered_nodes = [
                node for node in all_retrieved_nodes
                if getattr(node, 'score', 1.0) >= similarity_threshold
            ][:max_results]

            # Step 6: Context integration and response generation
            response_data = await self._generate_contextual_response(
                filtered_nodes,
                processed_query,
                query_type,
                conversation_context
            )

            # Step 7: Compile comprehensive results
            return {
                "success": True,
                "query": {
                    "original": query,
                    "processed": processed_query,
                    "type": query_type
                },
                "response": response_data["response"],
                "confidence_score": response_data["confidence"],
                "sources": self._format_source_nodes(filtered_nodes),
                "retrieval_stats": {
                    "total_documents_searched": len(target_documents),
                    "nodes_retrieved": len(all_retrieved_nodes),
                    "nodes_after_filtering": len(filtered_nodes),
                    "similarity_threshold": similarity_threshold,
                    "reranking_enabled": enable_reranking,
                    "per_document_results": retrieval_metadata
                },
                "query_metadata": {
                    "query_type": query_type,
                    "has_conversation_context": conversation_context is not None,
                    "metadata_filters_applied": metadata_filters is not None,
                    "processing_time_ms": response_data.get("processing_time_ms", 0)
                }
            }

        except Exception as e:
            self.logger.error(f"Advanced RAG query failed: {e}")
            return {
                "success": False,
                "query": query,
                "error": str(e),
                "query_type": query_type
            }

    async def _preprocess_query(
        self,
        query: str,
        query_type: str,
        conversation_context: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """
        Preprocess and enhance the query based on type and context.

        Args:
            query: Original query
            query_type: Type of query for specialized processing
            conversation_context: Previous conversation for context integration

        Returns:
            Enhanced/processed query string
        """
        processed_query = query.strip()

        # Add conversation context if available
        if conversation_context:
            context_summary = self._summarize_conversation_context(conversation_context)
            processed_query = f"Context: {context_summary}\n\nQuery: {processed_query}"

        # Query type specific enhancements
        if query_type == "analytical":
            processed_query = f"Analyze and provide detailed insights about: {processed_query}"
        elif query_type == "conversational":
            processed_query = f"In a conversational manner, please address: {processed_query}"
        elif query_type == "summarization":
            processed_query = f"Summarize the key information related to: {processed_query}"
        # 'factual' queries use the original query as-is

        return processed_query

    async def _retrieve_from_document(
        self,
        document_id: str,
        query: str,
        similarity_threshold: float,
        max_results: int,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> List[Any]:
        """
        Retrieve relevant nodes from a specific document with filtering.

        Args:
            document_id: Document to retrieve from
            query: Query string
            similarity_threshold: Minimum similarity score
            max_results: Maximum results per document
            metadata_filters: Optional metadata filters

        Returns:
            List of retrieved nodes with scores
        """
        try:
            index = self.indices[document_id]

            # Create base retriever with enhanced parameters
            base_retriever = VectorIndexRetriever(
                index=index,
                similarity_top_k=max_results * 2,  # Retrieve more for better filtering
                doc_ids=None,
                filters=metadata_filters
            )

            # Wrap with AutoMergingRetriever for hierarchical context
            retriever = AutoMergingRetriever(
                base_retriever=base_retriever,
                verbose=False
            )

            # Retrieve nodes with automatic parent-child merging
            retrieved_nodes = retriever.retrieve(query)

            # Filter by similarity threshold and add document context
            filtered_nodes = []
            for node in retrieved_nodes:
                if getattr(node, 'score', 1.0) >= similarity_threshold:
                    # Add document context to node metadata
                    if hasattr(node, 'node') and hasattr(node.node, 'metadata'):
                        node.node.metadata['source_document_id'] = document_id
                    filtered_nodes.append(node)

            return filtered_nodes[:max_results]

        except Exception as e:
            self.logger.error(f"Failed to retrieve from document {document_id}: {e}")
            return []

    async def _rerank_results(
        self,
        nodes: List[Any],
        query: str,
        query_type: str
    ) -> List[Any]:
        """
        Re-rank retrieved results based on relevance and query type.

        Args:
            nodes: Retrieved nodes to re-rank
            query: Original query
            query_type: Type of query for specialized ranking

        Returns:
            Re-ranked list of nodes
        """
        try:
            # Simple relevance-based re-ranking
            # In a production system, this could use a dedicated re-ranking model

            def calculate_relevance_score(node):
                base_score = getattr(node, 'score', 0.5)

                # Query type specific scoring adjustments
                text = node.node.text.lower() if hasattr(node, 'node') else ""
                query_lower = query.lower()

                # Boost score for exact matches
                if query_lower in text:
                    base_score += 0.1

                # Query type specific boosts
                if query_type == "analytical":
                    analytical_keywords = ["analysis", "insight", "conclusion", "result", "finding"]
                    if any(keyword in text for keyword in analytical_keywords):
                        base_score += 0.05
                elif query_type == "factual":
                    factual_keywords = ["fact", "data", "number", "statistic", "measurement"]
                    if any(keyword in text for keyword in factual_keywords):
                        base_score += 0.05

                return min(base_score, 1.0)  # Cap at 1.0

            # Re-rank nodes by calculated relevance
            ranked_nodes = sorted(nodes, key=calculate_relevance_score, reverse=True)

            # Update scores
            for node in ranked_nodes:
                node.score = calculate_relevance_score(node)

            return ranked_nodes

        except Exception as e:
            self.logger.error(f"Failed to re-rank results: {e}")
            return nodes  # Return original order on error

    async def _generate_contextual_response(
        self,
        nodes: List[Any],
        query: str,
        query_type: str,
        conversation_context: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Generate response with context integration and confidence scoring.

        Args:
            nodes: Retrieved and ranked nodes
            query: Processed query
            query_type: Type of query
            conversation_context: Previous conversation context

        Returns:
            Dict with response, confidence score, and metadata
        """
        import time
        start_time = time.time()

        try:
            if not nodes:
                return {
                    "response": "I couldn't find relevant information to answer your query.",
                    "confidence": 0.0,
                    "processing_time_ms": int((time.time() - start_time) * 1000)
                }

            # Select appropriate response mode based on query type
            response_mode_map = {
                "factual": "compact",
                "analytical": "tree_summarize",
                "conversational": "compact",
                "summarization": "tree_summarize"
            }
            response_mode = response_mode_map.get(query_type, "compact")

            # Create response synthesizer
            response_synthesizer = get_response_synthesizer(
                response_mode=ResponseMode(response_mode)
            )

            # Generate response
            response = response_synthesizer.synthesize(query, nodes)

            # Calculate confidence score based on node scores and count
            confidence = self._calculate_confidence_score(nodes, query_type)

            return {
                "response": str(response),
                "confidence": confidence,
                "processing_time_ms": int((time.time() - start_time) * 1000)
            }

        except Exception as e:
            self.logger.error(f"Failed to generate contextual response: {e}")
            return {
                "response": f"Error generating response: {str(e)}",
                "confidence": 0.0,
                "processing_time_ms": int((time.time() - start_time) * 1000)
            }

    def _calculate_confidence_score(self, nodes: List[Any], query_type: str) -> float:
        """Calculate confidence score based on retrieved nodes and query type."""
        if not nodes:
            return 0.0

        # Base confidence from node scores
        node_scores = [getattr(node, 'score', 0.5) for node in nodes]
        avg_score = sum(node_scores) / len(node_scores)

        # Adjust based on number of supporting nodes
        node_count_factor = min(len(nodes) / 3.0, 1.0)  # Optimal around 3 nodes

        # Query type adjustments
        type_multiplier = {
            "factual": 1.0,      # Factual queries need high precision
            "analytical": 0.9,   # Analytical queries are more subjective
            "conversational": 0.8, # Conversational queries are more flexible
            "summarization": 0.85  # Summarization depends on content coverage
        }.get(query_type, 0.8)

        confidence = avg_score * node_count_factor * type_multiplier
        return round(min(confidence, 1.0), 3)

    def _format_source_nodes(self, nodes: List[Any]) -> List[Dict[str, Any]]:
        """Format source nodes for response output."""
        sources = []
        for i, node in enumerate(nodes):
            if hasattr(node, 'node'):
                sources.append({
                    "rank": i + 1,
                    "node_id": node.node.node_id,
                    "score": round(getattr(node, 'score', 0.0), 3),
                    "text_snippet": node.node.text[:300] + "..." if len(node.node.text) > 300 else node.node.text,
                    "metadata": node.node.metadata or {},
                    "document_id": node.node.metadata.get('source_document_id', 'unknown')
                })
        return sources

    def _summarize_conversation_context(self, context: List[Dict[str, str]]) -> str:
        """Summarize conversation context for query enhancement."""
        if not context:
            return ""

        # Take last few turns for context (avoid too much context)
        recent_context = context[-3:] if len(context) > 3 else context

        summary_parts = []
        for turn in recent_context:
            if turn.get('role') == 'user':
                summary_parts.append(f"User asked: {turn.get('content', '')}")
            elif turn.get('role') == 'assistant':
                summary_parts.append(f"Assistant responded about: {turn.get('content', '')[:100]}...")

        return " | ".join(summary_parts)

    async def summarize_document(
        self,
        document_id: str,
        summary_type: str = "comprehensive"
    ) -> Dict[str, Any]:
        """
        Generate a summary of the document.

        Args:
            document_id: ID of the document to summarize
            summary_type: Type of summary ('brief', 'comprehensive', 'key_points')

        Returns:
            Dict containing the summary and metadata
        """
        if not self.available:
            raise RuntimeError("LlamaIndex service not available")

        # Define summary prompts
        prompts = {
            "brief": "Provide a brief 2-3 sentence summary of this document.",
            "comprehensive": "Provide a comprehensive summary of this document, including main topics, key findings, and important details.",
            "key_points": "Extract and list the key points, main arguments, and important information from this document."
        }

        prompt = prompts.get(summary_type, prompts["comprehensive"])

        return await self.query_document(
            document_id=document_id,
            query=prompt,
            response_mode="tree_summarize"
        )

    async def extract_entities(
        self,
        document_id: str,
        entity_types: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Extract named entities from the document.

        Args:
            document_id: ID of the document
            entity_types: List of entity types to extract (e.g., ['PERSON', 'ORG', 'DATE'])

        Returns:
            Dict containing extracted entities
        """
        if not self.available:
            raise RuntimeError("LlamaIndex service not available")

        entity_types_str = ", ".join(entity_types) if entity_types else "people, organizations, dates, locations, and other important entities"

        query = f"Extract and list all {entity_types_str} mentioned in this document. Provide them in a structured format."

        return await self.query_document(
            document_id=document_id,
            query=query,
            response_mode="compact"
        )

    async def compare_documents(
        self,
        document_ids: List[str],
        comparison_aspect: str = "general"
    ) -> Dict[str, Any]:
        """
        Compare multiple documents.

        Args:
            document_ids: List of document IDs to compare
            comparison_aspect: Aspect to compare ('general', 'topics', 'sentiment', etc.)

        Returns:
            Dict containing comparison results
        """
        if not self.available:
            raise RuntimeError("LlamaIndex service not available")

        if len(document_ids) < 2:
            return {
                "success": False,
                "error": "At least 2 documents required for comparison"
            }

        try:
            # Get summaries for each document
            summaries = {}
            for doc_id in document_ids:
                result = await self.summarize_document(doc_id, "comprehensive")
                if result["success"]:
                    summaries[doc_id] = result["response"]
                else:
                    return {
                        "success": False,
                        "error": f"Failed to summarize document {doc_id}: {result.get('error', 'Unknown error')}"
                    }

            # Create comparison prompt
            comparison_text = "\n\n".join([
                f"Document {doc_id}:\n{summary}"
                for doc_id, summary in summaries.items()
            ])

            comparison_prompts = {
                "general": "Compare and contrast these documents. What are the similarities and differences?",
                "topics": "What are the main topics covered in each document? How do they overlap or differ?",
                "sentiment": "Analyze the sentiment and tone of each document. How do they compare?",
                "key_insights": "What are the key insights from each document? How do they complement or contradict each other?"
            }

            prompt = comparison_prompts.get(comparison_aspect, comparison_prompts["general"])

            # Use the first document's index for the comparison query
            # (This is a limitation - ideally we'd create a combined index)
            result = await self.query_document(
                document_id=document_ids[0],
                query=f"{prompt}\n\nDocuments to compare:\n{comparison_text}",
                response_mode="tree_summarize"
            )

            if result["success"]:
                result["comparison_aspect"] = comparison_aspect
                result["compared_documents"] = document_ids

            return result

        except Exception as e:
            self.logger.error(f"Failed to compare documents: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _load_index(self, document_id: str) -> bool:
        """Load an index from storage."""
        try:
            index_dir = Path(self.storage_dir) / document_id
            if not index_dir.exists():
                return False

            storage_context = StorageContext.from_defaults(persist_dir=str(index_dir))
            index = load_index_from_storage(storage_context)
            self.indices[document_id] = index

            self.logger.info(f"Loaded index for document {document_id}")
            return True

        except Exception as e:
            self.logger.error(f"Failed to load index for {document_id}: {e}")
            return False

    async def list_indexed_documents(self) -> Dict[str, Any]:
        """List all indexed documents."""
        try:
            storage_path = Path(self.storage_dir)
            if not storage_path.exists():
                return {"documents": [], "count": 0}

            documents = []
            for item in storage_path.iterdir():
                if item.is_dir():
                    # Check if it's a valid index directory
                    if (item / "docstore.json").exists():
                        documents.append({
                            "document_id": item.name,
                            "storage_path": str(item),
                            "indexed": item.name in self.indices,
                            "size_mb": sum(f.stat().st_size for f in item.rglob('*') if f.is_file()) / (1024 * 1024)
                        })

            return {
                "documents": documents,
                "count": len(documents),
                "total_size_mb": sum(doc["size_mb"] for doc in documents)
            }

        except Exception as e:
            self.logger.error(f"Failed to list indexed documents: {e}")
            return {"error": str(e)}

    async def delete_document_index(self, document_id: str) -> Dict[str, Any]:
        """Delete a document index."""
        try:
            # Remove from cache
            if document_id in self.indices:
                del self.indices[document_id]

            # Remove from storage
            index_dir = Path(self.storage_dir) / document_id
            if index_dir.exists():
                import shutil
                shutil.rmtree(index_dir)

            return {
                "success": True,
                "document_id": document_id,
                "message": "Document index deleted successfully"
            }

        except Exception as e:
            self.logger.error(f"Failed to delete index for {document_id}: {e}")
            return {
                "success": False,
                "document_id": document_id,
                "error": str(e)
            }

    def __del__(self):
        """Cleanup resources when the service is destroyed."""
        if hasattr(self, 'executor') and self.executor:
            self.executor.shutdown(wait=True)

    # Conversation Memory Management Methods

    def manage_conversation_memory(
        self,
        session_id: str,
        user_message: str,
        assistant_response: str
    ) -> None:
        """
        Manage conversation memory for a session.

        Args:
            session_id: Unique session identifier
            user_message: User's message
            assistant_response: Assistant's response
        """
        if session_id not in self.conversation_memories:
            self.conversation_memories[session_id] = []

        # Add new conversation turn
        self.conversation_memories[session_id].append({
            "role": "user",
            "content": user_message,
            "timestamp": self._get_timestamp()
        })

        self.conversation_memories[session_id].append({
            "role": "assistant",
            "content": assistant_response,
            "timestamp": self._get_timestamp()
        })

        # Manage memory size
        self._manage_memory_size(session_id)

    def get_conversation_context(self, session_id: str) -> List[Dict[str, str]]:
        """
        Get conversation context for a session.

        Args:
            session_id: Session identifier

        Returns:
            List of conversation turns
        """
        return self.conversation_memories.get(session_id, [])

    def clear_conversation_memory(self, session_id: str) -> None:
        """
        Clear conversation memory for a session.

        Args:
            session_id: Session identifier
        """
        if session_id in self.conversation_memories:
            del self.conversation_memories[session_id]
            self.logger.info(f"Cleared conversation memory for session: {session_id}")

    def _manage_memory_size(self, session_id: str) -> None:
        """
        Manage conversation memory size, summarizing old conversations if needed.

        Args:
            session_id: Session identifier
        """
        memory = self.conversation_memories[session_id]

        if len(memory) > self.max_conversation_turns:
            # Check if we should summarize
            if len(memory) > self.conversation_summary_threshold:
                self._summarize_old_conversations(session_id)
            else:
                # Simple truncation - remove oldest turns
                excess = len(memory) - self.max_conversation_turns
                self.conversation_memories[session_id] = memory[excess:]

    def _summarize_old_conversations(self, session_id: str) -> None:
        """
        Summarize old conversations to preserve context while reducing memory.

        Args:
            session_id: Session identifier
        """
        try:
            memory = self.conversation_memories[session_id]

            # Keep recent conversations, summarize older ones
            recent_turns = 4  # Keep last 4 turns
            old_conversations = memory[:-recent_turns]
            recent_conversations = memory[-recent_turns:]

            if not old_conversations:
                return

            # Create summary of old conversations
            conversation_text = "\n".join([
                f"{turn['role']}: {turn['content']}"
                for turn in old_conversations
            ])

            summary_prompt = f"""Summarize the following conversation history concisely, preserving key context and topics discussed:

{conversation_text}

Summary:"""

            # Use LLM to create summary (if available)
            if hasattr(self, 'llm') and self.llm:
                try:
                    summary_response = self.llm.complete(summary_prompt)
                    summary = str(summary_response).strip()
                except Exception as e:
                    self.logger.warning(f"Failed to generate conversation summary: {e}")
                    summary = "Previous conversation covered multiple topics and queries."
            else:
                # Fallback summary
                summary = f"Previous conversation with {len(old_conversations)} turns covering various topics."

            # Replace old conversations with summary
            summary_entry = {
                "role": "system",
                "content": f"[Conversation Summary]: {summary}",
                "timestamp": self._get_timestamp()
            }

            self.conversation_memories[session_id] = [summary_entry] + recent_conversations

            self.logger.info(f"Summarized {len(old_conversations)} old conversation turns for session: {session_id}")

        except Exception as e:
            self.logger.error(f"Failed to summarize conversations for session {session_id}: {e}")
            # Fallback to simple truncation
            memory = self.conversation_memories[session_id]
            self.conversation_memories[session_id] = memory[-self.max_conversation_turns:]

    def _get_timestamp(self) -> str:
        """Get current timestamp string."""
        from datetime import datetime
        return datetime.now().isoformat()

    def get_memory_stats(self) -> Dict[str, Any]:
        """
        Get conversation memory statistics.

        Returns:
            Dict with memory statistics
        """
        stats = {
            "total_sessions": len(self.conversation_memories),
            "max_turns_per_session": self.max_conversation_turns,
            "summary_threshold": self.conversation_summary_threshold,
            "sessions": {}
        }

        for session_id, memory in self.conversation_memories.items():
            stats["sessions"][session_id] = {
                "turn_count": len(memory),
                "has_summary": any(turn.get("role") == "system" for turn in memory),
                "last_activity": memory[-1]["timestamp"] if memory else None
            }

        return stats

    async def _store_chunks_in_database(
        self,
        document_id: str,
        nodes: List,
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Store document chunks and embeddings in the database tables.

        Args:
            document_id: Document identifier
            nodes: LlamaIndex nodes containing chunks
            metadata: Document metadata

        Returns:
            Dict with storage statistics
        """
        try:
            from app.services.supabase_client import SupabaseClient
            supabase_client = SupabaseClient()

            chunks_stored = 0
            embeddings_stored = 0
            failed_chunks = 0

            # ‚úÖ NEW: Track rejection statistics
            rejection_stats = {
                'quality_rejected': 0,
                'exact_duplicates': 0,
                'semantic_duplicates': 0,
                'total_rejected': 0
            }
            rejection_details = []

            self.logger.info(f"üîÑ Starting database storage for {len(nodes)} chunks from document {document_id}")
            self.logger.info(f"   OpenAI embeddings available: {self.embeddings is not None}")
            if self.embeddings is None:
                self.logger.error("   ‚ö†Ô∏è WARNING: Embeddings not initialized - embeddings will NOT be generated!")

            # First, ensure the document exists in the documents table
            self._ensure_document_exists(supabase_client, document_id, metadata)

            # ‚úÖ ENHANCEMENT 1: Apply boundary-aware chunking (if enabled)
            # This splits chunks at product boundaries BEFORE storage
            try:
                from app.config import settings
                if settings.enable_boundary_detection:
                    # Get products for this document
                    products_result = supabase_client.client.table('products')\
                        .select('id, name, page_range, metadata')\
                        .eq('document_id', document_id)\
                        .execute()

                    if products_result.data:
                        # Convert nodes to dict format for boundary detection
                        chunks_dict = [{'content': node.text, 'metadata': node.metadata} for node in nodes]

                        # Apply boundary detection
                        boundary_aware_chunks = await self.boundary_aware_chunking_service.process_chunks(
                            chunks_dict, products_result.data, document_id
                        )

                        # Convert back to nodes (simplified - in production, preserve node structure)
                        # For now, we'll continue with original nodes
                        self.logger.info(f"‚úÖ Boundary detection processed {len(boundary_aware_chunks)} chunks")
            except Exception as boundary_error:
                self.logger.warning(f"‚ö†Ô∏è Boundary detection failed (using original chunks): {boundary_error}")

            # ‚úÖ ENHANCEMENT 3: Apply context enrichment (if enabled)
            # This adds product_id to chunk metadata BEFORE storage
            try:
                if settings.enable_context_enrichment:
                    # Get products for this document
                    products_result = supabase_client.client.table('products')\
                        .select('id, name, page_range, metadata')\
                        .eq('document_id', document_id)\
                        .execute()

                    if products_result.data:
                        # Convert nodes to dict format for enrichment
                        chunks_dict = [{'content': node.text, 'metadata': node.metadata} for node in nodes]

                        # Apply context enrichment
                        enriched_chunks = await self.chunk_context_enrichment_service.enrich_chunks(
                            chunks_dict, products_result.data, document_id
                        )

                        # Update node metadata with enrichment
                        for i, enriched_chunk in enumerate(enriched_chunks):
                            if i < len(nodes) and 'metadata' in enriched_chunk:
                                nodes[i].metadata.update(enriched_chunk['metadata'])

                        self.logger.info(f"‚úÖ Context enrichment applied to {len(enriched_chunks)} chunks")
            except Exception as enrichment_error:
                self.logger.warning(f"‚ö†Ô∏è Context enrichment failed (continuing without enrichment): {enrichment_error}")

            for i, node in enumerate(nodes):
                try:
                    # ‚úÖ NEW: Calculate quality score
                    quality_score = self._calculate_chunk_quality(node.text)

                    # ‚úÖ NEW: Quality validation gates
                    if not self._validate_chunk_quality(node.text, quality_score):
                        rejection_stats['quality_rejected'] += 1
                        rejection_stats['total_rejected'] += 1
                        rejection_details.append({
                            'chunk_index': i,
                            'reason': 'quality_score',
                            'quality_score': quality_score,
                            'length': len(node.text),
                            'content_preview': node.text[:100]
                        })
                        self.logger.warning(f"‚ö†Ô∏è Chunk {i} rejected: Low quality (score: {quality_score:.2f})")
                        continue

                    # ‚úÖ NEW: Generate content hash for deduplication
                    content_hash = self._generate_content_hash(node.text)

                    # ‚úÖ NEW: Check for exact duplicate chunks (hash-based)
                    duplicate_check = supabase_client.client.table('document_chunks')\
                        .select('id')\
                        .eq('content_hash', content_hash)\
                        .eq('document_id', document_id)\
                        .execute()

                    if duplicate_check.data and len(duplicate_check.data) > 0:
                        rejection_stats['exact_duplicates'] += 1
                        rejection_stats['total_rejected'] += 1
                        rejection_details.append({
                            'chunk_index': i,
                            'reason': 'exact_duplicate',
                            'content_hash': content_hash,
                            'duplicate_of': duplicate_check.data[0]['id']
                        })
                        self.logger.warning(f"‚ö†Ô∏è Chunk {i} skipped: Exact duplicate (hash: {content_hash[:8]}...)")
                        continue

                    # ‚úÖ ENHANCED: Check for semantic near-duplicates (embedding-based)
                    if self._check_semantic_duplicates(supabase_client, node.text, document_id, similarity_threshold=0.85):
                        rejection_stats['semantic_duplicates'] += 1
                        rejection_stats['total_rejected'] += 1
                        rejection_details.append({
                            'chunk_index': i,
                            'reason': 'semantic_duplicate',
                            'similarity_threshold': 0.85
                        })
                        self.logger.warning(f"‚ö†Ô∏è Chunk {i} skipped: Semantic near-duplicate detected")
                        continue

                    # ‚úÖ NEW: Classify chunk type BEFORE storing to exclude non-content chunks
                    try:
                        from .chunk_type_classification_service import ChunkType
                        classification_result = await self.chunk_classifier.classify_chunk(node.text)

                        # ‚úÖ EXCLUDE non-content chunks (index pages, specs, etc.)
                        EXCLUDED_TYPES = [
                            ChunkType.INDEX_CONTENT,           # Index pages with product listings
                            ChunkType.TECHNICAL_SPECS,         # Specs (goes in metadata instead)
                            ChunkType.CERTIFICATION_INFO,      # Certs (separate entity)
                            ChunkType.UNCLASSIFIED             # Too short/unclear
                        ]

                        if classification_result.chunk_type in EXCLUDED_TYPES:
                            rejection_stats['excluded_type'] = rejection_stats.get('excluded_type', 0) + 1
                            rejection_stats['total_rejected'] += 1
                            rejection_details.append({
                                'chunk_index': i,
                                'reason': 'excluded_chunk_type',
                                'chunk_type': classification_result.chunk_type.value,
                                'confidence': classification_result.confidence,
                                'reasoning': classification_result.reasoning
                            })
                            self.logger.info(f"‚ö†Ô∏è Chunk {i} excluded: {classification_result.chunk_type.value} (confidence: {classification_result.confidence:.2f})")
                            continue  # Skip storing this chunk

                    except Exception as classification_error:
                        self.logger.warning(f"‚ö†Ô∏è Failed to classify chunk {i} for exclusion check: {classification_error}")
                        # Continue with storage if classification fails (fail-safe)
                        classification_result = None

                    # Store chunk in document_chunks table
                    chunk_data = {
                        'document_id': document_id,
                        'workspace_id': metadata.get('workspace_id'),
                        'content': node.text,
                        'chunk_index': i,
                        'content_hash': content_hash,  # ‚úÖ NEW: Store hash
                        'metadata': {
                            'chunk_id': node.metadata.get('chunk_id', f"{document_id}_chunk_{i}"),
                            'chunk_strategy': 'hierarchical',
                            'chunk_size_actual': len(node.text),
                            'total_chunks': len(nodes),
                            'quality_score': quality_score,  # ‚úÖ NEW: Store quality score
                            'has_parent': node.parent_node is not None,
                            'has_children': len(node.child_nodes) > 0 if hasattr(node, 'child_nodes') and node.child_nodes is not None else False,
                            **node.metadata
                        }
                    }

                    # Insert chunk into database
                    chunk_result = supabase_client.client.table('document_chunks').insert(chunk_data).execute()

                    if chunk_result.data:
                        chunk_id = chunk_result.data[0]['id']
                        chunks_stored += 1
                        # Store database UUID in node metadata for later use (for image-chunk relationships)
                        node.metadata["db_chunk_id"] = chunk_id

                        # ‚úÖ ENHANCED: Flag borderline quality chunks for review (score 0.6-0.7)
                        if 0.6 <= quality_score < 0.7:
                            self._flag_low_quality_chunk(
                                supabase_client=supabase_client,
                                chunk_id=chunk_id,
                                document_id=document_id,
                                workspace_id=metadata.get('workspace_id'),
                                flag_type='borderline_quality',
                                flag_reason=f'Quality score {quality_score:.2f} is below optimal threshold (0.7)',
                                quality_score=quality_score,
                                content_preview=node.text
                            )

                        # Update chunk with classification results (already computed above)
                        if classification_result:
                            try:
                                classification_update = {
                                    'chunk_type': classification_result.chunk_type.value,
                                    'chunk_type_confidence': classification_result.confidence,
                                    'chunk_type_metadata': classification_result.metadata
                                }

                                update_result = supabase_client.client.table('document_chunks').update(classification_update).eq('id', chunk_id).execute()

                                if update_result.data:
                                    self.logger.debug(f"‚úÖ Classified chunk {i} as {classification_result.chunk_type.value} (confidence: {classification_result.confidence:.2f})")
                                else:
                                    self.logger.warning(f"‚ö†Ô∏è Failed to update chunk {i} with classification")

                            except Exception as classification_error:
                                self.logger.error(f"‚ùå Failed to update chunk {i} with classification: {classification_error}")
                                # Continue processing even if classification update fails

                        # Generate and store embedding
                        try:
                            # Check if embeddings are available
                            if self.embeddings is None:
                                self.logger.error(f"‚ùå Embeddings not initialized for chunk {i} - OPENAI_API_KEY not set in environment")
                                if i == 0:
                                    self.logger.error("   This will affect ALL chunks - no embeddings will be generated!")
                                continue

                            # Generate embedding for the chunk using OpenAI directly
                            embedding_vector = self.embeddings.get_text_embedding(node.text)

                            if embedding_vector:
                                    # Store embedding in embeddings table
                                    embedding_data = {
                                        'chunk_id': chunk_id,
                                        'workspace_id': metadata.get('workspace_id'),
                                        'embedding': embedding_vector,
                                        'model_name': 'text-embedding-3-small',
                                        'dimensions': 1536
                                    }

                                    embedding_result = supabase_client.client.table('embeddings').insert(embedding_data).execute()

                                    if embedding_result.data:
                                        embeddings_stored += 1

                                        # Also store in document_vectors table for comprehensive search
                                        vector_data = {
                                            'document_id': document_id,
                                            'chunk_id': chunk_id,
                                            'workspace_id': metadata.get('workspace_id'),
                                            'content': node.text,
                                            'embedding': embedding_vector,
                                            'metadata': chunk_data['metadata'],
                                            'model_name': 'text-embedding-3-small'
                                        }

                                        supabase_client.client.table('document_vectors').insert(vector_data).execute()

                        except Exception as embedding_error:
                            self.logger.warning(f"Failed to generate/store embedding for chunk {i}: {embedding_error}")

                except Exception as chunk_error:
                    self.logger.error(f"Failed to store chunk {i}: {chunk_error}")
                    failed_chunks += 1

            # ‚úÖ NEW: Calculate acceptance rate
            rejection_stats['acceptance_rate'] = f"{(chunks_stored / len(nodes) * 100):.1f}%" if nodes else "0%"

            result = {
                "chunks_stored": chunks_stored,
                "embeddings_stored": embeddings_stored,
                "failed_chunks": failed_chunks,
                "total_processed": len(nodes),
                "success_rate": (chunks_stored / len(nodes)) * 100 if nodes else 0,
                # ‚úÖ NEW: Add rejection statistics
                "rejection_stats": rejection_stats,
                "rejection_details": rejection_details[:10]  # Limit to first 10 for brevity
            }

            # Log detailed statistics
            self.logger.info(f"‚úÖ Database storage completed:")
            self.logger.info(f"   Chunks created by Anthropic: {len(nodes)}")
            self.logger.info(f"   Chunks accepted & stored: {chunks_stored}/{len(nodes)} ({rejection_stats['acceptance_rate']})")
            self.logger.info(f"   Chunks rejected: {rejection_stats['total_rejected']}")
            self.logger.info(f"     - Quality rejected: {rejection_stats['quality_rejected']}")
            self.logger.info(f"     - Exact duplicates: {rejection_stats['exact_duplicates']}")
            self.logger.info(f"     - Semantic duplicates: {rejection_stats['semantic_duplicates']}")
            self.logger.info(f"   Embeddings generated: {embeddings_stored}/{chunks_stored}")
            self.logger.info(f"   Failed chunks: {failed_chunks}")

            if embeddings_stored == 0 and chunks_stored > 0:
                self.logger.error(f"   ‚ö†Ô∏è CRITICAL: No embeddings were generated despite {chunks_stored} chunks being stored!")
                self.logger.error(f"   This indicates OPENAI_API_KEY is not set in the MIVAA environment")

            # ‚úÖ ENHANCEMENT 5: Create chunk relationships (if enabled)
            # This runs AFTER all chunks are stored (post-processing)
            try:
                from app.config import settings
                if settings.enable_chunk_relationships and chunks_stored > 0:
                    self.logger.info("üîó Creating chunk relationships...")
                    relationship_result = await self.chunk_relationship_service.create_relationships(
                        document_id=document_id,
                        workspace_id=metadata.get('workspace_id'),
                        job_id=metadata.get('job_id')
                    )

                    if relationship_result.get('success'):
                        result['relationships_created'] = relationship_result.get('relationships_created', 0)
                        self.logger.info(f"‚úÖ Created {result['relationships_created']} chunk relationships")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Chunk relationship creation failed: {relationship_result.get('error')}")
            except Exception as relationship_error:
                self.logger.warning(f"‚ö†Ô∏è Chunk relationship creation failed (continuing): {relationship_error}")
                # Don't fail the entire process if relationship creation fails

            return result

        except Exception as e:
            self.logger.error(f"‚ùå CRITICAL: Failed to store chunks in database: {e}", exc_info=True)
            self.logger.error(f"   Document ID: {document_id}")
            self.logger.error(f"   Total nodes to process: {len(nodes)}")
            self.logger.error(f"   Chunks stored before error: {chunks_stored}")
            self.logger.error(f"   Embeddings stored before error: {embeddings_stored}")
            # RE-RAISE the exception so the background task knows it failed
            raise RuntimeError(f"Failed to store chunks in database: {str(e)}") from e

    def _generate_content_hash(self, content: str) -> str:
        """
        ‚úÖ NEW: Generate SHA-256 hash of normalized content for deduplication.

        Args:
            content: Text content to hash

        Returns:
            SHA-256 hash string
        """
        import hashlib

        # Normalize content: lowercase, strip whitespace, remove extra spaces
        normalized = ' '.join(content.lower().strip().split())

        # Generate SHA-256 hash
        hash_object = hashlib.sha256(normalized.encode('utf-8'))
        return hash_object.hexdigest()

    def _calculate_chunk_quality(self, content: str) -> float:
        """
        ‚úÖ NEW: Calculate quality score for a chunk.

        Metrics:
        - Length score (0.0-1.0): Optimal length is 500-2000 characters
        - Boundary score (0.7 or 1.0): Ends with proper punctuation
        - Semantic score (0.0-1.0): Has 3+ sentences

        Args:
            content: Text content to score

        Returns:
            Quality score (0.0-1.0)
        """
        try:
            # Check content length (optimal: 500-2000 characters)
            length = len(content)
            if length < 500:
                length_score = length / 500  # Penalize short chunks
            elif length > 2000:
                length_score = max(0.5, 1.0 - (length - 2000) / 3000)  # Penalize very long chunks
            else:
                length_score = 1.0  # Optimal length

            # Check for proper boundaries (ends with punctuation)
            ends_with_punctuation = content.strip().endswith(('.', '!', '?', ':', ';'))
            boundary_score = 1.0 if ends_with_punctuation else 0.7

            # Check for semantic completeness (3+ sentences)
            sentences = content.count('.') + content.count('!') + content.count('?')
            semantic_score = min(1.0, sentences / 3)

            # Weighted average
            quality_score = (
                length_score * 0.3 +
                boundary_score * 0.4 +
                semantic_score * 0.3
            )

            return round(quality_score, 3)
        except Exception as e:
            self.logger.warning(f"Error calculating chunk quality: {e}")
            return 0.5  # Default to medium quality

    def _validate_chunk_quality(self, content: str, quality_score: float) -> bool:
        """
        ‚úÖ ENHANCED: Validate chunk quality against enhanced quality gates.

        Enhanced Quality Gates:
        - Minimum quality score: 0.7 (was 0.6)
        - Minimum content length: 50 characters
        - Maximum content length: 5000 characters
        - Semantic completeness: > 0.7 (at least 2-3 sentences)
        - Boundary quality: > 0.6 (proper punctuation)
        - Context preservation: > 0.7 (coherent content)

        Args:
            content: Text content to validate
            quality_score: Pre-calculated quality score

        Returns:
            True if chunk passes quality gates, False otherwise
        """
        try:
            # Gate 1: Minimum quality score (ENHANCED: 0.7 instead of 0.6)
            if quality_score < 0.7:
                self.logger.debug(f"Quality gate failed: Score {quality_score:.2f} < 0.7")
                return False

            # Gate 2: Minimum content length
            if len(content) < 50:
                self.logger.debug(f"Quality gate failed: Length {len(content)} < 50 characters")
                return False

            # Gate 3: Maximum content length
            if len(content) > 5000:
                self.logger.debug(f"Quality gate failed: Length {len(content)} > 5000 characters")
                return False

            # Gate 4: Semantic completeness (ENHANCED: at least 2 sentences)
            sentences = content.count('.') + content.count('!') + content.count('?')
            if sentences < 2:
                self.logger.debug(f"Quality gate failed: Only {sentences} sentence(s), need at least 2")
                return False

            # Gate 5: Boundary quality (ENHANCED: check proper punctuation)
            ends_with_punctuation = content.strip().endswith(('.', '!', '?', ':', ';'))
            if not ends_with_punctuation:
                self.logger.debug("Quality gate failed: No proper punctuation at end")
                return False

            # Gate 6: Context preservation (ENHANCED: check for coherent content)
            # Check if content has reasonable word count (not just punctuation)
            words = len(content.split())
            if words < 10:
                self.logger.debug(f"Quality gate failed: Only {words} words, need at least 10")
                return False

            return True
        except Exception as e:
            self.logger.warning(f"Error validating chunk quality: {e}")
            return True  # Default to accepting chunk if validation fails

    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """
        ‚úÖ NEW: Calculate semantic similarity between two text chunks using embeddings.

        Uses cosine similarity between OpenAI embeddings to detect near-duplicates.

        Args:
            text1: First text chunk
            text2: Second text chunk

        Returns:
            Similarity score (0.0-1.0), where 1.0 is identical
        """
        try:
            import numpy as np
            from numpy.linalg import norm

            # Generate embeddings for both texts
            if not hasattr(self, 'embeddings_service') or self.embeddings_service is None:
                self.logger.warning("Embeddings service not available for similarity calculation")
                return 0.0

            # Get embeddings (synchronous call)
            embedding1 = self.embeddings_service.generate_embedding(text1)
            embedding2 = self.embeddings_service.generate_embedding(text2)

            if not embedding1 or not embedding2:
                return 0.0

            # Convert to numpy arrays
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)

            # Calculate cosine similarity
            similarity = np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))

            return float(similarity)
        except Exception as e:
            self.logger.warning(f"Error calculating semantic similarity: {e}")
            return 0.0

    def _check_semantic_duplicates(
        self,
        supabase_client,
        content: str,
        document_id: str,
        similarity_threshold: float = 0.85
    ) -> bool:
        """
        ‚úÖ NEW: Check for semantically similar chunks (near-duplicates).

        Detects chunks that are similar in meaning but not exact duplicates.
        Uses embedding-based similarity with configurable threshold.

        Args:
            supabase_client: Supabase client instance
            content: Text content to check
            document_id: Document ID to scope the search
            similarity_threshold: Minimum similarity to consider as duplicate (default: 0.85)

        Returns:
            True if near-duplicate found, False otherwise
        """
        try:
            # Get recent chunks from same document (last 50 chunks)
            recent_chunks = supabase_client.client.table('document_chunks')\
                .select('content')\
                .eq('document_id', document_id)\
                .order('created_at', desc=True)\
                .limit(50)\
                .execute()

            if not recent_chunks.data:
                return False

            # Check similarity with each recent chunk
            for chunk in recent_chunks.data:
                existing_content = chunk.get('content', '')
                if not existing_content:
                    continue

                # Calculate semantic similarity
                similarity = self._calculate_semantic_similarity(content, existing_content)

                if similarity >= similarity_threshold:
                    self.logger.warning(
                        f"‚ö†Ô∏è Near-duplicate detected: Similarity {similarity:.2f} >= {similarity_threshold}"
                    )
                    return True

            return False
        except Exception as e:
            self.logger.warning(f"Error checking semantic duplicates: {e}")
            return False  # Don't block insertion on error

    def _flag_low_quality_chunk(
        self,
        supabase_client,
        chunk_id: str,
        document_id: str,
        workspace_id: str,
        flag_type: str,
        flag_reason: str,
        quality_score: float,
        content_preview: str
    ) -> None:
        """
        ‚úÖ NEW: Flag a low-quality chunk for manual review.

        Creates a record in chunk_quality_flags table for admin review.

        Args:
            supabase_client: Supabase client instance
            chunk_id: Chunk ID
            document_id: Document ID
            workspace_id: Workspace ID
            flag_type: Type of quality issue (e.g., 'low_quality', 'boundary_issue', 'semantic_incomplete')
            flag_reason: Detailed reason for flagging
            quality_score: Quality score that triggered the flag
            content_preview: First 200 characters of content
        """
        try:
            flag_data = {
                'chunk_id': chunk_id,
                'document_id': document_id,
                'workspace_id': workspace_id,
                'flag_type': flag_type,
                'flag_reason': flag_reason,
                'quality_score': quality_score,
                'content_preview': content_preview[:200] if content_preview else None,
                'reviewed': False
            }

            supabase_client.client.table('chunk_quality_flags').insert(flag_data).execute()
            self.logger.info(f"‚úÖ Flagged chunk {chunk_id} for review: {flag_type}")
        except Exception as e:
            self.logger.warning(f"Error flagging chunk for review: {e}")

    def _ensure_document_exists(
        self,
        supabase_client,
        document_id: str,
        metadata: Dict[str, Any]
    ):
        """Ensure the document exists in the documents table."""
        self.logger.info(f"üîç Checking if document {document_id} exists in documents table...")
        try:
            # Check if document exists
            existing = supabase_client.client.table('documents').select('id').eq('id', document_id).execute()

            if not existing.data:
                # Create document record
                document_data = {
                    'id': document_id,
                    'workspace_id': metadata.get('workspace_id'),
                    'filename': metadata.get('filename', f"{document_id}.pdf"),
                    'content_type': 'application/pdf',
                    'metadata': metadata,
                    'processing_status': 'completed'
                }

                supabase_client.client.table('documents').insert(document_data).execute()
                self.logger.info(f"‚úÖ Created document record for {document_id}")
            else:
                self.logger.info(f"‚úÖ Document {document_id} already exists")

        except Exception as e:
            self.logger.error(f"‚ùå Failed to ensure document exists: {e}")
            # Continue anyway - chunks can still be stored

    async def _process_extracted_images_with_context(
        self,
        document_id: str,
        extracted_images: List[Dict],
        nodes: List,
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process extracted images with layout-aware context linking, CLIP embeddings, material analysis,
        and Llama 4 Scout vision-based product extraction.

        Enhanced with:
        - Product detection using Llama 4 Scout Vision (69.4% MMMU, #1 OCR)
        - Material property extraction
        - CLIP embeddings for visual similarity
        """
        try:
            from .supabase_client import get_supabase_client
            from .product_vision_extractor import ProductVisionExtractor
            import base64
            import os

            supabase_client = get_supabase_client()
            stats = {
                "images_processed": 0,
                "clip_embeddings_generated": 0,
                "material_analyses_completed": 0,
                "images_stored": 0,
                "layout_links_created": 0,
                "products_detected": 0  # NEW: Track product detection
            }
            workspace_id = metadata.get("workspace_id", "ffafc28b-1b8b-4b0d-b226-9f9a6154004e")

            if not extracted_images:
                return stats

            self.logger.info(f"üñºÔ∏è Processing {len(extracted_images)} images with context linking...")
            self.logger.info(f"üîç DEBUG - First image info: {extracted_images[0] if extracted_images else 'No images'}")

            # ‚úÖ NEW: Initialize product vision extractor
            product_extractor = ProductVisionExtractor()

            # Build document context for product extraction
            document_context = {
                "catalog_name": metadata.get("filename", ""),
                "brand": metadata.get("brand", ""),
                "document_id": document_id
            }

            # ‚úÖ NEW: Extract products from all images using Llama 4 Scout Vision
            self.logger.info("üîç Extracting products using Llama 4 Scout Vision...")
            detected_products = await product_extractor.extract_products_from_images(
                extracted_images=extracted_images,
                document_context=document_context
            )

            if detected_products:
                self.logger.info(f"‚úÖ Detected {len(detected_products)} products via vision analysis")
                stats["products_detected"] = len(detected_products)

                # Store detected products in database
                await self._store_detected_products(
                    products=detected_products,
                    document_id=document_id,
                    workspace_id=workspace_id,
                    supabase_client=supabase_client
                )

                # ‚úÖ NEW: Apply metadata with scope detection and override logic
                try:
                    self.logger.info("üè∑Ô∏è Applying metadata with scope detection...")
                    metadata_stats = await self._apply_metadata_to_products(
                        document_id=document_id,
                        nodes=nodes,
                        detected_products=detected_products,
                        supabase_client=supabase_client
                    )
                    stats["metadata_applied"] = metadata_stats
                    self.logger.info(f"‚úÖ Metadata applied: {metadata_stats.get('products_updated', 0)} products updated")
                except Exception as metadata_error:
                    self.logger.error(f"‚ùå Failed to apply metadata: {metadata_error}", exc_info=True)
                    stats["metadata_error"] = str(metadata_error)

            # Extract heading hierarchy from document text for context
            heading_hierarchy = self._extract_heading_hierarchy(nodes)

            # ‚úÖ MEMORY OPTIMIZATION: Process images in batches to avoid OOM
            BATCH_SIZE = 5  # Process 5 images at a time
            total_images = len(extracted_images)

            for batch_start in range(0, total_images, BATCH_SIZE):
                batch_end = min(batch_start + BATCH_SIZE, total_images)
                batch_images = extracted_images[batch_start:batch_end]

                # Log memory usage before batch
                process = psutil.Process()
                mem_before = process.memory_info().rss / 1024 / 1024  # MB
                self.logger.info(f"üß† Memory before batch {batch_start//BATCH_SIZE + 1}: {mem_before:.1f} MB")

                for i, image_info in enumerate(batch_images, start=batch_start):
                    try:
                        image_path = image_info.get('path')
                        self.logger.info(f"üîç DEBUG - Image {i+1}/{total_images}: path={image_path}, exists={os.path.exists(image_path)}")
                        if not image_path or not os.path.exists(image_path):
                            continue

                        # Generate contextual name based on nearest heading
                        contextual_name, nearest_heading, heading_level = self._generate_contextual_image_name(
                            image_info, heading_hierarchy, i
                        )

                        # Find associated chunks based on layout proximity
                        associated_chunks = self._find_associated_chunks(image_info, nodes)

                        # Read image for processing
                        with open(image_path, 'rb') as img_file:
                            image_data = img_file.read()
                            image_base64 = base64.b64encode(image_data).decode('utf-8')

                        # Generate CLIP embeddings using existing service
                        clip_embeddings = await self._generate_clip_embeddings(image_base64, image_path)

                        # Perform material analysis using existing service
                        material_analysis = await self._analyze_image_material(
                            image_base64,
                            image_path,
                            document_id=document_id
                        )

                        # Store image with context in database
                        # Map to actual database columns
                        image_record = {
                            'document_id': document_id,
                            'workspace_id': workspace_id,
                            'chunk_id': associated_chunks[0].metadata.get('db_chunk_id') if associated_chunks else None,
                            'image_url': image_info.get('storage_url', ''),
                            'image_type': image_info.get('image_type', 'extracted'),
                            'caption': contextual_name,
                            'alt_text': f"Image from {nearest_heading}" if nearest_heading else None,
                            'bbox': image_info.get('bbox', {}),
                            'page_number': image_info.get('page_number', 1),
                            'proximity_score': 1.0 if associated_chunks else 0.0,
                            'confidence': image_info.get('quality_score', 0.0),
                            'contextual_name': contextual_name,
                            'nearest_heading': nearest_heading,
                            'heading_level': heading_level,
                            'quality_score': image_info.get('quality_score', 0.0),
                            'quality_metrics': image_info.get('quality_metrics', {}),
                            'ocr_extracted_text': image_info.get('ocr_result', {}).get('text', ''),
                            'ocr_confidence_score': image_info.get('ocr_result', {}).get('confidence', 0.0),
                            'image_analysis_results': material_analysis.get('material_properties', {}) or {},

                            # AI Analysis Columns (embeddings saved separately to embeddings table + VECS)
                            'claude_validation': material_analysis.get('claude_validation'),  # Claude 4.5 Sonnet validation
                            'llama_analysis': material_analysis.get('llama_analysis'),  # Llama 4 Scout 17B Vision analysis

                            # Note: Embeddings are now saved to embeddings table + VECS collections, not document_images
                            # visual_features kept for backward compatibility with metadata only
                            'visual_features': {
                                'model_used': clip_embeddings.get('model_used', 'ViT-B/32'),
                                'embedding_generated': bool(clip_embeddings.get('embedding_512'))
                            },
                            'processing_status': 'completed',
                            'multimodal_metadata': {
                                'associated_chunks': [chunk.metadata.get('db_chunk_id') for chunk in associated_chunks if chunk.metadata.get('db_chunk_id')],
                                'layout_context': {
                                    'bbox': image_info.get('bbox', {}),
                                    'quality_score': image_info.get('quality_score', 0.0),
                                    'image_type': image_info.get('image_type', 'unknown')
                                },
                                'image_metadata': {
                                    'format': image_info.get('format', 'unknown'),
                                    'size_bytes': len(image_data),
                                    'dimensions': f"{image_info.get('width', 0)}x{image_info.get('height', 0)}"
                                },
                                'extraction_confidence': image_info.get('quality_score', 0.0)
                            },
                            'metadata': {
                                'original_filename': os.path.basename(image_path),
                                'storage_path': image_info.get('storage_path', ''),
                                'storage_bucket': image_info.get('storage_bucket', 'pdf-tiles'),
                                'processing_timestamp': image_info.get('processing_timestamp', '')
                            },
                            'analysis_metadata': {
                                'material_analysis': material_analysis.get('material_properties', {}),
                                'clip_processing_time_ms': clip_embeddings.get('processing_time_ms', 0),
                                'llama_processing_time_ms': material_analysis.get('processing_time_ms', 0),
                                'analysis_method': material_analysis.get('analysis_method', 'unknown'),
                                'quality_score': material_analysis.get('quality_score', 0.0),
                                'confidence_score': material_analysis.get('confidence_score', 0.0),
                                'timestamp': material_analysis.get('timestamp', '')
                            }
                        }

                        # Store material metadata in materials_catalog if material analysis was successful
                        if material_analysis and material_analysis.get('material_type') != 'unknown':
                            await self._store_material_metadata(
                                document_id=document_id,
                                image_record=image_record,
                                material_analysis=material_analysis,
                                clip_embeddings=clip_embeddings
                            )

                        # Insert into database
                        result = supabase_client.client.table('document_images').insert(image_record).execute()

                        if result.data:
                            stats["images_stored"] += 1
                            stats["layout_links_created"] += len(associated_chunks)
                            if clip_embeddings.get('embedding_512') or clip_embeddings.get('embedding_1536'):
                                stats["clip_embeddings_generated"] += 1
                            if material_analysis:
                                stats["material_analyses_completed"] += 1

                        stats["images_processed"] += 1
                        self.logger.info(f"‚úÖ Processed image {i+1}/{total_images}: {contextual_name}")

                    except Exception as e:
                        self.logger.error(f"Failed to process image {i}: {e}")
                        continue

                # ‚úÖ MEMORY OPTIMIZATION: Clear memory after each batch
                # Force garbage collection to free memory
                gc.collect()

                # Log memory usage after batch
                mem_after = process.memory_info().rss / 1024 / 1024  # MB
                mem_freed = mem_before - mem_after
                self.logger.info(f"üß† Memory after batch {batch_start//BATCH_SIZE + 1}: {mem_after:.1f} MB (freed: {mem_freed:.1f} MB)")
                self.logger.info(f"‚úÖ Completed batch {batch_start//BATCH_SIZE + 1}/{(total_images + BATCH_SIZE - 1)//BATCH_SIZE}")

            # Clean up local image files and temp directory after processing
            self.logger.info("üßπ Cleaning up local image files and temp directory...")
            temp_dirs_to_clean = set()
            for image_info in extracted_images:
                try:
                    image_path = image_info.get('path')
                    if image_path:
                        # Track the temp directory for cleanup
                        # Path format: /tmp/pdf_processor_{doc_id}_images/images/{filename}
                        import os.path
                        temp_dir = os.path.dirname(os.path.dirname(image_path))  # Go up two levels
                        if 'pdf_processor_' in temp_dir:
                            temp_dirs_to_clean.add(temp_dir)
                except Exception as e:
                    self.logger.warning(f"Failed to track temp dir for {image_path}: {e}")

            # Clean up temp directories
            import shutil
            for temp_dir in temp_dirs_to_clean:
                try:
                    if os.path.exists(temp_dir):
                        shutil.rmtree(temp_dir)
                        self.logger.info(f"üßπ Cleaned up temp directory: {temp_dir}")
                except Exception as cleanup_error:
                    self.logger.warning(f"Failed to clean up temp directory {temp_dir}: {cleanup_error}")

            self.logger.info(f"üñºÔ∏è Image processing complete: {stats}")
            return stats

        except Exception as e:
            self.logger.error(f"Failed to process extracted images: {e}")
            return {"error": str(e)}

    def _extract_heading_hierarchy(self, nodes: List) -> List[Dict[str, Any]]:
        """Extract heading hierarchy from document nodes for contextual image naming."""
        import re

        headings = []
        for node in nodes:
            text = node.text
            lines = text.split('\n')

            for line_num, line in enumerate(lines):
                line = line.strip()
                # Match markdown headings (# ## ### etc.)
                heading_match = re.match(r'^(#{1,6})\s+(.+)', line)
                if heading_match:
                    level = len(heading_match.group(1))
                    title = heading_match.group(2).strip()
                    headings.append({
                        'level': level,
                        'title': title,
                        'text': line,
                        'node_index': nodes.index(node),
                        'line_number': line_num,
                        'chunk_id': node.metadata.get('chunk_id')
                    })

        return headings

    def _generate_contextual_image_name(
        self,
        image_info: Dict,
        heading_hierarchy: List[Dict],
        image_index: int
    ) -> tuple[str, str, int]:
        """Generate intelligent contextual name for image based on nearest heading and image properties."""
        import re

        # Default values
        default_name = f"image_{image_index + 1}"
        nearest_heading = "Document Image"
        heading_level = 0

        if not heading_hierarchy:
            return f"{default_name}.jpg", nearest_heading, heading_level

        # Enhanced heading selection based on image position and content
        best_heading = self._find_best_heading_for_image(image_info, heading_hierarchy)

        if best_heading:
            nearest_heading = best_heading['title']
            heading_level = best_heading['level']

            # Create intelligent filename based on heading content and image properties
            contextual_name = self._create_intelligent_filename(
                heading_title=nearest_heading,
                image_info=image_info,
                image_index=image_index
            )
        else:
            contextual_name = f"{default_name}.jpg"

        return contextual_name, nearest_heading, heading_level

    def _find_best_heading_for_image(self, image_info: Dict, heading_hierarchy: List[Dict]) -> Dict:
        """Find the most relevant heading for an image based on position and content."""
        if not heading_hierarchy:
            return None

        image_page = image_info.get('page_number', 1)
        image_bbox = image_info.get('bbox', {})
        image_y = image_bbox.get('y', 0)

        # Score headings based on relevance to image
        scored_headings = []

        for heading in heading_hierarchy:
            score = 0

            # Prefer headings from the same page
            if heading.get('page_number', 1) == image_page:
                score += 100

            # Prefer headings that appear before the image (in reading order)
            heading_y = heading.get('position_y', 0)
            if heading_y <= image_y:
                score += 50

            # Prefer more specific headings (higher level numbers = more specific)
            score += heading.get('level', 1) * 10

            # Prefer headings with material/technical keywords
            title_lower = heading.get('title', '').lower()
            material_keywords = ['material', 'specification', 'property', 'installation', 'technical', 'quality', 'performance']
            for keyword in material_keywords:
                if keyword in title_lower:
                    score += 20

            scored_headings.append((score, heading))

        # Return the highest scoring heading
        if scored_headings:
            scored_headings.sort(key=lambda x: x[0], reverse=True)
            return scored_headings[0][1]

        return heading_hierarchy[0]  # Fallback to first heading

    def _create_intelligent_filename(self, heading_title: str, image_info: Dict, image_index: int) -> str:
        """Create an intelligent filename based on heading content and image properties."""
        import re

        # Clean and process the heading title
        clean_title = re.sub(r'[^\w\s-]', '', heading_title.lower())
        clean_title = re.sub(r'[-\s]+', '-', clean_title)
        clean_title = clean_title.strip('-')

        # Add image type suffix based on image properties or heading content
        image_type_suffix = self._determine_image_type_suffix(heading_title, image_info)

        # Construct the filename
        if clean_title:
            if image_type_suffix:
                filename = f"{clean_title}-{image_type_suffix}-{image_index + 1}.jpg"
            else:
                filename = f"{clean_title}-{image_index + 1}.jpg"
        else:
            filename = f"image-{image_index + 1}.jpg"

        # Ensure filename is not too long
        if len(filename) > 100:
            # Truncate the clean_title part
            max_title_length = 100 - len(f"-{image_type_suffix}-{image_index + 1}.jpg") if image_type_suffix else 100 - len(f"-{image_index + 1}.jpg")
            clean_title = clean_title[:max_title_length]
            if image_type_suffix:
                filename = f"{clean_title}-{image_type_suffix}-{image_index + 1}.jpg"
            else:
                filename = f"{clean_title}-{image_index + 1}.jpg"

        return filename

    def _determine_image_type_suffix(self, heading_title: str, image_info: Dict) -> str:
        """Determine appropriate suffix for image based on context."""
        title_lower = heading_title.lower()

        # Map heading keywords to image type suffixes
        type_mappings = {
            'installation': 'install',
            'specification': 'spec',
            'technical': 'tech',
            'property': 'prop',
            'quality': 'quality',
            'maintenance': 'maint',
            'safety': 'safety',
            'performance': 'perf',
            'overview': 'overview',
            'diagram': 'diagram',
            'chart': 'chart',
            'table': 'table'
        }

        for keyword, suffix in type_mappings.items():
            if keyword in title_lower:
                return suffix

        # Check image properties for additional context
        image_type = image_info.get('image_type', '').lower()
        if 'diagram' in image_type:
            return 'diagram'
        elif 'chart' in image_type:
            return 'chart'
        elif 'table' in image_type:
            return 'table'

        return ''  # No specific suffix

    async def _get_associated_images(self, chunk_id: str, document_id: str) -> List[Dict[str, Any]]:
        """Get images associated with a specific chunk for multimodal search results."""
        try:
            from .supabase_client import get_supabase_client
            supabase_client = get_supabase_client()

            # Query for images associated with this chunk or document
            images_response = supabase_client.client.table('document_images').select('*').or_(
                f'chunk_id.eq.{chunk_id},document_id.eq.{document_id}'
            ).execute()

            if not images_response.data:
                return []

            associated_images = []
            for image_record in images_response.data:
                # Calculate relevance score based on chunk association
                relevance_score = 1.0 if image_record.get('chunk_id') == chunk_id else 0.7

                image_info = {
                    "image_id": image_record.get('id'),
                    "image_url": image_record.get('image_url'),
                    "contextual_name": image_record.get('contextual_name'),
                    "caption": image_record.get('caption'),
                    "alt_text": image_record.get('alt_text'),
                    "nearest_heading": image_record.get('nearest_heading'),
                    "heading_level": image_record.get('heading_level'),
                    "page_number": image_record.get('page_number'),
                    "relevance_score": relevance_score,
                    "extraction_confidence": image_record.get('extraction_confidence', 0.0),
                    "layout_context": image_record.get('layout_context', {}),
                    "material_analysis": image_record.get('material_analysis', {}),
                    "has_visual_embeddings": bool(
                        image_record.get('visual_embedding_512') or
                        image_record.get('visual_embedding_1536')
                    )
                }

                associated_images.append(image_info)

            # Sort by relevance score (chunk-specific images first)
            associated_images.sort(key=lambda x: x['relevance_score'], reverse=True)

            return associated_images

        except Exception as e:
            self.logger.error(f"Failed to get associated images for chunk {chunk_id}: {e}")
            return []

    async def search_with_visual_similarity(
        self,
        query: str = None,
        image_query: str = None,
        document_id: str = None,
        k: int = 5,
        similarity_threshold: float = 0.7,
        include_images: bool = True,
        lambda_mult: float = 0.5
    ) -> Dict[str, Any]:
        """
        Enhanced search with visual similarity capabilities.
        Supports both text and image-based queries with multimodal results.
        """
        try:
            start_time = time.time()

            # If image query is provided, convert to text query using CLIP
            if image_query and not query:
                query = await self._image_to_text_query(image_query)
                if not query:
                    return {
                        "results": [],
                        "message": "Failed to process image query",
                        "total_results": 0,
                        "processing_time": time.time() - start_time
                    }

            # Perform standard text search
            search_results = await self.search_documents(
                query=query,
                document_id=document_id,
                k=k,
                query_type="semantic",
                lambda_mult=lambda_mult
            )

            # If visual similarity is requested, enhance results with visual matching
            if include_images and image_query:
                enhanced_results = await self._enhance_with_visual_similarity(
                    search_results.get('results', []),
                    image_query,
                    similarity_threshold
                )
                search_results['results'] = enhanced_results
                search_results['enhanced_with_visual'] = True

            search_results['processing_time'] = time.time() - start_time
            return search_results

        except Exception as e:
            self.logger.error(f"Visual similarity search failed: {e}")
            return {
                "results": [],
                "message": f"Visual search failed: {str(e)}",
                "total_results": 0,
                "processing_time": time.time() - start_time
            }

    async def _image_to_text_query(self, image_query: str) -> str:
        """Convert image query to text query using vision analysis."""
        try:
            # Use existing material analysis service to understand the image
            from .together_ai_service import analyze_material_image_base64

            # Analyze the image to generate a text description
            analysis_result = await analyze_material_image_base64(
                image_base64=image_query,
                context="Generate a detailed description of this material for search purposes"
            )

            if analysis_result and hasattr(analysis_result, 'material_description'):
                return analysis_result.material_description

            return "material image query"  # Fallback

        except Exception as e:
            self.logger.error(f"Failed to convert image to text query: {e}")
            return None

    async def _enhance_with_visual_similarity(
        self,
        text_results: List[Dict],
        image_query: str,
        similarity_threshold: float
    ) -> List[Dict]:
        """Enhance text search results with visual similarity scoring."""
        try:
            # This would implement visual similarity comparison
            # For now, return the original results with visual enhancement flags
            for result in text_results:
                result['visual_similarity_available'] = True
                result['visual_similarity_score'] = 0.8  # Placeholder

            return text_results

        except Exception as e:
            self.logger.error(f"Failed to enhance with visual similarity: {e}")
            return text_results

    def _find_associated_chunks(self, image_info: Dict, nodes: List) -> List:
        """Find text chunks that are contextually related to the image."""
        # For now, associate with all chunks from the same page
        # This can be enhanced with actual layout analysis
        image_page = image_info.get('page_number', 1)

        associated_chunks = []
        for node in nodes:
            # If we have page information in metadata, use it
            node_page = node.metadata.get('page_number', 1)
            if node_page == image_page:
                associated_chunks.append(node)

        # If no page-based association, associate with first few chunks
        if not associated_chunks and nodes:
            associated_chunks = nodes[:2]  # Associate with first 2 chunks

        return associated_chunks

    async def _generate_clip_embeddings(self, image_base64: str, image_path: str) -> Dict[str, Any]:
        """Generate CLIP embeddings using existing MIVAA gateway service."""
        try:
            import aiohttp
            import json
            import asyncio

            self.logger.info(f"üîó Generating CLIP embeddings for image: {os.path.basename(image_path)}")

            # CRITICAL FIX: Reuse the shared embedding_service instance instead of creating new ones
            # This prevents memory accumulation from loading CLIP model for every image
            if not hasattr(self, 'embedding_service') or self.embedding_service is None:
                self.logger.warning("‚ö†Ô∏è Embedding service not initialized, creating new instance")
                from .real_embeddings_service import RealEmbeddingsService
                self.embedding_service = RealEmbeddingsService()

            # Generate ALL embeddings (CLIP, color, texture, application) using the SHARED service
            # CRITICAL FIX: Use image_data parameter for base64 string, not image_url
            embedding_result = await self.embedding_service.generate_all_embeddings(
                entity_id="temp",
                entity_type="image",
                text_content="",
                image_data=image_base64,  # Fixed: was image_url=image_base64
                material_properties={}
            )

            if embedding_result and embedding_result.get('success'):
                embeddings = embedding_result.get('embeddings', {})
                metadata = embedding_result.get('metadata', {})

                # Get CLIP embedding only (material properties stored in metadata)
                clip_embedding = embeddings.get('visual_512')  # CLIP 512D

                self.logger.info(f"‚úÖ Generated CLIP embedding: {len(clip_embedding) if clip_embedding else 0}D")

                return {
                    "embedding_512": clip_embedding,  # CLIP 512D
                    "embedding_1536": None,  # Not available
                    "model_used": metadata.get('model_versions', {}).get('visual', 'ViT-B/32'),
                    "processing_time_ms": metadata.get('processing_time_ms', 0),
                    "confidence_score": 1.0  # Default confidence
                }
            else:
                self.logger.warning(f"CLIP embedding generation failed or returned no results")
                return {}

        except Exception as e:
            self.logger.error(f"Failed to generate CLIP embeddings: {e}")
            # Fallback: Try direct HTTP call to MIVAA gateway
            try:
                return await self._fallback_clip_generation(image_base64, image_path)
            except Exception as fallback_error:
                self.logger.error(f"Fallback CLIP generation also failed: {fallback_error}")
                return {}

    async def _fallback_clip_generation(self, image_base64: str, image_path: str) -> Dict[str, Any]:
        """Fallback method for CLIP generation using direct HTTP calls."""
        try:
            import aiohttp
            import json

            # Direct call to MIVAA service for CLIP embeddings
            mivaa_url = "http://localhost:8000/api/embeddings/clip-generate"

            payload = {
                "image_data": image_base64,
                "embedding_type": "visual_similarity",
                "options": {
                    "normalize": True,
                    "dimensions": [512, 1536]  # Request both dimensions
                }
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mivaa_url,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        self.logger.info(f"‚úÖ Fallback CLIP generation successful")
                        return {
                            "embedding_512": result.get('embedding_512'),
                            "embedding_1536": result.get('embedding_1536'),
                            "model_used": result.get('model_used', 'clip-fallback'),
                            "processing_time_ms": result.get('processing_time_ms', 0),
                            "confidence_score": result.get('confidence_score', 0.0)
                        }
                    else:
                        self.logger.error(f"Fallback CLIP generation failed: {response.status}")
                        return {}

        except Exception as e:
            self.logger.error(f"Fallback CLIP generation error: {e}")
            return {}

    async def _analyze_image_material(
        self,
        image_base64: str,
        image_path: str,
        image_id: Optional[str] = None,
        document_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze image for material properties using Llama-only analysis.

        NEW ARCHITECTURE:
        - Llama 4 Scout Vision for sync processing
        - Queue Claude validation for low-quality images (score < 0.7)
        - Claude runs async before product creation

        Args:
            image_base64: Base64-encoded image data
            image_path: Path to image file (for logging)
            image_id: Database image ID (for Claude validation queue)
            document_id: Document ID (for Claude validation queue)
        """
        try:
            self.logger.info(f"üî¨ Analyzing material properties for image: {os.path.basename(image_path)}")

            # Use RealImageAnalysisService for Llama-only analysis
            from .real_image_analysis_service import RealImageAnalysisService

            analysis_service = RealImageAnalysisService()

            # Use actual image_id if provided, otherwise fallback to filename
            analysis_image_id = image_id if image_id else os.path.basename(image_path)

            # Analyze image using Llama 4 Scout Vision ONLY (prevents OOM)
            result = await analysis_service.analyze_image_from_base64(
                image_base64=image_base64,
                image_id=analysis_image_id,  # Use actual image ID for Claude validation queue
                context={},
                document_id=document_id  # Pass document_id for Claude validation queuing
            )

            # Return comprehensive analysis with both models' results
            return {
                'llama_analysis': result.llama_analysis,
                'claude_validation': result.claude_validation,
                'material_properties': result.material_properties,
                'quality_score': result.quality_score,
                'confidence_score': result.confidence_score,
                'processing_time_ms': result.processing_time_ms,
                'analysis_method': 'llama_claude_dual_vision',
                'timestamp': result.timestamp
            }

        except Exception as e:
            self.logger.error(f"Failed to analyze image material with Llama + Claude: {e}")
            # Fallback: Try direct HTTP call to material analysis endpoint
            try:
                return await self._fallback_material_analysis(image_base64, image_path)
            except Exception as fallback_error:
                self.logger.error(f"Fallback material analysis also failed: {fallback_error}")
                return {}

    async def _call_claude_image_analysis(self, image_base64: str, image_path: str) -> Dict[str, Any]:
        """Call Claude Vision API directly for comprehensive image analysis."""
        try:
            import anthropic
            import json

            self.logger.info(f"ü§ñ Calling Claude Vision API for image analysis: {os.path.basename(image_path)}")

            # Use centralized AI client service
            ai_service = get_ai_client_service()
            anthropic_client = ai_service.anthropic

            # Build prompt for material analysis
            prompt = """You are an expert material and product analyst. Analyze this image and provide detailed material properties in JSON format:

{
  "material_type": "<primary material type>",
  "color": "<dominant color>",
  "texture": "<surface texture>",
  "finish": "<surface finish>",
  "pattern": "<visible pattern>",
  "confidence": <0-1 confidence score>,
  "quality_score": <0-1 image quality>,
  "validation_status": "<valid/needs_review/invalid>",
  "content_description": "<what you see>",
  "materials_identified": ["<material1>", "<material2>"],
  "issues": ["<any quality issues>"],
  "recommendations": ["<improvement suggestions>"]
}

Focus on identifying construction materials, tiles, flooring, wall coverings, and architectural elements."""

            # Call Claude Vision API directly
            response = anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2048,
                messages=[{
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/jpeg",
                                "data": image_base64
                            }
                        },
                        {
                            "type": "text",
                            "text": prompt
                        }
                    ]
                }]
            )

            # Parse response
            response_text = response.content[0].text.strip()
            try:
                # Handle Claude's tendency to add extra text after JSON
                # Find the last closing brace to extract only the JSON portion
                last_brace = response_text.rfind('}')
                if last_brace != -1:
                    json_text = response_text[:last_brace + 1]
                    analysis_result = json.loads(json_text)
                else:
                    # No JSON found, raise error to trigger fallback
                    raise json.JSONDecodeError("No JSON object found", response_text, 0)

            except json.JSONDecodeError as e:
                # If JSON parsing fails, extract what we can
                self.logger.warning(f"Failed to parse Claude response as JSON: {e}")
                self.logger.debug(f"Raw response (first 500 chars): {response_text[:500]}")
                analysis_result = {
                    "material_type": "unknown",
                    "color": "unknown",
                    "texture": "unknown",
                    "finish": "unknown",
                    "pattern": "unknown",
                    "confidence": 0.0,
                    "quality_score": 0.5,
                    "validation_status": "needs_review",
                    "content_description": response_text[:200],
                    "materials_identified": [],
                    "issues": [f"Failed to parse Claude response: {str(e)}"],
                    "recommendations": []
                }

            # Transform Claude response to our expected format
            claude_analysis = {
                "success": True,
                "analysis": {
                    "material_type": analysis_result.get('material_type', 'unknown'),
                    "color": analysis_result.get('color', 'unknown'),
                    "texture": analysis_result.get('texture', 'unknown'),
                    "finish": analysis_result.get('finish', 'unknown'),
                    "pattern": analysis_result.get('pattern', 'unknown'),
                    "confidence": analysis_result.get('confidence', 0.0),
                    "quality_score": analysis_result.get('quality_score', 0.0),
                    "validation_status": analysis_result.get('validation_status', 'unknown'),
                    "extracted_features": {
                        "content_description": analysis_result.get('content_description', ''),
                        "materials_identified": analysis_result.get('materials_identified', []),
                        "issues": analysis_result.get('issues', []),
                        "recommendations": analysis_result.get('recommendations', [])
                    },
                    "composition": {},
                    "mechanical_properties": {},
                    "thermal_properties": {},
                    "safety_ratings": {}
                },
                "processing_time_ms": 0
            }

            self.logger.info(f"‚úÖ Claude analysis successful: {claude_analysis['analysis']['material_type']} (confidence: {claude_analysis['analysis']['confidence']:.3f})")
            return claude_analysis

        except Exception as e:
            self.logger.error(f"Failed to call Claude Vision API: {e}")
            return {"success": False, "error": str(e)}

    async def _fallback_material_analysis(self, image_base64: str, image_path: str) -> Dict[str, Any]:
        """Fallback method for material analysis using direct HTTP calls."""
        try:
            import aiohttp
            import json

            # Direct call to MIVAA service for material analysis
            mivaa_url = "http://localhost:8000/api/materials/analyze"

            payload = {
                "image_data": image_base64,
                "analysis_types": ["visual", "spectral", "chemical"],
                "options": {
                    "include_properties": True,
                    "include_safety": True,
                    "confidence_threshold": 0.3
                }
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    mivaa_url,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=45)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        self.logger.info(f"‚úÖ Fallback material analysis successful")
                        return {
                            "material_type": result.get('material_type', 'unknown'),
                            "properties": result.get('properties', {}),
                            "confidence": result.get('confidence', 0.0),
                            "analysis_method": "fallback_material_analysis",
                            "processing_time_ms": result.get('processing_time_ms', 0)
                        }
                    else:
                        self.logger.error(f"Fallback material analysis failed: {response.status}")
                        return {}

        except Exception as e:
            self.logger.error(f"Fallback material analysis error: {e}")
            return {}

    async def _store_material_metadata(
        self,
        document_id: str,
        image_record: Dict[str, Any],
        material_analysis: Dict[str, Any],
        clip_embeddings: Dict[str, Any]
    ) -> bool:
        """Store extracted material metadata in the materials_catalog table."""
        try:
            from .supabase_client import get_supabase_client
            import uuid

            supabase_client = get_supabase_client()

            # Extract material properties from analysis
            properties = material_analysis.get('properties', {})

            # Create comprehensive material catalog entry with proper metafields
            material_record = {
                'id': str(uuid.uuid4()),
                'name': f"{material_analysis.get('material_type', 'Unknown')} - {image_record.get('contextual_name', 'Material')}",
                'description': f"Material extracted from document analysis: {material_analysis.get('material_type', 'Unknown material')}",
                'category': self._determine_material_category(material_analysis.get('material_type', '')),

                # Core properties as JSONB
                'properties': properties,
                'chemical_composition': properties.get('composition', {}),
                'safety_data': properties.get('safety_ratings', {}),
                'standards': self._extract_standards_from_analysis(material_analysis),

                # Specific metafields (arrays and specific types)
                'finish': [properties.get('finish', 'unknown')] if properties.get('finish') else [],
                'size': self._extract_size_array(properties),
                'installation_method': self._extract_installation_methods(properties),
                'application': self._extract_applications(properties),
                'metal_types': self._extract_metal_types(material_analysis),

                # Performance and safety metafields (JSONB)
                'slip_safety_ratings': self._format_slip_safety_ratings(properties),
                'surface_gloss_reflectivity': self._format_surface_gloss(properties),
                'mechanical_properties': self._format_mechanical_properties(properties),
                'thermal_properties': self._format_thermal_properties(properties),
                'water_moisture_resistance': self._format_water_resistance(properties),
                'chemical_hygiene_resistance': self._format_chemical_resistance(properties),
                'acoustic_electrical_properties': self._format_acoustic_electrical(properties),
                'environmental_sustainability': self._format_environmental_properties(properties),
                'dimensional_aesthetic': self._format_dimensional_aesthetic(properties),

                # Visual embeddings
                'visual_embedding_512': clip_embeddings.get('embedding_512'),
                'visual_embedding_1536': clip_embeddings.get('embedding_1536'),

                # Analysis metadata
                'llama_analysis': material_analysis,
                'visual_analysis_confidence': material_analysis.get('confidence', 0.0),
                'extracted_properties': properties,
                'confidence_scores': {
                    'material_identification': material_analysis.get('confidence', 0.0),
                    'property_extraction': properties.get('extraction_confidence', 0.0),
                    'visual_analysis': clip_embeddings.get('confidence_score', 0.0)
                },
                'last_ai_extraction_at': 'now()',
                'extracted_entities': self._extract_entities_from_analysis(material_analysis, properties)
            }

            # Insert into materials_catalog table
            result = supabase_client.client.table('materials_catalog').insert(material_record).execute()

            if result.data:
                self.logger.info(f"‚úÖ Stored material metadata: {material_record['name']} (confidence: {material_record['visual_analysis_confidence']:.3f})")
                return True
            else:
                self.logger.warning(f"Failed to store material metadata: No data returned")
                return False

        except Exception as e:
            self.logger.error(f"Failed to store material metadata: {e}")
            return False

    def _determine_material_category(self, material_type: str) -> str:
        """Determine the appropriate category for a material type.
        Returns lowercase enum value with underscores (e.g., 'ceramic_tile', not 'Ceramics & Tiles')
        """
        material_type_lower = material_type.lower()

        # Map keywords to database enum values (lowercase with underscores)
        category_mappings = {
            'ceramic': 'ceramic_tile',
            'tile': 'ceramic_tile',
            'porcelain': 'porcelain_tile',
            'stone': 'natural_stone_tile',
            'marble': 'marble',
            'granite': 'granite',
            'travertine': 'travertine',
            'slate': 'slate',
            'limestone': 'limestone',
            'quartzite': 'quartzite',
            'sandstone': 'sandstone',
            'onyx': 'onyx',
            'wood': 'wood',
            'timber': 'wood',
            'metal': 'metal_tile',
            'steel': 'metal_tile',
            'aluminum': 'metal_tile',
            'fabric': 'textiles',
            'textile': 'textiles',
            'cotton': 'textiles',
            'concrete': 'concrete',
            'brick': 'concrete',
            'glass': 'glass',
            'plastic': 'plastics',
            'polymer': 'plastics',
            'vinyl': 'vinyl',
            'laminate': 'laminate',
            'carpet': 'carpet',
            'cork': 'cork',
            'bamboo': 'bamboo',
            'terrazzo': 'terrazzo',
            'quartz': 'quartz',
            'mosaic': 'mosaic'
        }

        for keyword, category in category_mappings.items():
            if keyword in material_type_lower:
                return category

        return 'other'  # Default category (lowercase)

    def _extract_standards_from_analysis(self, material_analysis: Dict[str, Any]) -> List[str]:
        """Extract standards and certifications from material analysis."""
        standards = []

        # Look for standards in various places
        if 'standards' in material_analysis:
            if isinstance(material_analysis['standards'], list):
                standards.extend(material_analysis['standards'])
            elif isinstance(material_analysis['standards'], str):
                standards.append(material_analysis['standards'])

        # Look for common standards patterns in text
        properties = material_analysis.get('properties', {})
        text_content = str(material_analysis) + str(properties)

        common_standards = [
            'ISO 13006', 'EN 14411', 'ANSI A137.1', 'ASTM C648', 'ISO 10545',
            'BS 7976', 'DIN 51130', 'DIN 51097', 'EN 12004', 'ISO 9001',
            'ISO 14001', 'GREENGUARD', 'LEED'
        ]

        for standard in common_standards:
            if standard in text_content and standard not in standards:
                standards.append(standard)

        return standards[:5]  # Limit to 5 standards

    def _extract_size_array(self, properties: Dict[str, Any]) -> List[str]:
        """Extract size information as array."""
        sizes = []

        if 'size' in properties:
            if isinstance(properties['size'], list):
                sizes.extend([str(s) for s in properties['size']])
            else:
                sizes.append(str(properties['size']))

        # Look for dimensional information
        if 'dimensions' in properties:
            sizes.append(str(properties['dimensions']))

        # Look for common size patterns
        if 'width' in properties and 'height' in properties:
            sizes.append(f"{properties['width']}x{properties['height']}")

        return sizes[:3]  # Limit to 3 sizes

    def _extract_installation_methods(self, properties: Dict[str, Any]) -> List[str]:
        """Extract installation methods as array."""
        methods = []

        if 'installation_method' in properties:
            if isinstance(properties['installation_method'], list):
                methods.extend(properties['installation_method'])
            else:
                methods.append(str(properties['installation_method']))

        # Look for installation-related keywords
        text_content = str(properties).lower()
        installation_keywords = [
            'adhesive', 'mechanical', 'nail down', 'floating', 'glue down',
            'click lock', 'tongue and groove', 'mortar', 'cement', 'epoxy'
        ]

        for keyword in installation_keywords:
            if keyword in text_content and keyword not in methods:
                methods.append(keyword)

        return methods[:3]  # Limit to 3 methods

    def _extract_applications(self, properties: Dict[str, Any]) -> List[str]:
        """Extract application areas as array."""
        applications = []

        if 'application' in properties:
            if isinstance(properties['application'], list):
                applications.extend(properties['application'])
            else:
                applications.append(str(properties['application']))

        # Look for application keywords
        text_content = str(properties).lower()
        application_keywords = [
            'interior', 'exterior', 'commercial', 'residential', 'industrial',
            'floor', 'wall', 'ceiling', 'backsplash', 'countertop', 'wet areas'
        ]

        for keyword in application_keywords:
            if keyword in text_content and keyword not in applications:
                applications.append(keyword)

        return applications[:4]  # Limit to 4 applications

    def _extract_metal_types(self, material_analysis: Dict[str, Any]) -> List[str]:
        """Extract metal types if material is metal."""
        metal_types = []

        material_type = material_analysis.get('material_type', '').lower()
        if 'metal' not in material_type and 'steel' not in material_type and 'aluminum' not in material_type:
            return metal_types

        # Look for specific metal types
        text_content = str(material_analysis).lower()
        metal_keywords = [
            'stainless steel', 'carbon steel', 'aluminum', 'copper', 'brass',
            'bronze', 'titanium', 'zinc', 'nickel', 'chrome'
        ]

        for metal in metal_keywords:
            if metal in text_content:
                metal_types.append(metal)

        return metal_types[:3]  # Limit to 3 metal types

    def _format_slip_safety_ratings(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format slip safety ratings as JSONB."""
        slip_ratings = {}

        safety_ratings = properties.get('safety_ratings', {})
        if 'slip_resistance' in safety_ratings:
            slip_ratings = safety_ratings['slip_resistance']

        # Look for R-ratings and other slip resistance values
        text_content = str(properties).upper()

        # Extract R-ratings (R9, R10, R11, R12, R13)
        import re
        r_ratings = re.findall(r'R(\d{1,2})', text_content)
        if r_ratings:
            slip_ratings['r_rating'] = f"R{max(r_ratings)}"

        # Extract pendulum test values
        pendulum_matches = re.findall(r'(\d+)\+?\s*(?:PTV|pendulum)', text_content.lower())
        if pendulum_matches:
            slip_ratings['pendulum_test_value'] = int(pendulum_matches[0])

        # Extract ramp test values
        ramp_matches = re.findall(r'(\d+)¬∞?\s*(?:ramp|slope)', text_content.lower())
        if ramp_matches:
            slip_ratings['ramp_test_degrees'] = int(ramp_matches[0])

        return slip_ratings

    def _format_surface_gloss(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format surface gloss and reflectivity as JSONB."""
        surface_props = {}

        if 'surface_properties' in properties:
            surface_props = properties['surface_properties'].copy()

        # Extract gloss values
        if 'gloss' in properties:
            surface_props['gloss_level'] = properties['gloss']

        # Look for gloss units (GU)
        text_content = str(properties)
        import re
        gloss_matches = re.findall(r'(\d+(?:\.\d+)?)\s*(?:GU|gloss\s*units?)', text_content.lower())
        if gloss_matches:
            surface_props['gloss_units'] = float(gloss_matches[0])

        # Extract finish type
        if 'finish' in properties:
            surface_props['finish_type'] = properties['finish']

        return surface_props

    def _format_mechanical_properties(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format mechanical properties as JSONB."""
        mechanical = {}

        if 'mechanical_properties' in properties:
            mechanical = properties['mechanical_properties'].copy()

        # Extract common mechanical properties
        prop_mappings = {
            'flexural_strength': ['flexural strength', 'bending strength'],
            'tensile_strength': ['tensile strength', 'ultimate tensile'],
            'compressive_strength': ['compressive strength', 'compression'],
            'breaking_strength': ['breaking strength', 'break strength'],
            'modulus_of_rupture': ['modulus of rupture', 'mor'],
            'elastic_modulus': ['elastic modulus', 'young\'s modulus']
        }

        text_content = str(properties).lower()
        import re

        for prop_key, keywords in prop_mappings.items():
            for keyword in keywords:
                # Look for values like "35 MPa", "1300 N", etc.
                pattern = rf'{keyword}[:\s]*(\d+(?:\.\d+)?)\s*(mpa|n|psi|ksi)'
                matches = re.findall(pattern, text_content)
                if matches and prop_key not in mechanical:
                    value, unit = matches[0]
                    mechanical[prop_key] = {
                        'value': float(value),
                        'unit': unit.upper()
                    }

        return mechanical

    def _format_thermal_properties(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format thermal properties as JSONB."""
        thermal = {}

        if 'thermal_properties' in properties:
            thermal = properties['thermal_properties'].copy()

        # Extract thermal properties
        text_content = str(properties).lower()
        import re

        # Thermal conductivity
        thermal_cond_matches = re.findall(r'thermal\s*conductivity[:\s]*(\d+(?:\.\d+)?)\s*(w/mk|w/m¬∑k)', text_content)
        if thermal_cond_matches:
            value, unit = thermal_cond_matches[0]
            thermal['thermal_conductivity'] = {
                'value': float(value),
                'unit': unit
            }

        # Thermal expansion
        expansion_matches = re.findall(r'thermal\s*expansion[:\s]*(\d+(?:\.\d+)?)\s*√ó?\s*10[‚Åª-](\d+)\s*/¬∞?c', text_content)
        if expansion_matches:
            value, exponent = expansion_matches[0]
            thermal['thermal_expansion'] = {
                'value': float(value),
                'exponent': int(exponent),
                'unit': '/¬∞C'
            }

        # Temperature resistance
        temp_matches = re.findall(r'(\d+)¬∞?c\s*(?:temperature|thermal|heat)', text_content)
        if temp_matches:
            thermal['max_temperature'] = {
                'value': int(temp_matches[0]),
                'unit': '¬∞C'
            }

        return thermal

    def _format_water_resistance(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format water and moisture resistance as JSONB."""
        water_resistance = {}

        if 'resistance_properties' in properties and 'water' in properties['resistance_properties']:
            water_resistance = properties['resistance_properties']['water'].copy()

        # Extract water absorption values
        text_content = str(properties).lower()
        import re

        # Water absorption percentage
        absorption_matches = re.findall(r'water\s*absorption[:\s]*[<‚â§]?\s*(\d+(?:\.\d+)?)\s*%', text_content)
        if absorption_matches:
            water_resistance['water_absorption'] = {
                'value': float(absorption_matches[0]),
                'unit': '%'
            }

        # Frost resistance
        if 'frost' in text_content and 'resistant' in text_content:
            water_resistance['frost_resistant'] = True

        return water_resistance

    def _format_chemical_resistance(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format chemical and hygiene resistance as JSONB."""
        chemical_resistance = {}

        if 'resistance_properties' in properties and 'chemical' in properties['resistance_properties']:
            chemical_resistance = properties['resistance_properties']['chemical'].copy()

        # Extract chemical resistance class
        text_content = str(properties).upper()
        import re

        # Chemical resistance class (A, B, C)
        class_matches = re.findall(r'class\s*([ABC])\s*(?:chemical|resistance)', text_content)
        if class_matches:
            chemical_resistance['resistance_class'] = class_matches[0]

        # Stain resistance class
        stain_matches = re.findall(r'stain\s*resistance[:\s]*class\s*(\d+)', text_content.lower())
        if stain_matches:
            chemical_resistance['stain_resistance_class'] = int(stain_matches[0])

        return chemical_resistance

    def _format_acoustic_electrical(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format acoustic and electrical properties as JSONB."""
        acoustic_electrical = {}

        # Extract any acoustic or electrical properties
        text_content = str(properties).lower()

        if 'acoustic' in text_content or 'sound' in text_content:
            acoustic_electrical['has_acoustic_properties'] = True

        if 'electrical' in text_content or 'conductivity' in text_content:
            acoustic_electrical['has_electrical_properties'] = True

        return acoustic_electrical

    def _format_environmental_properties(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format environmental and sustainability properties as JSONB."""
        environmental = {}

        text_content = str(properties).lower()

        # Look for sustainability keywords
        sustainability_keywords = [
            'sustainable', 'eco-friendly', 'recycled', 'renewable', 'green',
            'leed', 'greenguard', 'carbon neutral', 'low voc'
        ]

        for keyword in sustainability_keywords:
            if keyword in text_content:
                environmental['sustainability_features'] = environmental.get('sustainability_features', [])
                environmental['sustainability_features'].append(keyword)

        # Fire rating
        import re
        fire_ratings = re.findall(r'fire\s*rating[:\s]*([a-z0-9]+)', text_content)
        if fire_ratings:
            environmental['fire_rating'] = fire_ratings[0].upper()

        return environmental

    def _format_dimensional_aesthetic(self, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Format dimensional and aesthetic properties as JSONB."""
        dimensional = {}

        # Extract color, pattern, texture information
        if 'color' in properties:
            dimensional['color'] = properties['color']

        if 'pattern' in properties:
            dimensional['pattern'] = properties['pattern']

        if 'texture' in properties:
            dimensional['texture'] = properties['texture']

        # Extract dimensional information
        text_content = str(properties).lower()
        import re

        # Thickness
        thickness_matches = re.findall(r'thickness[:\s]*(\d+(?:\.\d+)?)\s*(mm|cm|inch)', text_content)
        if thickness_matches:
            value, unit = thickness_matches[0]
            dimensional['thickness'] = {
                'value': float(value),
                'unit': unit
            }

        # Warpage tolerance
        warpage_matches = re.findall(r'warpage[:\s]*[‚â§<¬±]?\s*(\d+(?:\.\d+)?)\s*%', text_content)
        if warpage_matches:
            dimensional['warpage_tolerance'] = {
                'value': float(warpage_matches[0]),
                'unit': '%'
            }

        return dimensional

    def _extract_entities_from_analysis(self, material_analysis: Dict[str, Any], properties: Dict[str, Any]) -> Dict[str, Any]:
        """Extract named entities from material analysis."""
        entities = {
            'material_type': material_analysis.get('material_type', 'unknown'),
            'confidence': material_analysis.get('confidence', 0.0),
            'extraction_timestamp': 'now()',
            'processing_method': 'enhanced_multimodal_rag'
        }

        # Add key properties as entities
        if 'color' in properties:
            entities['color'] = properties['color']

        if 'finish' in properties:
            entities['finish'] = properties['finish']

        if 'application' in properties:
            entities['application'] = properties['application']

        return entities

    async def _store_detected_products(
        self,
        products: List,
        document_id: str,
        workspace_id: str,
        supabase_client: Any
    ) -> None:
        """
        Store vision-detected products in the database.

        Products detected via Llama 4 Scout Vision are stored in the products table
        with metadata indicating they were detected via vision analysis.
        """
        try:
            for product in products:
                # Build product record
                product_record = {
                    'workspace_id': workspace_id,
                    'name': product.product_name,
                    'product_code': product.product_code,
                    'description': f"Detected from page {product.page_number}",
                    'category': 'vision_detected',  # Mark as vision-detected
                    'metadata': {
                        'detection_method': 'llama_4_scout_vision',
                        'confidence': product.confidence,
                        'page_number': product.page_number,
                        'dimensions': product.dimensions,
                        'colors': product.colors,
                        'materials': product.materials,
                        'finish': product.finish,
                        'pattern': product.pattern,
                        'designer': product.designer,
                        'collection': product.collection,
                        'raw_analysis': product.raw_analysis,
                        'source_document_id': document_id
                    },
                    'created_at': datetime.utcnow().isoformat(),
                    'updated_at': datetime.utcnow().isoformat()
                }

                # Insert into products table
                result = supabase_client.client.table('products').insert(product_record).execute()

                if result.data:
                    self.logger.info(f"‚úÖ Stored product: {product.product_name} (confidence: {product.confidence:.2f})")
                else:
                    self.logger.warning(f"‚ö†Ô∏è Failed to store product: {product.product_name}")

        except Exception as e:
            self.logger.error(f"‚ùå Failed to store detected products: {e}")

    async def _apply_metadata_to_products(
        self,
        document_id: str,
        nodes: List,
        detected_products: List,
        supabase_client: Any
    ) -> Dict[str, Any]:
        """
        Apply metadata to products with scope detection and override logic.

        This method:
        1. Detects scope for each chunk (product-specific vs catalog-general)
        2. Applies metadata in correct order (catalog-general FIRST, product-specific LAST)
        3. Tracks overrides when product-specific metadata overrides catalog-general

        Args:
            document_id: Document ID
            nodes: List of document chunks
            detected_products: List of detected products
            supabase_client: Supabase client

        Returns:
            Dict with metadata application statistics
        """
        try:
            from app.services.dynamic_metadata_extractor import MetadataScopeDetector
            from app.services.metadata_application_service import MetadataApplicationService

            # Initialize services
            scope_detector = MetadataScopeDetector()
            metadata_service = MetadataApplicationService(supabase_client)

            # Get product names for scope detection
            product_names = [p.product_name for p in detected_products]

            # Detect scope for each chunk
            chunks_with_scope = []
            for node in nodes:
                try:
                    scope_result = await scope_detector.detect_scope(
                        chunk_content=node.text,
                        product_names=product_names,
                        document_context=None
                    )

                    chunks_with_scope.append({
                        'chunk_id': node.metadata.get('db_chunk_id'),
                        'content': node.text,
                        'scope': scope_result['scope'],
                        'applies_to': scope_result['applies_to'],
                        'extracted_metadata': scope_result['extracted_metadata'],
                        'is_override': scope_result['is_override'],
                        'confidence': scope_result['confidence']
                    })

                except Exception as scope_error:
                    self.logger.warning(f"Failed to detect scope for chunk: {scope_error}")
                    continue

            # Apply metadata to products
            result = await metadata_service.apply_metadata_to_products(
                document_id=document_id,
                chunks_with_scope=chunks_with_scope
            )

            return result

        except Exception as e:
            self.logger.error(f"‚ùå Failed to apply metadata to products: {e}", exc_info=True)
            return {
                'products_updated': 0,
                'overrides_detected': 0,
                'error': str(e)
            }

    # ============================================================================
    # MULTI-STRATEGY SEARCH METHODS (Issue #54)
    # ============================================================================

    async def multi_vector_search(
        self,
        query: str,
        workspace_id: str,
        top_k: int = 10,
        material_filters: Optional[Dict[str, Any]] = None,
        text_weight: float = 0.20,
        visual_weight: float = 0.20,
        color_weight: float = 0.15,
        texture_weight: float = 0.15,
        style_weight: float = 0.15,
        material_weight: float = 0.15,
        similarity_threshold: float = 0.7
    ) -> Dict[str, Any]:
        """
        üéØ ENHANCED Multi-vector search combining 6 specialized CLIP embeddings + metadata filtering.

        Combines:
        - text_embedding_1536 (20% weight) - Semantic understanding
        - visual_clip_embedding_512 (20% weight) - Visual similarity
        - color_clip_embedding_512 (15% weight) - Color palette matching
        - texture_clip_embedding_512 (15% weight) - Texture pattern matching
        - style_clip_embedding_512 (15% weight) - Design style matching
        - material_clip_embedding_512 (15% weight) - Material type matching

        + JSONB metadata filtering for properties (waterproof, outdoor, finish, etc.)

        Args:
            query: Search query text
            workspace_id: Workspace ID to filter results
            top_k: Number of results to return
            material_filters: Optional JSONB metadata filters (e.g., {"finish": "matte", "properties": ["waterproof"]})
            text_weight: Weight for text embeddings (default 0.20)
            visual_weight: Weight for visual embeddings (default 0.20)
            color_weight: Weight for color embeddings (default 0.15)
            texture_weight: Weight for texture embeddings (default 0.15)
            style_weight: Weight for style embeddings (default 0.15)
            material_weight: Weight for material embeddings (default 0.15)
            similarity_threshold: Minimum similarity score (default 0.7)

        Returns:
            Dictionary containing search results with weighted scores
        """
        try:
            from .supabase_client import get_supabase_client
            import time

            start_time = time.time()
            supabase_client = get_supabase_client()

            # Check if embedding service is available
            if not self.embedding_service:
                return {
                    "results": [],
                    "message": "Embedding service not available",
                    "total_results": 0,
                    "processing_time": time.time() - start_time
                }

            # Generate query embedding using the centralized embedding service
            query_embedding = await self.embedding_service.generate_embedding(
                text=query,
                embedding_type="openai"  # Use OpenAI for text embedding
            )

            if not query_embedding:
                return {
                    "results": [],
                    "message": "Failed to generate query embedding",
                    "total_results": 0,
                    "processing_time": time.time() - start_time
                }

            # Convert embedding to PostgreSQL vector format
            embedding_str = f"[{','.join(map(str, query_embedding))}]"

            # Build JSONB metadata filter conditions
            metadata_conditions = []
            if material_filters:
                for key, value in material_filters.items():
                    if isinstance(value, dict) and "contains" in value:
                        # Array containment (e.g., properties contains ["waterproof", "outdoor"])
                        contains_values = value["contains"]
                        if isinstance(contains_values, list):
                            for item in contains_values:
                                metadata_conditions.append(f"p.metadata->'{key}' ? '{item}'")
                    elif isinstance(value, list):
                        # IN clause (e.g., color IN ["beige", "white"])
                        values_str = "', '".join(str(v) for v in value)
                        metadata_conditions.append(f"p.metadata->>'{key}' IN ('{values_str}')")
                    else:
                        # Exact match (e.g., finish = "matte")
                        metadata_conditions.append(f"p.metadata->>'{key}' = '{value}'")

            metadata_filter_sql = ""
            if metadata_conditions:
                metadata_filter_sql = "AND " + " AND ".join(metadata_conditions)

            # üéØ Enhanced multi-vector search query combining all 6 embedding types + metadata filters
            query_sql = f"""
            SELECT
                p.id,
                p.product_name,
                p.description,
                p.metadata,
                p.workspace_id,
                p.document_id,
                (
                    ({text_weight} * (1 - (p.text_embedding_1536 <=> '{embedding_str}'::vector(1536)))) +
                    ({visual_weight} * (1 - (p.visual_clip_embedding_512 <=> '{embedding_str}'::vector(512)))) +
                    ({color_weight} * (1 - (p.color_clip_embedding_512 <=> '{embedding_str}'::vector(512)))) +
                    ({texture_weight} * (1 - (p.texture_clip_embedding_512 <=> '{embedding_str}'::vector(512)))) +
                    ({style_weight} * (1 - (p.style_clip_embedding_512 <=> '{embedding_str}'::vector(512)))) +
                    ({material_weight} * (1 - (p.material_clip_embedding_512 <=> '{embedding_str}'::vector(512))))
                ) as weighted_score
            FROM products p
            WHERE p.workspace_id = '{workspace_id}'
                AND p.text_embedding_1536 IS NOT NULL
                AND p.visual_clip_embedding_512 IS NOT NULL
                AND p.color_clip_embedding_512 IS NOT NULL
                AND p.texture_clip_embedding_512 IS NOT NULL
                AND p.style_clip_embedding_512 IS NOT NULL
                AND p.material_clip_embedding_512 IS NOT NULL
                {metadata_filter_sql}
                AND (
                    ({text_weight} * (1 - (p.text_embedding_1536 <=> '{embedding_str}'::vector(1536)))) +
                    ({visual_weight} * (1 - (p.visual_clip_embedding_512 <=> '{embedding_str}'::vector(512)))) +
                    ({color_weight} * (1 - (p.color_clip_embedding_512 <=> '{embedding_str}'::vector(512)))) +
                    ({texture_weight} * (1 - (p.texture_clip_embedding_512 <=> '{embedding_str}'::vector(512)))) +
                    ({style_weight} * (1 - (p.style_clip_embedding_512 <=> '{embedding_str}'::vector(512)))) +
                    ({material_weight} * (1 - (p.material_clip_embedding_512 <=> '{embedding_str}'::vector(512))))
                ) >= {similarity_threshold}
            ORDER BY weighted_score DESC
            LIMIT {top_k}
            """

            # Execute query
            response = supabase_client.client.rpc('exec_sql', {'query': query_sql}).execute()

            results = []
            if response.data:
                for row in response.data:
                    results.append({
                        "id": row.get("id"),
                        "product_name": row.get("product_name"),
                        "description": row.get("description"),
                        "metadata": row.get("metadata", {}),
                        "workspace_id": row.get("workspace_id"),
                        "document_id": row.get("document_id"),
                        "score": row.get("weighted_score", 0.0),
                        "search_type": "multi_vector"
                    })

            # NEW: Apply metadata prototype validation scoring (enabled by default)
            from app.services.metadata_prototype_validator import get_metadata_validator

            if material_filters and results:
                try:
                    validator = get_metadata_validator()
                    await validator.load_prototypes()

                    # Enhance results with metadata validation scoring
                    for result in results:
                        metadata_boost = await self._calculate_metadata_validation_boost(
                            product_metadata=result.get("metadata", {}),
                            query_filters=material_filters,
                            validator=validator
                        )

                        # Apply boost to score (up to 20% increase)
                        original_score = result["score"]
                        result["score"] = original_score * (1.0 + metadata_boost * 0.2)
                        result["metadata_validation_boost"] = metadata_boost
                        result["original_score"] = original_score

                    # Re-sort by enhanced score
                    results.sort(key=lambda x: x["score"], reverse=True)

                except Exception as e:
                    self.logger.warning(f"Metadata validation scoring failed: {e}")

            # NEW: Track search query for prototype discovery
            from app.services.search_query_tracker import get_search_tracker

            try:
                tracker = get_search_tracker()
                # Track asynchronously (don't wait)
                asyncio.create_task(tracker.track_query(
                    workspace_id=workspace_id,
                    query_text=query,
                    query_metadata=material_filters,
                    search_type="multi_vector",
                    result_count=len(results),
                    response_time_ms=int((time.time() - start_time) * 1000)
                ))
            except Exception as e:
                self.logger.warning(f"Search tracking failed: {e}")

            return {
                "results": results,
                "total_results": len(results),
                "processing_time": time.time() - start_time,
                "weights": {
                    "text": text_weight,
                    "visual": visual_weight,
                    "color": color_weight,
                    "texture": texture_weight,
                    "style": style_weight,
                    "material": material_weight
                },
                "material_filters_applied": material_filters if material_filters else None,
                "metadata_validation_enabled": True,  # NEW: Always enabled
                "query": query,
                "search_type": "multi_vector_enhanced"
            }

        except Exception as e:
            self.logger.error(f"Multi-vector search failed: {e}", exc_info=True)
            return {
                "results": [],
                "error": str(e),
                "total_results": 0,
                "processing_time": time.time() - start_time if 'start_time' in locals() else 0
            }

    async def _calculate_metadata_validation_boost(
        self,
        product_metadata: Dict[str, Any],
        query_filters: Dict[str, Any],
        validator: Any
    ) -> float:
        """Calculate metadata validation boost score.

        Compares query filters to product metadata using prototype validation.
        Returns boost score between 0.0 and 1.0.

        Args:
            product_metadata: Product's metadata JSONB
            query_filters: User's search filters
            validator: MetadataPrototypeValidator instance

        Returns:
            Boost score (0.0 = no boost, 1.0 = perfect match)
        """
        if not query_filters or not product_metadata:
            return 0.0

        total_score = 0.0
        matched_fields = 0
        total_fields = len(query_filters)

        for filter_key, filter_value in query_filters.items():
            product_value = product_metadata.get(filter_key)

            if not product_value:
                continue

            # Check if this field has prototype validation
            if filter_key in validator._prototype_cache:
                # Check validation metadata
                validation_info = product_metadata.get('_validation', {}).get(filter_key, {})

                if validation_info.get('prototype_matched'):
                    # Both values are validated - check if they match
                    if str(product_value).lower() == str(filter_value).lower():
                        # Exact match ‚Üí full score
                        total_score += 1.0
                        matched_fields += 1
                    else:
                        # Different prototypes ‚Üí check semantic similarity
                        try:
                            import numpy as np

                            # Generate embeddings for both values
                            filter_embedding = await validator.embeddings_service._generate_text_embedding(
                                text=str(filter_value),
                                job_id=None,
                                dimensions=512
                            )
                            product_embedding = await validator.embeddings_service._generate_text_embedding(
                                text=str(product_value),
                                job_id=None,
                                dimensions=512
                            )

                            if filter_embedding and product_embedding:
                                similarity = validator._cosine_similarity(
                                    np.array(filter_embedding),
                                    np.array(product_embedding)
                                )

                                if similarity > 0.70:
                                    # Semantically similar ‚Üí partial score
                                    total_score += similarity
                                    matched_fields += 1
                        except Exception as e:
                            self.logger.warning(f"Similarity calculation failed: {e}")
                else:
                    # Product value not validated ‚Üí fuzzy match
                    if str(product_value).lower() == str(filter_value).lower():
                        total_score += 0.8  # Penalty for unvalidated
                        matched_fields += 1
            else:
                # No prototype validation ‚Üí exact match only
                if str(product_value).lower() == str(filter_value).lower():
                    total_score += 1.0
                    matched_fields += 1

        # Normalize score
        if total_fields > 0:
            return total_score / total_fields
        return 0.0

    async def hybrid_search(
        self,
        query: str,
        workspace_id: str,
        top_k: int = 10,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3,
        similarity_threshold: float = 0.6
    ) -> Dict[str, Any]:
        """
        Hybrid search combining semantic search (70%) with PostgreSQL full-text search (30%).

        Combines:
        - Semantic vector similarity using text_embedding_1536
        - PostgreSQL full-text search using search_vector tsvector column

        Args:
            query: Search query text
            workspace_id: Workspace ID to filter results
            top_k: Number of results to return
            semantic_weight: Weight for semantic similarity (default 0.7)
            keyword_weight: Weight for keyword matching (default 0.3)
            similarity_threshold: Minimum similarity score (default 0.6)

        Returns:
            Dictionary containing hybrid search results
        """
        try:
            from .supabase_client import get_supabase_client
            import time

            start_time = time.time()
            supabase_client = get_supabase_client()

            # Check if embedding service is available
            if not self.embedding_service:
                return {
                    "results": [],
                    "message": "Embedding service not available",
                    "total_results": 0,
                    "processing_time": time.time() - start_time
                }

            # Generate query embedding for semantic search
            query_embedding = await self.embedding_service.generate_embedding(
                text=query,
                embedding_type="openai"
            )

            if not query_embedding:
                return {
                    "results": [],
                    "message": "Failed to generate query embedding",
                    "total_results": 0,
                    "processing_time": time.time() - start_time
                }

            # Convert embedding to PostgreSQL vector format
            embedding_str = f"[{','.join(map(str, query_embedding))}]"

            # Hybrid search query combining vector similarity and full-text search
            query_sql = f"""
            SELECT
                p.id,
                p.product_name,
                p.description,
                p.metadata,
                p.workspace_id,
                p.document_id,
                (
                    ({semantic_weight} * (1 - (p.text_embedding_1536 <=> '{embedding_str}'::vector(1536)))) +
                    ({keyword_weight} * ts_rank(p.search_vector, plainto_tsquery('english', '{query}')))
                ) as hybrid_score,
                (1 - (p.text_embedding_1536 <=> '{embedding_str}'::vector(1536))) as semantic_score,
                ts_rank(p.search_vector, plainto_tsquery('english', '{query}')) as keyword_score
            FROM products p
            WHERE p.workspace_id = '{workspace_id}'
                AND p.text_embedding_1536 IS NOT NULL
                AND p.search_vector IS NOT NULL
                AND (
                    (1 - (p.text_embedding_1536 <=> '{embedding_str}'::vector(1536))) >= {similarity_threshold}
                    OR p.search_vector @@ plainto_tsquery('english', '{query}')
                )
            ORDER BY hybrid_score DESC
            LIMIT {top_k}
            """

            # Execute query
            response = supabase_client.client.rpc('exec_sql', {'query': query_sql}).execute()

            results = []
            if response.data:
                for row in response.data:
                    results.append({
                        "id": row.get("id"),
                        "product_name": row.get("product_name"),
                        "description": row.get("description"),
                        "metadata": row.get("metadata", {}),
                        "workspace_id": row.get("workspace_id"),
                        "document_id": row.get("document_id"),
                        "score": row.get("hybrid_score", 0.0),
                        "semantic_score": row.get("semantic_score", 0.0),
                        "keyword_score": row.get("keyword_score", 0.0),
                        "search_type": "hybrid"
                    })

            return {
                "results": results,
                "total_results": len(results),
                "processing_time": time.time() - start_time,
                "weights": {
                    "semantic": semantic_weight,
                    "keyword": keyword_weight
                },
                "query": query
            }

        except Exception as e:
            self.logger.error(f"Hybrid search failed: {e}", exc_info=True)
            return {
                "results": [],
                "error": str(e),
                "total_results": 0,
                "processing_time": time.time() - start_time if 'start_time' in locals() else 0
            }

    async def material_property_search(
        self,
        workspace_id: str,
        material_filters: Dict[str, Any],
        top_k: int = 10,
        match_mode: str = "AND"
    ) -> Dict[str, Any]:
        """
        JSONB-based property filtering with AND/OR logic support.

        Searches products based on material properties stored in metadata JSONB column.
        Supports complex queries like:
        - material_type = "fabric"
        - dimensions.width >= 100
        - color IN ["red", "blue"]

        Args:
            workspace_id: Workspace ID to filter results
            material_filters: Dictionary of property filters
                Example: {
                    "material_type": "fabric",
                    "dimensions.width": {"gte": 100},
                    "color": ["red", "blue"]
                }
            top_k: Number of results to return
            match_mode: "AND" or "OR" for combining filters

        Returns:
            Dictionary containing filtered products
        """
        try:
            from .supabase_client import get_supabase_client
            import time

            start_time = time.time()
            supabase_client = get_supabase_client()

            if not material_filters:
                return {
                    "results": [],
                    "message": "No filters provided",
                    "total_results": 0,
                    "processing_time": time.time() - start_time
                }

            # Build WHERE clauses for JSONB filtering
            where_clauses = []
            for key, value in material_filters.items():
                if isinstance(value, dict):
                    # Handle comparison operators (gte, lte, gt, lt, eq)
                    for op, val in value.items():
                        if op == "gte":
                            where_clauses.append(f"(p.metadata->>'{key}')::numeric >= {val}")
                        elif op == "lte":
                            where_clauses.append(f"(p.metadata->>'{key}')::numeric <= {val}")
                        elif op == "gt":
                            where_clauses.append(f"(p.metadata->>'{key}')::numeric > {val}")
                        elif op == "lt":
                            where_clauses.append(f"(p.metadata->>'{key}')::numeric < {val}")
                        elif op == "eq":
                            where_clauses.append(f"p.metadata->>'{key}' = '{val}'")
                elif isinstance(value, list):
                    # Handle IN operator
                    values_str = "', '".join(str(v) for v in value)
                    where_clauses.append(f"p.metadata->>'{key}' IN ('{values_str}')")
                else:
                    # Handle exact match
                    where_clauses.append(f"p.metadata->>'{key}' = '{value}'")

            # Combine clauses with AND/OR
            connector = " AND " if match_mode == "AND" else " OR "
            where_condition = connector.join(where_clauses)

            # Material property search query
            query_sql = f"""
            SELECT
                p.id,
                p.product_name,
                p.description,
                p.metadata,
                p.workspace_id,
                p.document_id
            FROM products p
            WHERE p.workspace_id = '{workspace_id}'
                AND ({where_condition})
            LIMIT {top_k}
            """

            # Execute query
            response = supabase_client.client.rpc('exec_sql', {'query': query_sql}).execute()

            results = []
            if response.data:
                for row in response.data:
                    results.append({
                        "id": row.get("id"),
                        "product_name": row.get("product_name"),
                        "description": row.get("description"),
                        "metadata": row.get("metadata", {}),
                        "workspace_id": row.get("workspace_id"),
                        "document_id": row.get("document_id"),
                        "search_type": "material_property"
                    })

            return {
                "results": results,
                "total_results": len(results),
                "processing_time": time.time() - start_time,
                "filters": material_filters,
                "match_mode": match_mode
            }

        except Exception as e:
            self.logger.error(f"Material property search failed: {e}", exc_info=True)
            return {
                "results": [],
                "error": str(e),
                "total_results": 0,
                "processing_time": time.time() - start_time if 'start_time' in locals() else 0
            }

    async def image_similarity_search(
        self,
        workspace_id: str,
        image_url: Optional[str] = None,
        image_base64: Optional[str] = None,
        top_k: int = 10,
        similarity_threshold: float = 0.7,
        document_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ‚úÖ UPDATED: Visual similarity search using VECS with relationship enrichment.

        Searches images using VECS CLIP embeddings with HNSW indexing.
        Returns enriched results with related products and chunks.

        Args:
            workspace_id: Workspace ID to filter results
            image_url: URL of the query image (optional)
            image_base64: Base64-encoded query image (optional)
            top_k: Number of results to return
            similarity_threshold: Minimum similarity score (default 0.7)
            document_id: Optional document ID to filter results

        Returns:
            Dictionary containing visually similar images with products and chunks
        """
        try:
            from .vecs_service import get_vecs_service
            from .search_enrichment_service import SearchEnrichmentService
            import time
            import base64
            import requests
            from io import BytesIO
            from PIL import Image

            start_time = time.time()

            # Validate input
            if not image_url and not image_base64:
                return {
                    "results": [],
                    "message": "Either image_url or image_base64 must be provided",
                    "total_results": 0,
                    "processing_time": time.time() - start_time
                }

            # Load image
            if image_url:
                response = requests.get(image_url)
                image = Image.open(BytesIO(response.content))
            else:
                image_data = base64.b64decode(image_base64)
                image = Image.open(BytesIO(image_data))

            # Generate CLIP embedding for the query image
            query_embedding = await self._generate_clip_embedding(image)

            if not query_embedding:
                return {
                    "results": [],
                    "message": "Failed to generate image embedding",
                    "total_results": 0,
                    "processing_time": time.time() - start_time
                }

            # ‚úÖ NEW: Use VECS for similarity search with HNSW indexing
            vecs_service = get_vecs_service()

            # Build metadata filters
            filters = {"workspace_id": {"$eq": workspace_id}}  # ‚úÖ ADD: Always filter by workspace
            if document_id:
                filters["document_id"] = {"$eq": document_id}  # ‚úÖ FIX: Add to existing filters dict

            # Search VECS collection
            vecs_results = await vecs_service.search_similar_images(
                query_embedding=query_embedding,
                limit=top_k * 2,  # Get more results to filter by threshold
                filters=filters,
                include_metadata=True
            )

            # Filter by similarity threshold
            filtered_results = [
                r for r in vecs_results
                if r.get('similarity_score', 0) >= similarity_threshold
            ][:top_k]

            # ‚úÖ NEW: Enrich results with relationship data + re-ranking
            enrichment_service = SearchEnrichmentService()
            enriched_results = await enrichment_service.enrich_image_results(
                image_results=filtered_results,
                include_products=True,
                include_chunks=True,
                min_relevance=0.5,  # Only include relationships with relevance >= 0.5
                rerank=True,  # ‚úÖ NEW: Enable re-ranking by combined score
                visual_weight=0.6,  # ‚úÖ NEW: 60% weight for visual similarity
                relevance_weight=0.4  # ‚úÖ NEW: 40% weight for product relevance
            )

            # Format results
            results = []
            for item in enriched_results:
                results.append({
                    "image_id": item.get("image_id"),
                    "similarity_score": item.get("similarity_score"),
                    "combined_score": item.get("combined_score"),  # ‚úÖ NEW: Include combined score
                    "max_product_relevance": item.get("max_product_relevance"),  # ‚úÖ NEW: Include max relevance
                    "metadata": item.get("metadata", {}),
                    "related_products": item.get("related_products", []),
                    "related_chunks": item.get("related_chunks", []),
                    "search_type": "vecs_image_similarity_reranked"  # ‚úÖ NEW: Updated search type
                })

            self.logger.info(f"‚úÖ VECS image search: {len(results)} results in {time.time() - start_time:.2f}s")

            return {
                "results": results,
                "total_results": len(results),
                "processing_time": time.time() - start_time,
                "query_type": "image_url" if image_url else "image_base64",
                "search_method": "vecs_hnsw"
            }

        except Exception as e:
            self.logger.error(f"Image similarity search failed: {e}", exc_info=True)
            return {
                "results": [],
                "error": str(e),
                "total_results": 0,
                "processing_time": time.time() - start_time if 'start_time' in locals() else 0
            }

    async def _generate_clip_embedding(self, image: 'Image') -> Optional[List[float]]:
        """
        Generate CLIP embedding for an image.

        Args:
            image: PIL Image object

        Returns:
            List of floats representing the CLIP embedding (512 dimensions)
        """
        try:
            # TODO: Integrate with actual CLIP model
            # For now, return a placeholder embedding
            # In production, this should call the CLIP embedding service
            self.logger.warning("CLIP embedding generation not yet implemented - using placeholder")
            return [0.0] * 512  # Placeholder 512-dimensional embedding

        except Exception as e:
            self.logger.error(f"Failed to generate CLIP embedding: {e}")
            return None

    async def search_all_strategies(
        self,
        query: str,
        workspace_id: str,
        top_k: int = 10,
        similarity_threshold: float = 0.7,
        material_filters: Optional[Dict[str, Any]] = None,
        image_url: Optional[str] = None,
        image_base64: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Run all 6 search strategies in parallel using asyncio.gather() for 3-4x performance improvement.

        Sequential execution: ~800ms (150+100+200+150+50+150)
        Parallel execution: ~200-300ms (limited by slowest query)

        Strategies executed in parallel:
        1. Semantic Search (MMR with diversity)
        2. Vector Search (pure similarity)
        3. Multi-Vector Search (3 embeddings combined)
        4. Hybrid Search (semantic + full-text)
        5. Material Property Search (JSONB filtering) - if material_filters provided
        6. Image Similarity Search (CLIP embeddings) - if image_url/image_base64 provided

        Args:
            query: Search query text
            workspace_id: Workspace ID to filter results
            top_k: Number of results per strategy
            similarity_threshold: Minimum similarity score
            material_filters: Optional material property filters
            image_url: Optional image URL for image search
            image_base64: Optional base64 image for image search

        Returns:
            Dictionary containing merged results from all strategies with metadata
        """
        try:
            import time
            import asyncio

            start_time = time.time()

            # Build list of strategy coroutines to execute in parallel
            strategy_tasks = []
            strategy_names = []

            # Always run these 4 core strategies
            strategy_tasks.extend([
                self.semantic_search_with_mmr(query=query, k=top_k, lambda_mult=0.5),
                self.semantic_search_with_mmr(query=query, k=top_k, lambda_mult=1.0),
                self.multi_vector_search(query=query, workspace_id=workspace_id, top_k=top_k, similarity_threshold=similarity_threshold),
                self.hybrid_search(query=query, workspace_id=workspace_id, top_k=top_k, similarity_threshold=similarity_threshold)
            ])
            strategy_names.extend(['semantic', 'vector', 'multi_vector', 'hybrid'])

            # Conditionally add material search if filters provided
            if material_filters:
                strategy_tasks.append(
                    self.material_property_search(workspace_id=workspace_id, material_filters=material_filters, top_k=top_k)
                )
                strategy_names.append('material')

            # Conditionally add image search if image provided
            if image_url or image_base64:
                strategy_tasks.append(
                    self.image_similarity_search(workspace_id=workspace_id, image_url=image_url, image_base64=image_base64, top_k=top_k, similarity_threshold=similarity_threshold)
                )
                strategy_names.append('image')

            # Execute all strategies in parallel with error handling
            self.logger.info(f"üöÄ Executing {len(strategy_tasks)} search strategies in parallel...")
            results = await asyncio.gather(*strategy_tasks, return_exceptions=True)

            # Process results and handle errors
            strategy_results = {}
            successful_strategies = 0
            failed_strategies = 0

            for i, (result, name) in enumerate(zip(results, strategy_names)):
                if isinstance(result, Exception):
                    self.logger.error(f"‚ùå Strategy '{name}' failed: {result}")
                    strategy_results[name] = {
                        "results": [],
                        "error": str(result),
                        "total_results": 0
                    }
                    failed_strategies += 1
                else:
                    strategy_results[name] = result
                    successful_strategies += 1
                    self.logger.info(f"‚úÖ Strategy '{name}' returned {result.get('total_results', 0)} results")

            # Merge and deduplicate results from all successful strategies
            merged_results = await self._merge_strategy_results(strategy_results, top_k)

            processing_time = time.time() - start_time

            return {
                "results": merged_results,
                "total_results": len(merged_results),
                "processing_time": processing_time,
                "strategies_executed": len(strategy_tasks),
                "strategies_successful": successful_strategies,
                "strategies_failed": failed_strategies,
                "strategy_breakdown": {
                    name: {
                        "count": len(strategy_results[name].get("results", [])),
                        "success": not isinstance(results[i], Exception)
                    }
                    for i, name in enumerate(strategy_names)
                },
                "query": query,
                "search_type": "all_strategies_parallel"
            }

        except Exception as e:
            self.logger.error(f"All-strategies search failed: {e}", exc_info=True)
            return {
                "results": [],
                "error": str(e),
                "total_results": 0,
                "processing_time": time.time() - start_time if 'start_time' in locals() else 0
            }

    async def _merge_strategy_results(
        self,
        strategy_results: Dict[str, Dict[str, Any]],
        top_k: int
    ) -> List[Dict[str, Any]]:
        """
        Merge and deduplicate results from multiple search strategies with weighted scoring.

        Strategy weights (based on reliability and accuracy):
        - semantic: 1.0 (highest - natural language understanding)
        - multi_vector: 0.9 (high - comprehensive multimodal)
        - hybrid: 0.85 (high - balanced approach)
        - vector: 0.8 (good - pure similarity)
        - material: 0.7 (medium - property-based)
        - image: 0.75 (medium-high - visual similarity)

        Args:
            strategy_results: Dictionary of results from each strategy
            top_k: Maximum number of results to return

        Returns:
            List of merged and deduplicated results sorted by weighted score
        """
        try:
            # Strategy weights for scoring
            strategy_weights = {
                'semantic': 1.0,
                'multi_vector': 0.9,
                'hybrid': 0.85,
                'vector': 0.8,
                'image': 0.75,
                'material': 0.7
            }

            # Collect all results with weighted scores
            result_map = {}  # Key: product_id or content hash, Value: result with metadata

            for strategy_name, strategy_data in strategy_results.items():
                if 'error' in strategy_data or not strategy_data.get('results'):
                    continue

                weight = strategy_weights.get(strategy_name, 0.5)

                for result in strategy_data.get('results', []):
                    # Generate unique key for deduplication
                    # Use product_id if available, otherwise use content hash
                    result_id = result.get('id') or result.get('product_id') or result.get('chunk_id')
                    if not result_id:
                        # Fallback to content hash for deduplication
                        content = result.get('content', '') or result.get('description', '')
                        result_id = hash(content[:200]) if content else hash(str(result))

                    # Get original score from result
                    original_score = result.get('score', 0.0)
                    if isinstance(original_score, (int, float)):
                        weighted_score = original_score * weight
                    else:
                        weighted_score = 0.5 * weight  # Default score

                    # If result already exists, combine scores
                    if result_id in result_map:
                        existing = result_map[result_id]
                        # Average the weighted scores
                        existing['weighted_score'] = (existing['weighted_score'] + weighted_score) / 2
                        # Track which strategies found this result
                        existing['found_by_strategies'].append(strategy_name)
                        existing['strategy_scores'][strategy_name] = original_score
                    else:
                        # New result
                        result_map[result_id] = {
                            **result,
                            'weighted_score': weighted_score,
                            'found_by_strategies': [strategy_name],
                            'strategy_scores': {strategy_name: original_score}
                        }

            # Sort by weighted score and return top_k
            merged_results = sorted(
                result_map.values(),
                key=lambda x: x['weighted_score'],
                reverse=True
            )[:top_k]

            self.logger.info(f"üìä Merged {len(result_map)} unique results from {len(strategy_results)} strategies, returning top {len(merged_results)}")

            return merged_results

        except Exception as e:
            self.logger.error(f"Failed to merge strategy results: {e}", exc_info=True)
            # Fallback: return results from first successful strategy
            for strategy_data in strategy_results.values():
                if 'results' in strategy_data and strategy_data['results']:
                    return strategy_data['results'][:top_k]
            return []
