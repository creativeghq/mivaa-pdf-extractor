name: K8s Deploy - MIVAA PDF Extractor

on:
  push:
    branches:
      - main
      - k8s-deployment
  workflow_dispatch:  # Allow manual triggers

permissions:
  contents: read
  packages: write
  id-token: write

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  K8S_CLUSTER_ID: e56b1987-f9d0-4e4d-8e50-b27e12592f19

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to GitHub Container Registry
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata (tags, labels)
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./k8s/Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache
        cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache,mode=max
        build-args: |
          BUILDKIT_INLINE_CACHE=1
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}

  deploy-to-k8s:
    name: Deploy to Kubernetes
    runs-on: ubuntu-latest
    needs: build-and-push
    environment: production
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install doctl
      uses: digitalocean/action-doctl@v2
      with:
        token: ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }}
    
    - name: Save kubeconfig
      run: doctl kubernetes cluster kubeconfig save ${{ env.K8S_CLUSTER_ID }}
    
    - name: Verify cluster connection
      run: kubectl cluster-info
    
    - name: Create namespace if not exists
      run: kubectl create namespace default --dry-run=client -o yaml | kubectl apply -f -
    
    - name: Create Docker registry secret
      run: |
        kubectl create secret docker-registry ghcr-secret \
          --docker-server=ghcr.io \
          --docker-username=${{ github.actor }} \
          --docker-password=${{ secrets.GITHUB_TOKEN }} \
          --namespace=default \
          --dry-run=client -o yaml | kubectl apply -f -
    
    - name: Create ConfigMap
      run: kubectl apply -f k8s/configmap.yaml

    - name: Create Secrets
      run: |
        kubectl create secret generic mivaa-secrets \
          --from-literal=SUPABASE_ANON_KEY="${{ secrets.SUPABASE_ANON_KEY }}" \
          --from-literal=SUPABASE_SERVICE_ROLE_KEY="${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}" \
          --from-literal=SUPABASE_DB_PASSWORD="${{ secrets.SUPABASE_DB_PASSWORD }}" \
          --from-literal=SUPABASE_PROJECT_ID="${{ secrets.SUPABASE_PROJECT_ID }}" \
          --from-literal=ANTHROPIC_API_KEY="${{ secrets.ANTHROPIC_API_KEY }}" \
          --from-literal=OPENAI_API_KEY="${{ secrets.OPENAI_API_KEY }}" \
          --from-literal=TOGETHER_API_KEY="${{ secrets.TOGETHER_API_KEY }}" \
          --from-literal=HUGGINGFACE_API_KEY="${{ secrets.HUGGINGFACE_API_KEY }}" \
          --from-literal=HUGGING_FACE_ACCESS_TOKEN="${{ secrets.HUGGING_FACE_ACCESS_TOKEN }}" \
          --from-literal=REPLICATE_API_TOKEN="${{ secrets.REPLICATE_API_TOKEN }}" \
          --from-literal=JINA_API_KEY="${{ secrets.JINA_API_KEY }}" \
          --from-literal=FIRECRAWL_API_KEY="${{ secrets.FIRECRAWL_API_KEY }}" \
          --from-literal=JWT_SECRET_KEY="${{ secrets.JWT_SECRET_KEY }}" \
          --from-literal=ENCRYPTION_KEY="${{ secrets.ENCRYPTION_KEY }}" \
          --from-literal=MATERIAL_KAI_API_KEY="${{ secrets.MATERIAL_KAI_API_KEY }}" \
          --from-literal=MATERIAL_KAI_API_URL="${{ secrets.MATERIAL_KAI_API_URL }}" \
          --from-literal=MATERIAL_KAI_WORKSPACE_ID="${{ secrets.MATERIAL_KAI_WORKSPACE_ID }}" \
          --from-literal=MATERIAL_KAI_CLIENT_ID="${{ secrets.MATERIAL_KAI_CLIENT_ID }}" \
          --from-literal=SENTRY_DSN="https://73f48f6581b882c707ded429e384fb8a@o4509716458045440.ingest.de.sentry.io/4510132019658832" \
          --namespace=default \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Create KEDA Supabase Secret
      run: |
        # Build Supabase DB connection string for KEDA
        SUPABASE_DB_CONNECTION_STRING="postgresql://postgres.bgbavxtjlbvgplozizxu:${{ secrets.SUPABASE_DB_PASSWORD }}@aws-0-eu-west-3.pooler.supabase.com:6543/postgres"

        kubectl create secret generic keda-supabase-secret \
          --from-literal=SUPABASE_DB_CONNECTION_STRING="$SUPABASE_DB_CONNECTION_STRING" \
          --namespace=default \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Install KEDA v2.18.0 (if not already installed)
      run: |
        echo "üîç Checking if KEDA is already installed..."

        # Check if KEDA namespace exists and has deployments
        if kubectl get namespace keda 2>/dev/null && \
           kubectl get deployment keda-operator -n keda 2>/dev/null && \
           kubectl get deployment keda-operator-metrics-apiserver -n keda 2>/dev/null; then
          echo "‚úÖ KEDA is already installed - skipping installation"
          kubectl get deployments -n keda
        else
          echo "üì¶ Installing KEDA v2.18.0..."
          # Using --server-side flag for better CRD and webhook management
          kubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.18.0/keda-2.18.0.yaml

          echo "‚è≥ Waiting for KEDA to be ready..."
          sleep 10

          # Wait for KEDA operator to be available
          kubectl wait --for=condition=available deployment/keda-operator -n keda --timeout=300s || echo "‚ö†Ô∏è  keda-operator not ready yet"
          kubectl wait --for=condition=available deployment/keda-operator-metrics-apiserver -n keda --timeout=300s || echo "‚ö†Ô∏è  keda-operator-metrics-apiserver not ready yet"

          echo "‚úÖ KEDA v2.18.0 installation complete"
          kubectl get pods -n keda
        fi

    - name: Cleanup old LoadBalancer (if exists)
      run: |
        # Check if service exists and is LoadBalancer type
        if kubectl get svc mivaa-pdf-extractor -n default &> /dev/null; then
          SERVICE_TYPE=$(kubectl get svc mivaa-pdf-extractor -n default -o jsonpath='{.spec.type}')
          if [ "$SERVICE_TYPE" = "LoadBalancer" ]; then
            echo "‚ö†Ô∏è  Found old LoadBalancer service - deleting to prevent duplicate LBs..."
            LB_IP=$(kubectl get svc mivaa-pdf-extractor -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}' || echo "N/A")
            echo "   Old LoadBalancer IP: $LB_IP"
            kubectl delete svc mivaa-pdf-extractor -n default
            echo "‚úÖ Old LoadBalancer deleted - DigitalOcean LB will be removed automatically"
            sleep 10  # Wait for deletion to propagate
          else
            echo "‚úÖ Service is already ClusterIP - no cleanup needed"
          fi
        else
          echo "‚úÖ No existing service - fresh deployment"
        fi

    - name: Delete old LoadBalancer service (if exists)
      run: |
        echo "Checking for old LoadBalancer service..."
        if kubectl get svc mivaa-pdf-extractor -n default 2>/dev/null | grep -q LoadBalancer; then
          echo "‚ö†Ô∏è  Found old LoadBalancer service - deleting..."
          kubectl delete svc mivaa-pdf-extractor -n default
          echo "‚úÖ Old LoadBalancer deleted"
        else
          echo "‚úÖ No old LoadBalancer found (or already ClusterIP)"
        fi

    - name: Deploy Service (ClusterIP)
      run: kubectl apply -f k8s/service.yaml

    - name: Deploy Deployment
      run: kubectl apply -f k8s/deployment.yaml

    - name: Create KEDA Supabase Secret
      run: |
        kubectl create secret generic keda-supabase-secret \
          --from-literal=SUPABASE_DB_CONNECTION_STRING="${{ secrets.SUPABASE_DB_CONNECTION_STRING }}" \
          --dry-run=client -o yaml | kubectl apply -f -

    # NOTE: KEDA and HPA cannot both manage the same deployment
    # We use KEDA for scale-to-zero capability (minReplicaCount: 0)
    # HPA cannot scale to zero (minimum is 1 replica)

    - name: Delete old HPA (if exists) to avoid conflict with KEDA
      run: |
        if kubectl get hpa mivaa-pdf-extractor-hpa -n default 2>/dev/null; then
          echo "‚ö†Ô∏è  Found old HPA - deleting to avoid conflict with KEDA..."
          kubectl delete hpa mivaa-pdf-extractor-hpa -n default
          echo "‚úÖ Old HPA deleted"
        else
          echo "‚úÖ No old HPA found"
        fi

    - name: Deploy KEDA ScaledObject (scale-to-zero based on job queue)
      run: kubectl apply -f k8s/keda-scaledobject.yaml

    - name: Deploy PodDisruptionBudget
      run: kubectl apply -f k8s/pdb.yaml

    - name: Deploy Ingress
      run: kubectl apply -f k8s/ingress.yaml

    - name: Force pod restart to pull new image
      run: |
        echo "üîÑ Forcing pod restart to pull latest image..."
        kubectl rollout restart deployment/mivaa-pdf-extractor -n default
        echo "‚úÖ Rollout restart initiated"

    - name: Wait for rollout
      run: kubectl rollout status deployment/mivaa-pdf-extractor -n default --timeout=5m

    - name: Get deployment status
      run: |
        echo "=== Deployment Status ==="
        kubectl get deployments -n default
        echo ""
        echo "=== Pods ==="
        kubectl get pods -n default -l app=mivaa-pdf-extractor
        echo ""
        echo "=== Pod Image ==="
        kubectl get pods -n default -l app=mivaa-pdf-extractor -o jsonpath='{.items[0].spec.containers[0].image}'
        echo ""
        echo ""
        echo "=== Services ==="
        kubectl get services -n default
        echo ""
        echo "=== Ingress ==="
        kubectl get ingress -n default

    - name: Scale node pool to 1 node and enable autoscaling (1-4)
      run: |
        echo "üîß Configuring node pool autoscaling..."

        # Get cluster ID
        CLUSTER_ID=$(doctl kubernetes cluster list --format ID --no-header)
        echo "Cluster ID: $CLUSTER_ID"

        # Get node pool ID
        NODE_POOL_ID=$(doctl kubernetes cluster node-pool list $CLUSTER_ID --format ID --no-header | head -n 1)
        echo "Node Pool ID: $NODE_POOL_ID"

        # Update node pool: scale to 1 node, enable autoscaling 1-4
        echo "Updating node pool to 1 node with autoscaling 1-4..."
        doctl kubernetes cluster node-pool update $CLUSTER_ID $NODE_POOL_ID \
          --count 1 \
          --auto-scale \
          --min-nodes 1 \
          --max-nodes 4

        echo "‚úÖ Node pool configured:"
        echo "  - Current nodes: 1"
        echo "  - Autoscaling: enabled"
        echo "  - Min nodes: 1"
        echo "  - Max nodes: 4"

        # Wait for nodes to scale down
        echo "‚è≥ Waiting for nodes to scale down..."
        sleep 30

        echo "üìä Current nodes:"
        kubectl get nodes

    - name: Run health check
      run: |
        echo "Waiting for service to be ready..."
        sleep 30
        # Service is now ClusterIP (not LoadBalancer), accessed via Ingress
        echo "Service is accessible via Ingress at: https://v1api.materialshub.gr"
        echo "Health check: curl https://v1api.materialshub.gr/health"

    - name: Deployment summary
      if: always()
      run: |
        echo "üéâ Deployment completed!"
        echo "‚úÖ Docker image built and pushed"
        echo "‚úÖ Kubernetes resources deployed"
        echo "‚úÖ KEDA v2.18.0 installed for scale-to-zero"
        echo "‚úÖ Service exposed via Ingress (ClusterIP, not LoadBalancer)"
        echo "‚úÖ Node pool scaled to 1 node with autoscaling enabled"
        echo ""
        echo "üìä Pod Autoscaling (KEDA):"
        echo "  - Min replicas: 0 (scales to zero when idle)"
        echo "  - Max replicas: 8 (scales up based on job queue)"
        echo "  - Trigger: background_jobs table (pending/running jobs)"
        echo "  - Cooldown: 5 minutes before scaling down"
        echo ""
        echo "üìä Node Autoscaling (Cluster Autoscaler):"
        echo "  - Min nodes: 1 (cost-optimized idle state)"
        echo "  - Max nodes: 4 (scales up when pods can't be scheduled)"
        echo "  - Current nodes: 1"
        echo ""
        echo "üí∞ Cost Optimization:"
        echo "  - Idle: ~\$24/month (1 node, 0 pods)"
        echo "  - Peak: ~\$96/month (4 nodes, 8 pods)"
        echo "  - Only 1 LoadBalancer (Ingress): ~\$10/month"
        echo ""
        echo "üìä Next steps:"
        echo "1. Monitor pods: kubectl get pods -n default -w"
        echo "2. Check KEDA scaling: kubectl get scaledobject -n default"
        echo "3. Check nodes: kubectl get nodes"
        echo "4. View logs: kubectl logs -f deployment/mivaa-pdf-extractor -n default"
        echo "5. Test endpoint: curl https://v1api.materialshub.gr/health"

