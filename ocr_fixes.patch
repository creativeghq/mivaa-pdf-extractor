From 239d08835fe2ea77fa29a6bdb019471508f0e310 Mon Sep 17 00:00:00 2001
From: Basilis Kanonidis <basiliskan@gmail.com>
Date: Wed, 29 Oct 2025 10:53:32 +0000
Subject: [PATCH 1/2] fix: Restore OCR optimization code (multi-phase
 filtering)

- Phase 1: OpenCV fast text detection (~0.1s per image)
- Phase 2: CLIP AI classification (~0.5s per image)
- Phase 3: Full EasyOCR processing (~30s per image)
- Phase 4: Manual admin reprocessing endpoint

Expected performance: 10-14x faster (84 min -> 6-8 min)

Previous commit accidentally had empty files - this restores the actual implementation.
---
 app/api/admin.py              | 1767 ++++++++++++++++++++++++++++++
 app/services/pdf_processor.py | 1910 +++++++++++++++++++++++++++++++++
 2 files changed, 3677 insertions(+)

diff --git a/app/api/admin.py b/app/api/admin.py
index e69de29..7cdb2e4 100644
--- a/app/api/admin.py
+++ b/app/api/admin.py
@@ -0,0 +1,1767 @@
+"""
+Administrative and Monitoring API Endpoints
+
+This module provides comprehensive administrative and monitoring capabilities including:
+- Job management and status tracking
+- Service statistics and health monitoring
+- Administrative endpoints for data management
+- Bulk operations for document processing
+- System monitoring and performance metrics
+"""
+
+from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks, Query
+from fastapi.responses import JSONResponse
+from typing import List, Optional, Dict, Any
+import asyncio
+import logging
+from datetime import datetime, timedelta
+import psutil
+import os
+from pathlib import Path
+
+from ..schemas.jobs import (
+    JobResponse, JobStatusResponse, JobListResponse, JobListItem,
+    BulkProcessingRequest, BulkProcessingResponse,
+    JobStatistics, SystemMetrics, JobProgressDetail
+)
+from ..schemas.common import BaseResponse, PaginationParams
+from ..services.pdf_processor import PDFProcessor
+from ..services.supabase_client import SupabaseClient
+from ..services.llamaindex_service import LlamaIndexService
+from ..services.product_creation_service import ProductCreationService
+from ..services.material_kai_service import MaterialKaiService
+from ..services.progress_tracker import get_progress_service
+from ..dependencies import get_current_user, get_workspace_context, require_admin
+from ..middleware.jwt_auth import WorkspaceContext, User
+
+# Configure logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+router = APIRouter(prefix="/api", tags=["Health & Monitoring"])
+
+# Global job tracking
+active_jobs: Dict[str, Dict[str, Any]] = {}
+job_history: List[Dict[str, Any]] = []
+
+def migrate_job_data():
+    """Migrate existing job data to match JobListItem schema"""
+    global job_history, active_jobs
+
+    # Migrate job_history
+    for job in job_history:
+        if "priority" not in job:
+            job["priority"] = "normal"
+        if "started_at" not in job:
+            job["started_at"] = None
+        if "completed_at" not in job:
+            job["completed_at"] = None
+        if "progress_percentage" not in job:
+            job["progress_percentage"] = 100.0 if job.get("status") == "completed" else 0.0
+        if "current_step" not in job:
+            job["current_step"] = None
+        if "description" not in job:
+            job["description"] = None
+        if "success" not in job:
+            job["success"] = job.get("status") == "completed"
+        if "error_message" not in job:
+            job["error_message"] = None
+
+    # Migrate active_jobs
+    for job in active_jobs.values():
+        if "priority" not in job:
+            job["priority"] = "normal"
+        if "started_at" not in job:
+            job["started_at"] = None
+        if "completed_at" not in job:
+            job["completed_at"] = None
+        if "progress_percentage" not in job:
+            job["progress_percentage"] = 50.0 if job.get("status") == "running" else 0.0
+        if "current_step" not in job:
+            job["current_step"] = None
+        if "description" not in job:
+            job["description"] = None
+        if "success" not in job:
+            job["success"] = None
+        if "error_message" not in job:
+            job["error_message"] = None
+
+# Run migration on startup
+migrate_job_data()
+
+def get_pdf_processor():
+    """Dependency to get PDF processor instance"""
+    return PDFProcessor()
+
+def get_supabase_client():
+    """Dependency to get Supabase client instance"""
+    return SupabaseClient()
+
+async def get_llamaindex_service() -> LlamaIndexService:
+    """Get LlamaIndex service instance from app state."""
+    from fastapi import HTTPException
+    from app.main import app
+    if not hasattr(app.state, 'llamaindex_service') or app.state.llamaindex_service is None:
+        raise HTTPException(
+            status_code=503,
+            detail="LlamaIndex service is not available"
+        )
+    return app.state.llamaindex_service
+
+def get_material_kai_service():
+    """Dependency to get Material Kai service instance"""
+    return MaterialKaiService()
+
+async def track_job(job_id: str, job_type: str, status: str, details: Dict[str, Any] = None):
+    """Track job status and update global job tracking"""
+    current_time = datetime.utcnow().isoformat()
+
+    # Check if job already exists
+    existing_job = active_jobs.get(job_id)
+
+    if existing_job:
+        # Update existing job
+        job_info = existing_job.copy()
+        job_info["status"] = status
+        job_info["updated_at"] = current_time
+
+        # Set started_at when job starts running
+        if status == "running" and job_info.get("started_at") is None:
+            job_info["started_at"] = current_time
+            logger.info(f"üöÄ Job {job_id} started at {current_time}")
+
+        # Set completed_at when job finishes
+        if status in ["completed", "failed", "cancelled", "error"]:
+            job_info["completed_at"] = current_time
+            if status == "completed":
+                job_info["success"] = True
+                job_info["progress_percentage"] = 100.0
+            elif status in ["failed", "error"]:
+                job_info["success"] = False
+            logger.info(f"üèÅ Job {job_id} finished with status: {status}")
+
+        # Update progress from details if provided
+        if details:
+            if "progress_percentage" in details:
+                job_info["progress_percentage"] = details["progress_percentage"]
+                logger.info(f"üìä Job {job_id} progress: {details['progress_percentage']}%")
+            if "current_step" in details:
+                job_info["current_step"] = details["current_step"]
+                logger.info(f"üìã Job {job_id} step: {details['current_step']}")
+            if "error" in details:
+                job_info["error_message"] = details["error"]
+
+            # Merge details
+            existing_details = job_info.get("details", {})
+            existing_details.update(details)
+            job_info["details"] = existing_details
+    else:
+        # Create new job
+        job_info = {
+            "job_id": job_id,
+            "job_type": job_type,
+            "status": status,
+            "priority": "normal",
+            "created_at": current_time,
+            "updated_at": current_time,
+            "started_at": current_time if status == "running" else None,
+            "completed_at": None,
+            "progress_percentage": 10.0 if status == "running" else 0.0,
+            "current_step": details.get("current_step") if details else None,
+            "description": None,
+            "tags": [],
+            "success": None,
+            "error_message": None,
+            "details": details or {},
+            "parameters": details or {},
+            "retry_count": 3,
+            "current_retry": 0,
+            "result": None
+        }
+        logger.info(f"üìù Created new job {job_id} with status: {status}")
+
+    active_jobs[job_id] = job_info
+
+    # Add to history if completed or failed
+    if status in ["completed", "failed", "cancelled"]:
+        job_history.append(job_info.copy())
+        if job_id in active_jobs:
+            del active_jobs[job_id]
+        logger.info(f"üìö Job {job_id} moved to history")
+
+# Job Management Endpoints
+
+@router.get("/jobs", response_model=JobListResponse)
+async def list_jobs(
+    status: Optional[str] = Query(None, description="Filter by job status"),
+    job_type: Optional[str] = Query(None, description="Filter by job type"),
+    pagination: PaginationParams = Depends(),
+
+):
+    """
+    List all jobs with optional filtering and pagination
+    
+    - **status**: Filter jobs by status (pending, running, completed, failed, cancelled)
+    - **job_type**: Filter jobs by type (document_processing, bulk_processing, etc.)
+    - **limit**: Number of jobs to return (default: 50, max: 100)
+    - **offset**: Number of jobs to skip for pagination
+    """
+    try:
+        # Combine active jobs and job history
+        all_jobs = list(active_jobs.values()) + job_history
+        
+        # Apply filters
+        filtered_jobs = all_jobs
+        if status:
+            filtered_jobs = [job for job in filtered_jobs if job["status"] == status]
+        if job_type:
+            filtered_jobs = [job for job in filtered_jobs if job["job_type"] == job_type]
+        
+        # Sort by created_at descending
+        filtered_jobs.sort(key=lambda x: x["created_at"], reverse=True)
+        
+        # Apply pagination
+        start_idx = (pagination.page - 1) * pagination.page_size
+        end_idx = start_idx + pagination.page_size
+        paginated_jobs = filtered_jobs[start_idx:end_idx]
+        
+        return JobListResponse(
+            success=True,
+            message="Jobs retrieved successfully",
+            jobs=[JobListItem(**job) for job in paginated_jobs],
+            total_count=len(filtered_jobs),
+            page=pagination.page,
+            page_size=pagination.page_size,
+            status_counts={
+                "active": len(active_jobs),
+                "completed": len([j for j in job_history if j["status"] == "completed"]),
+                "failed": len([j for j in job_history if j["status"] == "failed"])
+            },
+            type_counts={}
+        )
+        
+    except Exception as e:
+        logger.error(f"Error listing jobs: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to list jobs: {str(e)}")
+
+@router.get("/jobs/statistics", response_model=Dict[str, Any])
+async def get_job_statistics():
+    """
+    Get comprehensive job statistics and metrics
+    """
+    try:
+        # Calculate statistics
+        all_jobs = list(active_jobs.values()) + job_history
+        
+        # Status distribution
+        status_counts = {}
+        for job in all_jobs:
+            status = job["status"]
+            status_counts[status] = status_counts.get(status, 0) + 1
+        
+        # Job type distribution
+        type_counts = {}
+        for job in all_jobs:
+            job_type = job["job_type"]
+            type_counts[job_type] = type_counts.get(job_type, 0) + 1
+        
+        # Recent activity (last 24 hours)
+        recent_cutoff = datetime.utcnow() - timedelta(hours=24)
+        recent_jobs = [
+            job for job in all_jobs 
+            if datetime.fromisoformat(job["created_at"].replace('Z', '+00:00')) > recent_cutoff
+        ]
+        
+        # Average processing time for completed jobs
+        completed_jobs = [job for job in job_history if job["status"] == "completed"]
+        avg_processing_time = None
+        if completed_jobs:
+            processing_times = []
+            for job in completed_jobs:
+                created = datetime.fromisoformat(job["created_at"].replace('Z', '+00:00'))
+                updated = datetime.fromisoformat(job["updated_at"].replace('Z', '+00:00'))
+                processing_times.append((updated - created).total_seconds())
+            avg_processing_time = sum(processing_times) / len(processing_times)
+        
+        statistics = JobStatistics(
+            total_jobs=len(all_jobs),
+            active_jobs=len(active_jobs),
+            completed_jobs=len([j for j in job_history if j["status"] == "completed"]),
+            failed_jobs=len([j for j in job_history if j["status"] == "failed"]),
+            cancelled_jobs=len([j for j in job_history if j["status"] == "cancelled"]),
+            status_distribution=status_counts,
+            type_distribution=type_counts,
+            recent_jobs_24h=len(recent_jobs),
+            average_processing_time_seconds=avg_processing_time
+        )
+        
+        return {
+            "success": True,
+            "message": "Job statistics retrieved successfully",
+            "data": statistics.model_dump(),
+            "timestamp": datetime.utcnow().isoformat()
+        }
+        
+    except Exception as e:
+        logger.error(f"Error getting job statistics: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to get job statistics: {str(e)}")
+
+# Bulk Operations
+
+@router.get("/jobs/{job_id}", response_model=JobStatusResponse)
+async def get_job_status(
+    job_id: str,
+
+):
+    """
+    Get detailed status information for a specific job
+
+    - **job_id**: Unique identifier for the job
+    """
+    try:
+        # Check active jobs first
+        if job_id in active_jobs:
+            job_info = active_jobs[job_id]
+        else:
+            # Check job history
+            job_info = next((job for job in job_history if job["job_id"] == job_id), None)
+
+            # If not found in memory, check database
+            if not job_info:
+                from app.services.supabase_client import get_supabase_client
+                supabase_client = get_supabase_client()
+                response = supabase_client.client.table('background_jobs').select('*').eq('id', job_id).execute()
+
+                if response.data and len(response.data) > 0:
+                    db_job = response.data[0]
+                    # Convert database format to expected format
+                    job_info = {
+                        "job_id": db_job['id'],
+                        "job_type": "document_processing",
+                        "status": db_job['status'],
+                        "progress": db_job.get('progress', 0),
+                        "document_id": db_job.get('document_id'),
+                        "filename": db_job.get('filename'),
+                        "error": db_job.get('error'),
+                        "created_at": db_job.get('created_at'),
+                        "updated_at": db_job.get('updated_at')
+                    }
+
+        if not job_info:
+            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
+
+        return JobStatusResponse(
+            success=True,
+            message="Job status retrieved successfully",
+            data=JobResponse(**job_info)
+        )
+
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Error getting job status: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to get job status: {str(e)}")
+
+@router.get("/jobs/{job_id}/status")
+async def get_job_status_alt(
+    job_id: str,
+):
+    """
+    Get detailed status information for a specific job (alternative endpoint)
+
+    - **job_id**: Unique identifier for the job
+    """
+    try:
+        # Check active jobs first
+        if job_id in active_jobs:
+            job_info = active_jobs[job_id]
+        else:
+            # Check job history
+            job_info = next((job for job in job_history if job["job_id"] == job_id), None)
+
+            # If not found in memory, check database
+            if not job_info:
+                from app.services.supabase_client import get_supabase_client
+                supabase_client = get_supabase_client()
+                response = supabase_client.client.table('background_jobs').select('*').eq('id', job_id).execute()
+
+                if response.data and len(response.data) > 0:
+                    db_job = response.data[0]
+                    # Convert database format to expected format
+                    job_info = {
+                        "job_id": db_job['id'],
+                        "job_type": "document_processing",
+                        "status": db_job['status'],
+                        "progress": db_job.get('progress', 0),
+                        "document_id": db_job.get('document_id'),
+                        "filename": db_job.get('filename'),
+                        "error": db_job.get('error'),
+                        "created_at": db_job.get('created_at'),
+                        "updated_at": db_job.get('updated_at')
+                    }
+
+        if not job_info:
+            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
+
+        return {
+            "success": True,
+            "message": f"Job {job_id} status retrieved successfully",
+            "data": job_info,
+            "timestamp": datetime.utcnow().isoformat()
+        }
+
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Error getting job status: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to get job status: {str(e)}")
+
+@router.delete("/jobs/{job_id}")
+async def cancel_job(
+    job_id: str,
+
+):
+    """
+    Cancel a running job
+    
+    - **job_id**: Unique identifier for the job to cancel
+    """
+    try:
+        if job_id not in active_jobs:
+            raise HTTPException(status_code=404, detail=f"Active job {job_id} not found")
+        
+        job_info = active_jobs[job_id]
+        if job_info["status"] in ["completed", "failed", "cancelled"]:
+            raise HTTPException(status_code=400, detail=f"Job {job_id} is already {job_info['status']}")
+        
+        # Update job status to cancelled
+        await track_job(job_id, job_info["job_type"], "cancelled", job_info["details"])
+        
+        return BaseResponse(
+            success=True,
+            message=f"Job {job_id} cancelled successfully"
+        )
+        
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Error cancelling job: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to cancel job: {str(e)}")
+
+@router.post("/bulk/process")
+async def bulk_process_documents(
+    request: BulkProcessingRequest,
+    background_tasks: BackgroundTasks,
+    pdf_processor: PDFProcessor = Depends(get_pdf_processor),
+):
+    logger.info(f"üîç ENTRY DEBUG: request type: {type(request)}")
+    logger.info(f"üîç ENTRY DEBUG: request: {request}")
+    """
+    Process multiple documents in bulk
+
+    - **urls**: List of document URLs to process
+    - **options**: Processing options (extract_images, generate_summary, etc.)
+    - **batch_size**: Number of documents to process concurrently (default: 5)
+    """
+    try:
+        # Extract request data from Pydantic model
+        logger.info(f"üîç DEBUG: request type: {type(request)}")
+        logger.info(f"üîç DEBUG: request.urls type: {type(request.urls)}")
+        logger.info(f"üîç DEBUG: request.options type: {type(request.options)}")
+
+        urls = request.urls
+        batch_size = request.batch_size
+        options = request.options or {}
+
+        if not urls:
+            raise HTTPException(status_code=400, detail="No URLs provided")
+
+        job_id = f"bulk_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
+
+        # Track the bulk job
+        await track_job(
+            job_id,
+            "bulk_processing",
+            "pending",
+            {
+                "total_documents": len(urls),
+                "batch_size": batch_size,
+                "options": options
+            }
+        )
+
+        # Start background processing
+        background_tasks.add_task(
+            process_bulk_documents,
+            job_id,
+            urls,
+            options,
+            batch_size,
+            pdf_processor
+        )
+
+        return {
+            "success": True,
+            "message": "Bulk processing started successfully",
+            "data": {
+                "job_id": job_id,
+                "total_documents": len(urls),
+                "estimated_completion_time": (datetime.utcnow() + timedelta(
+                    minutes=len(urls) * 2  # Rough estimate
+                )).isoformat()
+            },
+            "timestamp": datetime.utcnow().isoformat()
+        }
+
+    except Exception as e:
+        import traceback
+        logger.error(f"Error starting bulk processing: {str(e)}")
+        logger.error(f"Full traceback: {traceback.format_exc()}")
+        raise HTTPException(status_code=500, detail=f"Failed to start bulk processing: {str(e)}")
+
+async def process_bulk_documents(
+    job_id: str,
+    urls: List[str],
+    options: Any,
+    batch_size: int,
+    pdf_processor: PDFProcessor
+):
+    """Background task for bulk document processing with comprehensive error handling"""
+    try:
+        logger.info(f"üöÄ Background task starting for job {job_id}")
+        await track_job(job_id, "bulk_processing", "running", {
+            "current_step": "Background processing started",
+            "progress_percentage": 10.0
+        })
+        logger.info(f"‚úÖ Job {job_id} status updated to running")
+
+        processed_count = 0
+        failed_count = 0
+        results = []
+        total_chunks = 0
+        total_images = 0
+        
+        # Process documents in batches
+        for i in range(0, len(urls), batch_size):
+            batch_urls = urls[i:i + batch_size]
+            batch_tasks = []
+            
+            for url in batch_urls:
+                task = asyncio.create_task(process_single_document(url, options, pdf_processor, job_id))
+                batch_tasks.append(task)
+            
+            # Wait for batch completion
+            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
+            
+            for j, result in enumerate(batch_results):
+                if isinstance(result, Exception):
+                    failed_count += 1
+                    logger.error(f"Document processing failed for {batch_urls[j]}: {str(result)}")
+                    results.append({
+                        "url": batch_urls[j],
+                        "status": "failed",
+                        "error": str(result)
+                    })
+                else:
+                    processed_count += 1
+                    # Extract metrics from result
+                    chunks_count = result.get("chunks", 0)
+                    images_count = result.get("images", 0)
+                    total_chunks += chunks_count
+                    total_images += images_count
+
+                    logger.info(f"‚úÖ Document processed: {batch_urls[j]} - {chunks_count} chunks, {images_count} images")
+                    results.append({
+                        "url": batch_urls[j],
+                        "status": "completed",
+                        "document_id": result.get("document_id"),
+                        "chunks": chunks_count,
+                        "images": images_count
+                    })
+
+            # Update job progress with detailed metrics
+            progress = (processed_count + failed_count) / len(urls) * 100
+            current_step = f"Processed {processed_count + failed_count}/{len(urls)} documents"
+
+            logger.info(f"üìä Progress update: {progress:.1f}% - {current_step}")
+            await track_job(
+                job_id,
+                "bulk_processing",
+                "running",
+                {
+                    "progress_percentage": progress,
+                    "current_step": current_step,
+                    "processed_count": processed_count,
+                    "failed_count": failed_count,
+                    "total_documents": len(urls),
+                    "chunks_created": total_chunks,
+                    "images_extracted": total_images,
+                    "results": results
+                }
+            )
+        
+        # Mark job as completed
+        logger.info(f"üéâ Job {job_id} completed successfully: {processed_count} processed, {failed_count} failed")
+        await track_job(
+            job_id,
+            "bulk_processing",
+            "completed",
+            {
+                "progress_percentage": 100.0,
+                "current_step": "Processing completed",
+                "total_processed": processed_count,
+                "total_failed": failed_count,
+                "total_documents": len(urls),
+                "chunks_created": total_chunks,
+                "images_extracted": total_images,
+                "results": results
+            }
+        )
+
+    except Exception as e:
+        import traceback
+        error_details = traceback.format_exc()
+        logger.error(f"‚ùå Critical error in bulk processing job {job_id}: {str(e)}")
+        logger.error(f"Full traceback: {error_details}")
+        await track_job(
+            job_id,
+            "bulk_processing",
+            "failed",
+            {
+                "error": str(e),
+                "error_details": error_details,
+                "processed_count": processed_count,
+                "failed_count": failed_count
+            }
+        )
+
+async def process_single_document(url: str, options: Any, pdf_processor: PDFProcessor, job_id: str = None):
+    """Process a single document as part of bulk processing with progress tracking"""
+    try:
+        from app.services.supabase_client import get_supabase_client
+        from app.schemas.jobs import ProcessingStage
+
+        # Use the actual PDF processor to process the document
+        import uuid
+        document_id = str(uuid.uuid4())  # Generate proper UUID for database compatibility
+
+        # Extract filename from URL
+        original_filename = url.split('/')[-1] if url else f"{document_id}.pdf"
+
+        # Initialize progress tracking
+        progress_service = get_progress_service()
+        tracker = None
+        if job_id:
+            tracker = progress_service.create_tracker(job_id, document_id, 1)
+            tracker.start_processing()
+
+        # Create async progress callback for PDF processor
+        async def progress_callback(progress_percentage: float, current_step: str, details: dict = None):
+            """Async callback to update job progress during PDF processing"""
+            try:
+                if job_id:
+                    # Ensure progress is within bounds and meaningful
+                    progress_percentage = max(10.0, min(95.0, progress_percentage))
+
+                    # Update job tracking in active_jobs
+                    if job_id in active_jobs:
+                        job_info = active_jobs[job_id]
+                        job_info["progress_percentage"] = progress_percentage
+                        job_info["current_step"] = current_step
+                        job_info["updated_at"] = datetime.utcnow().isoformat()
+
+                        # Update details
+                        if details:
+                            if "details" not in job_info:
+                                job_info["details"] = {}
+                            job_info["details"].update(details)
+
+                        logger.info(f"üìä Job {job_id} progress updated: {progress_percentage}% - {current_step}")
+                    else:
+                        logger.warning(f"Job {job_id} not found in active_jobs for progress update")
+            except Exception as e:
+                logger.warning(f"Failed to update job progress: {e}")
+
+        # Update progress before processing
+        await progress_callback(15.0, "Starting PDF download and analysis")
+
+        result = await pdf_processor.process_pdf_from_url(url, document_id, options or {}, progress_callback)
+
+        # Update progress after PDF processing
+        await progress_callback(40.0, "PDF processing completed, analyzing content")
+
+        # Update progress tracking with actual page count
+        if tracker:
+            tracker.total_pages = result.page_count
+            tracker.update_stage(ProcessingStage.EXTRACTING_TEXT)
+
+        # Create chunks from markdown content using LlamaIndex
+        chunks = []
+        chunks_created = 0
+        if result.markdown_content:
+            try:
+                # Update progress for chunking
+                await progress_callback(60.0, "Creating text chunks for RAG pipeline")
+
+                await track_job(
+                    job_id,
+                    "bulk_processing",
+                    "running",
+                    {
+                        "current_step": "Creating text chunks for RAG pipeline",
+                        "progress_percentage": 60.0,
+                        "text_length": len(result.markdown_content)
+                    }
+                )
+
+                # Use LlamaIndex for intelligent chunking
+                llamaindex_service = await get_llamaindex_service()
+
+                # Create a temporary text file for chunking
+                import tempfile
+                with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as temp_file:
+                    temp_file.write(result.markdown_content)
+                    temp_file_path = temp_file.name
+
+                try:
+                    # Read the temp file content
+                    with open(temp_file_path, 'rb') as f:
+                        file_content = f.read()
+
+                    # Index the document with chunking and embedding generation
+                    chunk_result = await llamaindex_service.index_document_content(
+                        file_content=file_content,
+                        document_id=document_id,
+                        file_path=temp_file_path,
+                        metadata={
+                            "source_url": url,
+                            "original_filename": original_filename,
+                            "processing_timestamp": datetime.utcnow().isoformat(),
+                            "page_count": result.page_count,
+                            "image_count": len(result.extracted_images) if result.extracted_images else 0,
+                            "workspace_id": None  # Use None instead of "default" to avoid UUID validation error
+                        }
+                    )
+
+                    chunks_created = chunk_result.get("statistics", {}).get("total_chunks", 0)
+
+                    # CRITICAL FIX: Extract actual chunks from the result
+                    if chunk_result.get("chunks"):
+                        chunks = chunk_result["chunks"]
+                        logger.info(f"‚úÖ Extracted {len(chunks)} chunks from LlamaIndex result")
+                    else:
+                        # Fallback: Use simple text splitting if chunks not in result
+                        text_chunks = result.markdown_content.split('\n\n')
+                        chunks = [chunk.strip() for chunk in text_chunks if chunk.strip()]
+                        chunks_created = len(chunks)
+                        logger.info(f"‚úÖ Fallback chunking created {chunks_created} chunks")
+
+                    logger.info(f"‚úÖ Total chunks ready for database: {len(chunks)}")
+
+                finally:
+                    # Clean up temp file
+                    os.unlink(temp_file_path)
+
+                # Note: Embeddings are generated by LlamaIndex service during index_document_content
+                # This happens automatically when the document is indexed for RAG search
+                await progress_callback(70.0, "Preparing for RAG indexing with embeddings")
+
+            except Exception as chunk_error:
+                logger.warning(f"Chunking failed, using simple text splitting: {chunk_error}")
+                # Fallback to simple text splitting
+                text_chunks = result.markdown_content.split('\n\n')
+                chunks = [chunk.strip() for chunk in text_chunks if chunk.strip()]
+                chunks_created = len(chunks)
+
+        # ‚úÖ NEW: Upload processed document to RAG system for indexing
+        try:
+            logger.info(f"üîç Starting RAG upload for document {document_id}")
+            await progress_callback(85.0, "Uploading document to RAG system for search indexing")
+
+            await track_job(
+                job_id,
+                "bulk_processing",
+                "running",
+                {
+                    "current_step": "Uploading document to RAG system for search indexing",
+                    "progress_percentage": 85.0,
+                    "chunks_created": chunks_created,
+                    "document_id": document_id
+                }
+            )
+
+            # Get LlamaIndex service for RAG upload
+            logger.info(f"üîß Getting LlamaIndex service for RAG upload...")
+            llamaindex_service = await get_llamaindex_service()
+            logger.info(f"‚úÖ LlamaIndex service obtained: {type(llamaindex_service)}")
+
+            # Download PDF bytes for RAG indexing
+            logger.info(f"üì• Downloading PDF bytes from: {url}")
+            import aiohttp
+            async with aiohttp.ClientSession() as session:
+                async with session.get(url) as response:
+                    if response.status == 200:
+                        pdf_bytes = await response.read()
+                        logger.info(f"‚úÖ PDF downloaded: {len(pdf_bytes)} bytes")
+                    else:
+                        raise Exception(f"Failed to download PDF: HTTP {response.status}")
+
+            # Index the document content for RAG search
+            logger.info(f"üöÄ Calling index_document_content for document {document_id}")
+            rag_result = await llamaindex_service.index_document_content(
+                file_content=pdf_bytes,
+                document_id=document_id,
+                file_path=f"{document_id}.pdf",
+                metadata={
+                    "filename": url.split('/')[-1] if url else f"{document_id}.pdf",
+                    "total_pages": result.page_count,
+                    "chunks_created": chunks_created,
+                    "images_extracted": len(result.extracted_images or []),
+                    "processing_method": getattr(result, 'processing_method', 'unknown'),
+                    "file_url": url
+                },
+                chunk_size=1000,
+                chunk_overlap=200
+            )
+
+            logger.info(f"‚úÖ Document {document_id} successfully indexed in RAG system")
+            logger.info(f"üìä RAG indexing result: {rag_result}")
+
+        except Exception as rag_error:
+            logger.error(f"‚ö†Ô∏è Failed to index document in RAG system: {rag_error}")
+            logger.error(f"üîç RAG error details: {type(rag_error).__name__}: {str(rag_error)}")
+            import traceback
+            logger.error(f"üîç RAG error traceback: {traceback.format_exc()}")
+            # Don't fail the entire job if RAG indexing fails - this is optional
+            logger.info("üìÑ Continuing with database save despite RAG indexing failure")
+
+        # Save to database
+        supabase_client = get_supabase_client()
+
+        # Save PDF processing result to database
+        try:
+            pdf_result = await supabase_client.save_pdf_processing_result(
+                result=result,  # Pass the PDFProcessingResult object
+                original_filename=original_filename,
+                file_url=url  # Pass the source URL
+            )
+            logger.info(f"‚úÖ PDF processing result saved to database: {pdf_result}")
+
+            # Update progress tracking
+            if tracker:
+                tracker.update_database_stats(records_created=1)
+                tracker.update_stage(ProcessingStage.SAVING_TO_DATABASE)
+
+        except Exception as e:
+            logger.error(f"‚ùå Failed to save PDF processing result: {str(e)}")
+            # Continue processing even if database save fails
+
+        # Save knowledge base entries
+        try:
+            kb_entries_saved = await supabase_client.save_knowledge_base_entries(
+                document_id=result.document_id,
+                chunks=chunks,
+                images=result.extracted_images
+            )
+            logger.info(f"‚úÖ Knowledge base entries saved: {kb_entries_saved}")
+
+        except Exception as e:
+            logger.error(f"‚ùå Failed to save knowledge base entries: {str(e)}")
+            # Continue processing even if knowledge base save fails
+        # Final progress tracking update
+        chunks_saved = len(chunks)
+        images_saved = len(result.extracted_images)
+
+        if tracker:
+            tracker.update_database_stats(
+                kb_entries=chunks_saved,
+                images_stored=images_saved
+            )
+            tracker.update_stage(ProcessingStage.COMPLETED)
+
+        logger.info(f"üéâ Document processing completed: {chunks_saved} chunks, {images_saved} images")
+
+        return {
+            "document_id": result.document_id,
+            "status": "completed",
+            "chunks": len(chunks),
+            "images": len(result.extracted_images),
+            "text_content": result.markdown_content,
+            "ocr_text": result.ocr_text,
+            "metadata": {
+                "pages": result.page_count,
+                "word_count": result.word_count,
+                "character_count": result.character_count,
+                "processing_time": result.processing_time
+            }
+        }
+
+    except Exception as e:
+        import traceback
+        error_details = traceback.format_exc()
+        logger.error(f"‚ùå Error processing document {url}: {str(e)}")
+        logger.error(f"Full traceback: {error_details}")
+
+        # Update tracker with error if available
+        if tracker:
+            tracker.add_error("Document Processing Failed", str(e), {"url": url, "traceback": error_details})
+
+        raise
+
+# System Monitoring
+
+@router.get("/system/health")
+async def get_system_health(
+
+):
+    """
+    Get comprehensive system health status
+    """
+    try:
+        # System metrics
+        cpu_percent = psutil.cpu_percent(interval=1)
+        memory = psutil.virtual_memory()
+        disk = psutil.disk_usage('/')
+        
+        # Service health checks
+        services_health = {}
+        
+        # Check Supabase connection
+        try:
+            import time
+            start_time = time.time()
+            supabase_client = get_supabase_client()
+            # Simple health check query using the health_check method
+            if supabase_client.health_check():
+                response_time_ms = int((time.time() - start_time) * 1000)
+                services_health["supabase"] = {
+                    "status": "healthy",
+                    "response_time_ms": response_time_ms
+                }
+            else:
+                services_health["supabase"] = {
+                    "status": "unhealthy",
+                    "error": "Health check failed"
+                }
+        except Exception as e:
+            services_health["supabase"] = {
+                "status": "unhealthy",
+                "error": str(e)
+            }
+        
+        # Check LlamaIndex service
+        try:
+            llamaindex_service = LlamaIndexService()
+            health_check = await llamaindex_service.health_check()
+            services_health["llamaindex"] = {
+                "status": "healthy" if health_check["status"] == "healthy" else "unhealthy",
+                "details": health_check
+            }
+        except Exception as e:
+            services_health["llamaindex"] = {
+                "status": "unhealthy",
+                "error": str(e)
+            }
+        
+        # Check Material Kai service
+        try:
+            material_kai_service = MaterialKaiService()
+            health_check = await material_kai_service.health_check()
+            services_health["material_kai"] = {
+                "status": "healthy" if health_check["status"] == "healthy" else "unhealthy",
+                "details": health_check
+            }
+        except Exception as e:
+            services_health["material_kai"] = {
+                "status": "unhealthy",
+                "error": str(e)
+            }
+        
+        # Overall health status
+        all_services_healthy = all(
+            service["status"] == "healthy" 
+            for service in services_health.values()
+        )
+        
+        # Get real system uptime
+        import time
+        boot_time = psutil.boot_time()
+        uptime_seconds = time.time() - boot_time
+
+        system_metrics = SystemMetrics(
+            cpu_usage_percent=cpu_percent,
+            memory_usage_percent=memory.percent,
+            memory_available_gb=memory.available / (1024**3),
+            disk_usage_percent=disk.percent,
+            disk_free_gb=disk.free / (1024**3),
+            active_jobs_count=len(active_jobs),
+            uptime_seconds=uptime_seconds
+        )
+        
+        return {
+            "success": True,
+            "message": "System health retrieved successfully",
+            "data": {
+                "overall_status": "healthy" if all_services_healthy and cpu_percent < 80 and memory.percent < 80 else "degraded",
+                "system_metrics": system_metrics.model_dump(),
+                "services": services_health,
+                "active_jobs": len(active_jobs),
+                "timestamp": datetime.utcnow().isoformat()
+            }
+        }
+        
+    except Exception as e:
+        logger.error(f"Error getting system health: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to get system health: {str(e)}")
+
+@router.get("/system/metrics")
+async def get_system_metrics():
+    """
+    Get detailed system performance metrics
+    """
+    try:
+        # CPU metrics
+        cpu_count = psutil.cpu_count()
+        cpu_percent_per_core = psutil.cpu_percent(percpu=True, interval=1)
+        cpu_freq = psutil.cpu_freq()
+        
+        # Memory metrics
+        memory = psutil.virtual_memory()
+        swap = psutil.swap_memory()
+        
+        # Disk metrics
+        disk_usage = psutil.disk_usage('/')
+        disk_io = psutil.disk_io_counters()
+        
+        # Network metrics
+        network_io = psutil.net_io_counters()
+        
+        # Process metrics
+        process = psutil.Process()
+        process_memory = process.memory_info()
+        
+        return {
+            "success": True,
+            "message": "System metrics retrieved successfully",
+            "data": {
+                "cpu": {
+                    "count": cpu_count,
+                    "usage_percent": psutil.cpu_percent(),
+                    "usage_per_core": cpu_percent_per_core,
+                    "frequency_mhz": cpu_freq.current if cpu_freq else None
+                },
+                "memory": {
+                    "total_gb": memory.total / (1024**3),
+                    "available_gb": memory.available / (1024**3),
+                    "used_gb": memory.used / (1024**3),
+                    "usage_percent": memory.percent,
+                    "swap_total_gb": swap.total / (1024**3),
+                    "swap_used_gb": swap.used / (1024**3),
+                    "swap_percent": swap.percent
+                },
+                "disk": {
+                    "total_gb": disk_usage.total / (1024**3),
+                    "used_gb": disk_usage.used / (1024**3),
+                    "free_gb": disk_usage.free / (1024**3),
+                    "usage_percent": disk_usage.percent,
+                    "read_bytes": disk_io.read_bytes if disk_io else None,
+                    "write_bytes": disk_io.write_bytes if disk_io else None
+                },
+                "network": {
+                    "bytes_sent": network_io.bytes_sent,
+                    "bytes_received": network_io.bytes_recv,
+                    "packets_sent": network_io.packets_sent,
+                    "packets_received": network_io.packets_recv
+                },
+                "process": {
+                    "memory_rss_mb": process_memory.rss / (1024**2),
+                    "memory_vms_mb": process_memory.vms / (1024**2),
+                    "cpu_percent": process.cpu_percent()
+                },
+                "jobs": {
+                    "active_count": len(active_jobs),
+                    "total_history": len(job_history)
+                }
+            },
+            "timestamp": datetime.utcnow().isoformat()
+        }
+        
+    except Exception as e:
+        logger.error(f"Error getting system metrics: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to get system metrics: {str(e)}")
+
+# Administrative Data Management
+
+@router.delete("/data/cleanup")
+async def cleanup_old_data(
+    days_old: int = Query(30, description="Delete data older than this many days"),
+    dry_run: bool = Query(True, description="Preview what would be deleted without actually deleting")
+):
+    """
+    Clean up old data from the system
+    
+    - **days_old**: Delete data older than this many days (default: 30)
+    - **dry_run**: Preview what would be deleted without actually deleting (default: true)
+    """
+    try:
+        global job_history  # Declare global at the beginning of the function
+
+        cutoff_date = datetime.utcnow() - timedelta(days=days_old)
+
+        # Find old job history entries
+        old_jobs = [
+            job for job in job_history
+            if datetime.fromisoformat(job["created_at"].replace('Z', '+00:00')) < cutoff_date
+        ]
+
+        cleanup_summary = {
+            "old_jobs_count": len(old_jobs),
+            "cutoff_date": cutoff_date.isoformat(),
+            "dry_run": dry_run
+        }
+
+        if not dry_run:
+            # Actually remove old jobs from history
+            job_history = [
+                job for job in job_history
+                if datetime.fromisoformat(job["created_at"].replace('Z', '+00:00')) >= cutoff_date
+            ]
+            cleanup_summary["jobs_deleted"] = len(old_jobs)
+        
+        return {
+            "success": True,
+            "message": f"Data cleanup {'preview' if dry_run else 'completed'} successfully",
+            "data": cleanup_summary
+        }
+        
+    except Exception as e:
+        logger.error(f"Error during data cleanup: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to cleanup data: {str(e)}")
+
+@router.post("/data/backup")
+async def create_data_backup():
+    """
+    Create a backup of system data
+    """
+    try:
+        backup_data = {
+            "timestamp": datetime.utcnow().isoformat(),
+            "active_jobs": active_jobs,
+            "job_history": job_history,
+            "system_info": {
+                "version": "1.0.0",  # Would come from app config
+                "backup_type": "administrative_data"
+            }
+        }
+        
+        # In a real implementation, this would save to a file or external storage
+        backup_id = f"backup_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
+        
+        return {
+            "success": True,
+            "message": "Data backup created successfully",
+            "data": {
+                "backup_id": backup_id,
+                "backup_size_bytes": len(str(backup_data)),
+                "items_backed_up": {
+                    "active_jobs": len(active_jobs),
+                    "job_history": len(job_history)
+                }
+            }
+        }
+        
+    except Exception as e:
+        logger.error(f"Error creating data backup: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to create data backup: {str(e)}")
+
+@router.get("/data/export")
+async def export_system_data(
+    format: str = Query("json", description="Export format (json, csv)"),
+    data_type: str = Query("jobs", description="Type of data to export (jobs, metrics)")
+):
+    """
+    Export system data in various formats
+    
+    - **format**: Export format (json, csv)
+    - **data_type**: Type of data to export (jobs, metrics)
+    """
+    try:
+        if data_type == "jobs":
+            all_jobs = list(active_jobs.values()) + job_history
+            
+            if format == "json":
+                return JSONResponse(
+                    content={
+                        "success": True,
+                        "message": "Jobs data exported successfully",
+                        "data": all_jobs,
+                        "export_info": {
+                            "format": format,
+                            "data_type": data_type,
+                            "record_count": len(all_jobs),
+                            "exported_at": datetime.utcnow().isoformat()
+                        }
+                    }
+                )
+            elif format == "csv":
+                # In a real implementation, this would return a CSV file
+                return {
+                    "success": True,
+                    "message": "CSV export not implemented yet",
+                    "data": {"note": "CSV export would be implemented here"}
+                }
+        
+        return {
+            "success": True,
+            "message": f"Export completed for {data_type} in {format} format",
+            "data": {"note": f"Export functionality for {data_type} would be implemented here"}
+        }
+        
+    except Exception as e:
+        logger.error(f"Error exporting data: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to export data: {str(e)}")
+
+
+@router.get("/packages/status")
+async def get_package_status():
+    """
+    Get the status of all system packages and dependencies.
+
+    Returns package information for both critical and optional dependencies,
+    including version information and availability status.
+    """
+    try:
+        return await get_basic_package_status()
+    except Exception as e:
+        logger.error(f"Error getting package status: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to get package status: {str(e)}")
+
+
+async def get_basic_package_status():
+    """Get package status by parsing requirements.txt and checking imports"""
+    import importlib
+    import re
+    import os
+
+    # Parse requirements.txt to get all packages
+    requirements_path = "/var/www/mivaa-pdf-extractor/requirements.txt"
+    if not os.path.exists(requirements_path):
+        requirements_path = "requirements.txt"  # Fallback for local development
+
+    packages_from_requirements = {}
+
+    try:
+        with open(requirements_path, 'r') as f:
+            for line in f:
+                line = line.strip()
+                # Skip comments and empty lines
+                if line.startswith('#') or not line:
+                    continue
+
+                # Parse package name and version
+                # Handle formats like: package>=1.0.0, package==1.0.0, package[extra]>=1.0.0
+                match = re.match(r'^([a-zA-Z0-9_-]+)(\[.*?\])?([><=!]+.*)?', line)
+                if match:
+                    package_name = match.group(1)
+                    version_spec = match.group(3) or ''
+
+                    # Map some package names to their import names
+                    import_name = package_name
+                    if package_name == 'opencv-python-headless':
+                        import_name = 'cv2'
+                    elif package_name == 'pillow':
+                        import_name = 'PIL'
+                    elif package_name == 'python-dotenv':
+                        import_name = 'dotenv'
+                    elif package_name == 'python-multipart':
+                        import_name = 'multipart'
+                    elif package_name == 'python-dateutil':
+                        import_name = 'dateutil'
+                    elif package_name == 'python-json-logger':
+                        import_name = 'pythonjsonlogger'
+                    elif package_name == 'email-validator':
+                        import_name = 'email_validator'
+                    elif package_name == 'llama-index':
+                        import_name = 'llama_index'
+
+                    packages_from_requirements[package_name] = {
+                        'import_name': import_name,
+                        'version_spec': version_spec.strip(),
+                        'required': True
+                    }
+    except Exception as e:
+        logger.error(f"Error reading requirements.txt: {e}")
+
+    # Check each package
+    package_status = {}
+    critical_packages = {
+        'fastapi', 'uvicorn', 'pydantic', 'supabase', 'pymupdf4llm',
+        'numpy', 'pandas', 'opencv-python-headless', 'pillow', 'llama-index',
+        'openai', 'anthropic', 'torch'
+    }
+
+    for package_name, info in packages_from_requirements.items():
+        try:
+            module = importlib.import_module(info['import_name'])
+            version = getattr(module, '__version__', 'unknown')
+            package_status[package_name] = {
+                'available': True,
+                'version': version,
+                'version_spec': info['version_spec'],
+                'critical': package_name in critical_packages,
+                'import_name': info['import_name']
+            }
+        except ImportError:
+            package_status[package_name] = {
+                'available': False,
+                'version': None,
+                'version_spec': info['version_spec'],
+                'critical': package_name in critical_packages,
+                'import_name': info['import_name'],
+                'error': 'Package not found'
+            }
+
+    # Calculate summary
+    critical_missing = sum(1 for pkg, status in package_status.items()
+                          if pkg in critical_packages and not status['available'])
+    total_packages = len(package_status)
+    total_critical = len([pkg for pkg in package_status.keys() if pkg in critical_packages])
+    available_packages = sum(1 for status in package_status.values() if status['available'])
+
+    return {
+        "success": True,
+        "data": {
+            "packages": package_status,
+            "summary": {
+                "total_packages": total_packages,
+                "available_packages": available_packages,
+                "missing_packages": total_packages - available_packages,
+                "critical_missing": critical_missing,
+                "total_critical": total_critical,
+                "deployment_ready": critical_missing == 0
+            }
+        },
+        "timestamp": datetime.utcnow().isoformat(),
+        "source": "requirements.txt"
+    }
+
+
+@router.get("/jobs/{job_id}/progress", response_model=JobProgressDetail)
+async def get_job_progress(job_id: str):
+    """
+    Get detailed progress information for a specific job.
+
+    This endpoint provides real-time progress tracking including:
+    - Current processing stage
+    - Page-by-page status
+    - OCR progress and confidence scores
+    - Database integration status
+    - Error and warning details
+    - Performance metrics
+    """
+    try:
+        progress_service = get_progress_service()
+        tracker = progress_service.get_tracker(job_id)
+
+        if not tracker:
+            raise HTTPException(
+                status_code=404,
+                detail=f"Progress tracking not found for job {job_id}"
+            )
+
+        return tracker.to_progress_detail()
+
+    except Exception as e:
+        logger.error(f"Error getting job progress for {job_id}: {str(e)}")
+        raise HTTPException(
+            status_code=500,
+            detail=f"Failed to get job progress: {str(e)}"
+        )
+
+
+@router.get("/jobs/progress/active")
+async def get_all_active_progress():
+    """
+    Get progress information for all currently active jobs.
+
+    Returns a summary of all jobs currently being processed,
+    useful for monitoring multiple concurrent operations.
+    """
+    try:
+        progress_service = get_progress_service()
+        active_trackers = progress_service.get_all_active_trackers()
+
+        progress_summaries = []
+        for job_id, tracker in active_trackers.items():
+            progress_detail = tracker.to_progress_detail()
+            progress_summaries.append({
+                "job_id": job_id,
+                "document_id": tracker.document_id,
+                "current_stage": tracker.current_stage.value,
+                "progress_percentage": progress_detail.progress_percentage,
+                "pages_completed": tracker.pages_completed,
+                "total_pages": tracker.total_pages,
+                "current_page": tracker.current_page,
+                "errors_count": len(tracker.errors),
+                "warnings_count": len(tracker.warnings),
+                "estimated_completion": progress_detail.estimated_completion_time
+            })
+
+        return {
+            "success": True,
+            "message": f"Retrieved progress for {len(active_trackers)} active jobs",
+            "data": {
+                "active_jobs_count": len(active_trackers),
+                "jobs": progress_summaries
+            },
+            "timestamp": datetime.utcnow().isoformat()
+        }
+
+    except Exception as e:
+        logger.error(f"Error getting active job progress: {str(e)}")
+        raise HTTPException(
+            status_code=500,
+            detail=f"Failed to get active job progress: {str(e)}"
+        )
+
+
+@router.get("/jobs/{job_id}/progress/pages")
+async def get_job_page_progress(job_id: str):
+    """
+    Get detailed page-by-page progress for a specific job.
+
+    This endpoint provides granular information about each page:
+    - Processing status (pending, processing, success, failed, skipped)
+    - Text and image extraction results
+    - OCR application and confidence scores
+    - Processing time per page
+    - Error messages for failed pages
+    - Database save status
+    """
+    try:
+        progress_service = get_progress_service()
+        tracker = progress_service.get_tracker(job_id)
+
+        if not tracker:
+            raise HTTPException(
+                status_code=404,
+                detail=f"Progress tracking not found for job {job_id}"
+            )
+
+        # Group pages by status for easier analysis
+        pages_by_status = {
+            "pending": [],
+            "processing": [],
+            "success": [],
+            "failed": [],
+            "skipped": []
+        }
+
+        for page_status in tracker.page_statuses.values():
+            status_key = page_status.status
+            if status_key in pages_by_status:
+                pages_by_status[status_key].append(page_status.model_dump())
+
+        return {
+            "success": True,
+            "message": f"Retrieved page progress for job {job_id}",
+            "data": {
+                "job_id": job_id,
+                "total_pages": tracker.total_pages,
+                "summary": {
+                    "pending": len(pages_by_status["pending"]),
+                    "processing": len(pages_by_status["processing"]),
+                    "success": len(pages_by_status["success"]),
+                    "failed": len(pages_by_status["failed"]),
+                    "skipped": len(pages_by_status["skipped"])
+                },
+                "pages_by_status": pages_by_status,
+                "current_page": tracker.current_page,
+                "current_stage": tracker.current_stage.value
+            },
+            "timestamp": datetime.utcnow().isoformat()
+        }
+
+    except Exception as e:
+        logger.error(f"Error getting page progress for {job_id}: {str(e)}")
+        raise HTTPException(
+            status_code=500,
+            detail=f"Failed to get page progress: {str(e)}"
+        )
+
+
+@router.get("/jobs/{job_id}/progress/stream")
+async def stream_job_progress(job_id: str):
+    """
+    Stream real-time progress updates for a job.
+
+    This endpoint provides server-sent events (SSE) for real-time monitoring.
+    Useful for frontend applications that need live progress updates.
+    """
+    try:
+        from fastapi.responses import StreamingResponse
+        import json
+
+        progress_service = get_progress_service()
+
+        async def generate_progress_stream():
+            """Generate progress updates as server-sent events."""
+            while True:
+                tracker = progress_service.get_tracker(job_id)
+
+                if not tracker:
+                    # Job not found or completed
+                    yield f"data: {json.dumps({'error': 'Job not found or completed'})}\n\n"
+                    break
+
+                progress_detail = tracker.to_progress_detail()
+                progress_data = progress_detail.model_dump()
+
+                yield f"data: {json.dumps(progress_data)}\n\n"
+
+                # Check if job is completed
+                if tracker.current_stage.value in ["completed", "failed"]:
+                    break
+
+                # Wait before next update
+                await asyncio.sleep(2)  # Update every 2 seconds
+
+        return StreamingResponse(
+            generate_progress_stream(),
+            media_type="text/plain",
+            headers={
+                "Cache-Control": "no-cache",
+                "Connection": "keep-alive",
+                "Content-Type": "text/event-stream"
+            }
+        )
+
+    except Exception as e:
+        logger.error(f"Error streaming progress for {job_id}: {str(e)}")
+        raise HTTPException(
+            status_code=500,
+            detail=f"Failed to stream progress: {str(e)}"
+        )
+
+
+@router.post("/test-product-creation")
+async def test_product_creation(
+    document_id: str,
+    workspace_id: str = "ffafc28b-1b8b-4b0d-b226-9f9a6154004e"
+):
+    """
+    ‚úÖ NEW: Test endpoint for enhanced product creation.
+    Tests the improved product detection with no limits and better filtering.
+    """
+    try:
+        logger.info(f"üß™ Testing enhanced product creation for document: {document_id}")
+
+        # Initialize product creation service
+        supabase_client = SupabaseClient()
+        product_service = ProductCreationService(supabase_client)
+
+        # Test the enhanced product creation
+        result = await product_service.create_products_from_layout_candidates(
+            document_id=document_id,
+            workspace_id=workspace_id,
+            min_confidence=0.5,
+            min_quality_score=0.5
+        )
+
+        logger.info(f"‚úÖ Product creation test completed: {result}")
+
+        return {
+            "success": True,
+            "document_id": document_id,
+            "result": result,
+            "message": "Enhanced product creation test completed"
+        }
+
+    except Exception as e:
+        logger.error(f"‚ùå Product creation test failed: {str(e)}")
+        raise HTTPException(
+            status_code=500,
+            detail=f"Product creation test failed: {str(e)}"
+        )
+
+# ============================================================================
+# PHASE 4: MANUAL OCR REPROCESSING ENDPOINT
+# ============================================================================
+
+@router.post("/admin/images/{image_id}/process-ocr")
+async def reprocess_image_ocr(
+    image_id: str,
+    workspace_context: WorkspaceContext = Depends(get_workspace_context),
+    current_user: User = Depends(require_admin)
+):
+    """
+    Manually reprocess a single image with OCR and update all related entities.
+    
+    This endpoint is used when an image was skipped during initial processing
+    but the admin determines it should have OCR applied.
+    
+    Process:
+    1. Run full EasyOCR on the image
+    2. Update image.ocr_extracted_text and ocr_confidence_score
+    3. Update related chunks with new OCR text
+    4. Regenerate text embeddings for updated chunks
+    5. Update product associations based on new OCR text
+    6. Update metadata relationships
+    
+    Args:
+        image_id: UUID of the image to reprocess
+        workspace_context: Current workspace context
+        current_user: Current admin user
+        
+    Returns:
+        Comprehensive results of the reprocessing operation
+    """
+    try:
+        from ..services.ocr_service import get_ocr_service, OCRConfig
+        from ..services.real_embeddings_service import get_embeddings_service
+        
+        logger.info(f"üîÑ Admin OCR reprocessing requested for image: {image_id}")
+        
+        # Initialize services
+        supabase = SupabaseClient()
+        ocr_service = get_ocr_service(OCRConfig(languages=['en']))
+        embeddings_service = get_embeddings_service()
+        
+        # Step 1: Get image from database
+        image_response = supabase.client.table('document_images').select('*').eq('id', image_id).execute()
+        
+        if not image_response.data or len(image_response.data) == 0:
+            raise HTTPException(status_code=404, detail=f"Image not found: {image_id}")
+        
+        image = image_response.data[0]
+        image_url = image.get('image_url')
+        
+        if not image_url:
+            raise HTTPException(status_code=400, detail="Image has no URL")
+        
+        logger.info(f"üì∑ Processing image: {image_url}")
+        
+        # Step 2: Download image temporarily
+        import httpx
+        import tempfile
+        
+        async with httpx.AsyncClient(timeout=30.0) as client:
+            response = await client.get(image_url)
+            if response.status_code != 200:
+                raise HTTPException(status_code=400, detail=f"Failed to download image: {response.status_code}")
+            
+            # Save to temporary file
+            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
+                tmp_file.write(response.content)
+                tmp_image_path = tmp_file.name
+        
+        try:
+            # Step 3: Run full EasyOCR
+            logger.info("üîç Running EasyOCR...")
+            ocr_results = ocr_service.extract_text_from_image(tmp_image_path)
+            
+            extracted_text = ' '.join([r.text for r in ocr_results])
+            avg_confidence = sum([r.confidence for r in ocr_results]) / len(ocr_results) if ocr_results else 0.0
+            
+            logger.info(f"‚úÖ OCR extracted {len(ocr_results)} text regions, avg confidence: {avg_confidence:.2f}")
+            
+            # Step 4: Update image record
+            update_data = {
+                'ocr_extracted_text': extracted_text,
+                'ocr_confidence_score': avg_confidence,
+                'processing_status': 'ocr_complete',
+                'metadata': {
+                    **(image.get('metadata') or {}),
+                    'ocr_metadata': {
+                        'text_regions_count': len(ocr_results),
+                        'reprocessed_at': datetime.utcnow().isoformat(),
+                        'reprocessed_by': current_user.id,
+                        'can_reprocess': False
+                    }
+                }
+            }
+            
+            supabase.client.table('document_images').update(update_data).eq('id', image_id).execute()
+            logger.info("‚úÖ Updated image record")
+            
+            # Step 5: Update related chunks
+            chunks_response = supabase.client.table('document_chunks').select('*').eq('id', image.get('chunk_id')).execute()
+            
+            chunks_updated = 0
+            embeddings_regenerated = 0
+            
+            if chunks_response.data:
+                for chunk in chunks_response.data:
+                    # Update chunk content with OCR text
+                    updated_content = f"{chunk.get('content', '')}\n\n[Image OCR: {extracted_text}]"
+                    
+                    # Generate new text embedding
+                    try:
+                        embedding = await embeddings_service.generate_text_embedding(
+                            text=updated_content,
+                            model="text-embedding-3-small"
+                        )
+                        
+                        # Update chunk
+                        supabase.client.table('document_chunks').update({
+                            'content': updated_content,
+                            'text_embedding': embedding,
+                            'metadata': {
+                                **(chunk.get('metadata') or {}),
+                                'ocr_enriched': True,
+                                'ocr_enriched_at': datetime.utcnow().isoformat()
+                            }
+                        }).eq('id', chunk['id']).execute()
+                        
+                        chunks_updated += 1
+                        embeddings_regenerated += 1
+                        
+                    except Exception as e:
+                        logger.error(f"Failed to update chunk {chunk['id']}: {str(e)}")
+            
+            logger.info(f"‚úÖ Updated {chunks_updated} chunks, regenerated {embeddings_regenerated} embeddings")
+            
+            # Step 6: Extract metadata from OCR text
+            metadata_extracted = {
+                'has_dimensions': any(word in extracted_text.lower() for word in ['mm', 'cm', 'inch', 'x', '√ó']),
+                'has_specifications': any(word in extracted_text.lower() for word in ['spec', 'material', 'finish', 'color']),
+                'has_product_codes': any(char.isdigit() for char in extracted_text),
+                'text_length': len(extracted_text)
+            }
+            
+            # Step 7: Find potential product associations
+            # (This is a simplified version - you can enhance with more sophisticated matching)
+            products_associated = 0
+            if extracted_text:
+                # Search for products that might match the OCR text
+                # This is a placeholder - implement your product matching logic
+                pass
+            
+            return {
+                'success': True,
+                'image_id': image_id,
+                'ocr_results': {
+                    'text': extracted_text,
+                    'confidence': avg_confidence,
+                    'text_regions_count': len(ocr_results)
+                },
+                'updates': {
+                    'chunks_updated': chunks_updated,
+                    'embeddings_regenerated': embeddings_regenerated,
+                    'products_associated': products_associated
+                },
+                'metadata_extracted': metadata_extracted,
+                'message': f'Successfully reprocessed image with OCR. Extracted {len(ocr_results)} text regions.'
+            }
+            
+        finally:
+            # Clean up temporary file
+            if os.path.exists(tmp_image_path):
+                os.unlink(tmp_image_path)
+        
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Error reprocessing image OCR: {str(e)}")
+        raise HTTPException(status_code=500, detail=f"Failed to reprocess image: {str(e)}")
+
diff --git a/app/services/pdf_processor.py b/app/services/pdf_processor.py
index e69de29..9be7ef3 100644
--- a/app/services/pdf_processor.py
+++ b/app/services/pdf_processor.py
@@ -0,0 +1,1910 @@
+"""
+PDF Processing Service - Integration with existing PyMuPDF4LLM functionality
+
+This service wraps the existing extractor.py functionality to work with the 
+production FastAPI application structure, providing async interfaces and 
+proper error handling while leveraging the proven PDF extraction code.
+"""
+
+import asyncio
+import base64
+import inspect
+import logging
+import os
+import tempfile
+import time
+import uuid
+from concurrent.futures import ThreadPoolExecutor
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Callable, Dict, List, Optional, Tuple
+
+import aiofiles
+import httpx
+from dataclasses import dataclass
+
+# Import image processing libraries (headless OpenCV)
+try:
+    import cv2
+    CV2_AVAILABLE = True
+    logging.info("OpenCV (headless) loaded successfully")
+except ImportError as e:
+    logging.error(f"OpenCV (headless) not available: {e}. Image processing features will be limited.")
+    CV2_AVAILABLE = False
+    cv2 = None
+
+import numpy as np
+from PIL import Image, ImageEnhance, ImageFilter
+from PIL.ExifTags import TAGS
+import imageio
+
+try:
+    from skimage import filters, morphology, measure
+    SKIMAGE_AVAILABLE = True
+except ImportError as e:
+    logging.warning(f"scikit-image not available: {e}. Some advanced image processing features will be disabled.")
+    SKIMAGE_AVAILABLE = False
+from scipy import ndimage
+
+# Import existing extraction functions
+try:
+    # Try to import from the proper location first
+    from ..core.extractor import extract_pdf_to_markdown, extract_pdf_tables, extract_json_and_images
+except ImportError:
+    # Fall back to the root level extractor if it exists
+    import sys
+    import os
+    root_path = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
+    if root_path not in sys.path:
+        sys.path.append(root_path)
+    try:
+        from extractor import extract_pdf_to_markdown, extract_pdf_tables, extract_json_and_images
+    except ImportError as e:
+        # Log the error and provide a fallback
+        import logging
+        logger = logging.getLogger(__name__)
+        logger.error(f"Failed to import extractor functions: {e}")
+        # Define placeholder functions that will raise NotImplementedError
+        def extract_pdf_to_markdown(*args, **kwargs):
+            raise NotImplementedError("PDF extraction functions not available")
+        def extract_pdf_tables(*args, **kwargs):
+            raise NotImplementedError("PDF table extraction functions not available")
+        def extract_json_and_images(*args, **kwargs):
+            raise NotImplementedError("PDF image extraction functions not available")
+
+# Import OCR service
+from app.services.ocr_service import get_ocr_service, OCRConfig
+
+# Import Supabase client for storage
+from app.services.supabase_client import get_supabase_client
+
+# Import custom exceptions
+from app.utils.exceptions import (
+    PDFProcessingError,
+    PDFValidationError,
+    PDFExtractionError,
+    PDFDownloadError,
+    PDFSizeError,
+    PDFTimeoutError,
+    PDFStorageError,
+    PDFFormatError
+)
+
+# Import unified chunking service (Step 6)
+from app.services.unified_chunking_service import UnifiedChunkingService, ChunkingConfig, ChunkingStrategy
+
+
+@dataclass
+class PDFProcessingResult:
+    """Result of PDF processing operation"""
+    document_id: str
+    markdown_content: str
+    extracted_images: List[Dict[str, Any]]
+    ocr_text: str  # Combined OCR text from all images
+    ocr_results: List[Dict[str, Any]]  # Detailed OCR results per image
+    metadata: Dict[str, Any]
+    processing_time: float
+    page_count: int
+    word_count: int
+    character_count: int
+    multimodal_enabled: bool = False
+
+
+class PDFProcessor:
+    """
+    Core PDF processing service using existing PyMuPDF4LLM functionality.
+    
+    This class provides async interfaces to the existing extractor.py functions,
+    adding proper error handling, logging, and integration with the FastAPI app.
+    """
+    
+    def __init__(self, config: Optional[Dict[str, Any]] = None):
+        """Initialize the PDF processor with configuration."""
+        self.config = config or {}
+        self.logger = logging.getLogger(__name__)
+
+        # Initialize unified chunking service (Step 6)
+        chunking_config = ChunkingConfig(
+            strategy=ChunkingStrategy.HYBRID,  # Use hybrid strategy by default
+            max_chunk_size=1000,
+            min_chunk_size=100,
+            overlap_size=100,
+            preserve_structure=True,
+            split_on_sentences=True,
+            split_on_paragraphs=True,
+            respect_hierarchy=True
+        )
+        self.chunking_service = UnifiedChunkingService(chunking_config)
+        
+        # Default processing options
+        self.default_timeout = self.config.get('timeout_seconds', 1800)  # 30 minutes for large PDFs with OCR
+        self.max_file_size = self.config.get('max_file_size_mb', 50) * 1024 * 1024  # Convert to bytes
+        self.temp_dir_base = self.config.get('temp_dir', tempfile.gettempdir())
+        
+        # Initialize thread pool executor for async processing
+        max_workers = self.config.get('max_workers', 4)
+        self.executor = ThreadPoolExecutor(max_workers=max_workers)
+        
+        self.logger.info("PDFProcessor initialized with config: %s", self.config)
+    
+    def __del__(self):
+        """Cleanup resources when the processor is destroyed."""
+        if hasattr(self, 'executor') and self.executor:
+            self.executor.shutdown(wait=True)
+            self.logger.debug("ThreadPoolExecutor shutdown completed")
+
+    async def health_check(self) -> Dict[str, Any]:
+        """
+        Perform health check for PDF processing capabilities.
+
+        Returns:
+            Dict with health status and details
+        """
+        try:
+            # Check if required dependencies are available
+            dependencies = {
+                "pymupdf": True,  # Should be available if we got this far
+                "opencv": CV2_AVAILABLE,
+                "skimage": SKIMAGE_AVAILABLE,
+                "pil": True,  # PIL is imported successfully
+                "numpy": True,  # numpy is imported successfully
+            }
+
+            # Check if executor is available
+            executor_healthy = hasattr(self, 'executor') and self.executor is not None
+
+            # Check temp directory access
+            temp_dir_accessible = os.access(self.temp_dir_base, os.W_OK)
+
+            # Overall health status
+            all_critical_deps = dependencies["pymupdf"] and dependencies["pil"] and dependencies["numpy"]
+            overall_healthy = all_critical_deps and executor_healthy and temp_dir_accessible
+
+            return {
+                "status": "healthy" if overall_healthy else "degraded",
+                "dependencies": dependencies,
+                "executor_available": executor_healthy,
+                "temp_directory_accessible": temp_dir_accessible,
+                "max_file_size_mb": self.max_file_size // (1024 * 1024),
+                "timeout_seconds": self.default_timeout,
+                "max_workers": getattr(self.executor, '_max_workers', 'unknown') if executor_healthy else 0,
+                "timestamp": datetime.utcnow().isoformat()
+            }
+
+        except Exception as e:
+            self.logger.error(f"Health check failed: {e}")
+            return {
+                "status": "unhealthy",
+                "error": str(e),
+                "timestamp": datetime.utcnow().isoformat()
+            }
+
+    async def process_pdf_from_bytes(
+        self,
+        pdf_bytes: bytes,
+        document_id: Optional[str] = None,
+        processing_options: Optional[Dict[str, Any]] = None,
+        progress_callback: Optional[callable] = None
+    ) -> PDFProcessingResult:
+        """
+        Process PDF from bytes and return markdown + images.
+        
+        Args:
+            pdf_bytes: Raw PDF file bytes
+            document_id: Optional document identifier
+            processing_options: Processing configuration options
+            
+        Returns:
+            PDFProcessingResult with extracted content and metadata
+            
+        Raises:
+            PDFProcessingError: If processing fails
+            ProcessingTimeoutError: If processing exceeds timeout
+        """
+        start_time = time.time()
+        document_id = document_id or str(uuid.uuid4())
+        processing_options = processing_options or {}
+        
+        self.logger.info("Starting PDF processing for document %s", document_id)
+        
+        # Validate file size
+        if len(pdf_bytes) > self.max_file_size:
+            raise PDFSizeError(f"PDF file too large: {len(pdf_bytes)} bytes (max: {self.max_file_size})")
+        
+        temp_dir = None
+        try:
+            # Create temporary directory for processing
+            temp_dir = self._create_temp_directory(document_id)
+            self.logger.info(f"üìÅ Created temp directory: {temp_dir}")
+            
+            # Save PDF bytes to temporary file
+            temp_pdf_path = os.path.join(temp_dir, f"{document_id}.pdf")
+            self.logger.info(f"üíæ Saving PDF to: {temp_pdf_path}")
+            async with aiofiles.open(temp_pdf_path, 'wb') as f:
+                await f.write(pdf_bytes)
+            self.logger.info(f"‚úÖ PDF saved successfully, file size: {os.path.getsize(temp_pdf_path)} bytes")
+            
+            # Process with timeout
+            timeout = processing_options.get('timeout_seconds', self.default_timeout)
+            
+            try:
+                result = await asyncio.wait_for(
+                    self._process_pdf_file(temp_pdf_path, document_id, processing_options, progress_callback),
+                    timeout=timeout
+                )
+                
+                processing_time = time.time() - start_time
+                result.processing_time = processing_time
+                
+                self.logger.info(
+                    "PDF processing completed for document %s in %.2f seconds", 
+                    document_id, processing_time
+                )
+                
+                return result
+                
+            except asyncio.TimeoutError:
+                raise PDFTimeoutError(f"PDF processing timed out after {timeout} seconds")
+                
+        except Exception as e:
+            self.logger.error("PDF processing failed for document %s: %s", document_id, str(e))
+            if isinstance(e, (PDFProcessingError, PDFTimeoutError)):
+                raise
+            raise PDFProcessingError(f"Unexpected error during PDF processing: {str(e)}") from e
+            
+        finally:
+            # Cleanup temporary files
+            if temp_dir:
+                self._cleanup_temp_files(temp_dir)
+    
+    async def process_pdf_from_url(
+        self,
+        pdf_url: str,
+        document_id: Optional[str] = None,
+        processing_options: Optional[Dict[str, Any]] = None,
+        progress_callback: Optional[callable] = None
+    ) -> PDFProcessingResult:
+        """
+        Process PDF from URL and return markdown + images.
+        
+        Args:
+            pdf_url: URL to PDF file
+            document_id: Optional document identifier
+            processing_options: Processing configuration options
+            
+        Returns:
+            PDFProcessingResult with extracted content and metadata
+            
+        Raises:
+            PDFDownloadError: If PDF download fails
+            PDFProcessingError: If processing fails
+        """
+        document_id = document_id or str(uuid.uuid4())
+        self.logger.info("Downloading PDF from URL for document %s: %s", document_id, pdf_url)
+        
+        try:
+            # Download PDF with timeout
+            timeout = processing_options.get('download_timeout', 30) if processing_options else 30
+            
+            async with httpx.AsyncClient(timeout=timeout) as client:
+                response = await client.get(pdf_url)
+                response.raise_for_status()
+                
+                # Validate content type
+                content_type = response.headers.get('content-type', '').lower()
+                if 'application/pdf' not in content_type and not pdf_url.lower().endswith('.pdf'):
+                    self.logger.warning("Unexpected content type for PDF: %s", content_type)
+                
+                pdf_bytes = response.content
+                
+        except httpx.HTTPError as e:
+            raise PDFDownloadError(f"Failed to download PDF from {pdf_url}: {str(e)}") from e
+        except Exception as e:
+            raise PDFDownloadError(f"Unexpected error downloading PDF: {str(e)}") from e
+        
+        # Process the downloaded PDF
+        return await self.process_pdf_from_bytes(pdf_bytes, document_id, processing_options, progress_callback)
+    
+    async def _process_pdf_file(
+        self,
+        pdf_path: str,
+        document_id: str,
+        processing_options: Dict[str, Any],
+        progress_callback: Optional[callable] = None
+    ) -> PDFProcessingResult:
+        """
+        Internal method to process PDF file using existing extractor functions.
+
+        This method runs the existing synchronous extractor functions in a thread pool
+        to maintain async compatibility.
+        """
+        loop = asyncio.get_event_loop()
+
+        try:
+            # Extract markdown content using existing function
+            markdown_content, metadata = await loop.run_in_executor(
+                None,
+                self._extract_markdown_sync,
+                pdf_path,
+                processing_options,
+                progress_callback
+            )
+            
+            # Extract images if requested
+            extracted_images = []
+            if processing_options.get('extract_images', True):
+                extracted_images = await self._extract_images_async(
+                    pdf_path,
+                    document_id,
+                    processing_options,
+                    progress_callback
+                )
+            
+            # Calculate content metrics
+            content_metrics = self._calculate_content_metrics(markdown_content)
+            
+            # Initialize OCR results
+            ocr_text = ""
+            ocr_results = []
+
+            # Intelligent multimodal detection
+            manual_multimodal = processing_options.get('enable_multimodal', None)
+            if manual_multimodal is not None:
+                # User explicitly set multimodal preference
+                multimodal_enabled = manual_multimodal
+                multimodal_reason = "manual_override"
+            else:
+                # Auto-detect if multimodal processing would be beneficial
+                multimodal_enabled = self._should_use_multimodal(extracted_images, markdown_content)
+                multimodal_reason = "auto_detected"
+
+            ocr_languages = processing_options.get('ocr_languages', ['en'])  # Define outside conditional
+
+            self.logger.info(f"üîç Multimodal: {'enabled' if multimodal_enabled else 'disabled'} "
+                           f"({multimodal_reason}, {len(extracted_images)} images available)")
+
+            # Process images with OCR if multimodal is enabled
+            if multimodal_enabled and extracted_images:
+                self.logger.info(f"Processing {len(extracted_images)} images with OCR")
+                ocr_text, ocr_results = await self._process_images_with_ocr(
+                    extracted_images, ocr_languages, progress_callback
+                )
+                
+                # Enhance extracted images with OCR data
+                for i, image_data in enumerate(extracted_images):
+                    if i < len(ocr_results):
+                        image_data['ocr_result'] = ocr_results[i]
+            
+            # Update content metrics to include OCR text
+            if ocr_text:
+                ocr_word_count = len(ocr_text.split())
+                content_metrics['word_count'] += ocr_word_count
+                content_metrics['character_count'] += len(ocr_text)
+
+            return PDFProcessingResult(
+                document_id=document_id,
+                markdown_content=markdown_content,
+                extracted_images=extracted_images,
+                ocr_text=ocr_text,
+                ocr_results=ocr_results,
+                metadata={
+                    **metadata,
+                    **content_metrics,
+                    'processing_options': processing_options,
+                    'timestamp': datetime.utcnow().isoformat(),
+                    'multimodal_enabled': multimodal_enabled,
+                    'ocr_enabled': multimodal_enabled and bool(extracted_images),
+                    'ocr_languages': ocr_languages if multimodal_enabled else [],
+                    'ocr_text_length': len(ocr_text) if ocr_text else 0
+                },
+                processing_time=0.0,  # Will be set by caller
+                page_count=metadata.get('page_count', 0),
+                word_count=content_metrics['word_count'],
+                character_count=content_metrics['character_count'],
+                multimodal_enabled=multimodal_enabled
+            )
+            
+        except Exception as e:
+            self.logger.error("Error processing PDF file %s: %s", pdf_path, str(e))
+            raise PDFExtractionError(f"Failed to parse PDF content: {str(e)}") from e
+    
+    def _extract_markdown_sync(
+        self,
+        pdf_path: str,
+        processing_options: Dict[str, Any],
+        progress_callback: Optional[callable] = None
+    ) -> Tuple[str, Dict[str, Any]]:
+        """
+        Enhanced markdown extraction with intelligent OCR-first approach for image-based PDFs.
+
+        Features:
+        - Smart detection of image-based vs text-based PDFs
+        - OCR-first approach for image-heavy PDFs (like WIFI MOMO lookbook)
+        - PyMuPDF4LLM fallback for text-based PDFs
+        - Advanced text processing and cleaning
+        - Metadata extraction and analysis
+        """
+        try:
+            # First, analyze the PDF to determine if it's image-based
+            import fitz
+            doc = fitz.open(pdf_path)
+            total_pages = len(doc)
+
+            # Sample pages strategically to determine PDF type
+            # Sample first 3, middle 2, and last 2 pages for better detection
+            pages_to_sample = []
+            
+            # Always sample first 3 pages
+            pages_to_sample.extend(range(min(3, total_pages)))
+            
+            # Sample middle pages if document is long enough
+            if total_pages > 5:
+                mid_page = total_pages // 2
+                pages_to_sample.extend([mid_page - 1, mid_page])
+            
+            # Sample last pages if document is long enough
+            if total_pages > 3:
+                pages_to_sample.extend([total_pages - 2, total_pages - 1])
+            
+            # Remove duplicates and sort
+            pages_to_sample = sorted(set(pages_to_sample))
+            
+            total_text_chars = 0
+            total_images = 0
+
+            for page_num in pages_to_sample:
+                page = doc[page_num]
+                text = page.get_text()
+                images = page.get_images()
+                total_text_chars += len(text.strip())
+                total_images += len(images)
+
+            doc.close()
+
+            # Enhanced PDF type detection with multiple criteria
+            avg_text_per_page = total_text_chars / len(pages_to_sample)
+            avg_images_per_page = total_images / len(pages_to_sample)
+
+            # Multiple detection criteria for better accuracy
+            criteria = {
+                'low_text': avg_text_per_page < 50,
+                'very_low_text': avg_text_per_page < 10,  # More restrictive threshold
+                'has_images': avg_images_per_page >= 1,
+                'many_images': avg_images_per_page >= 3,
+                'text_to_image_ratio': (avg_text_per_page / max(avg_images_per_page, 1)) < 30,
+                'no_images': avg_images_per_page == 0
+            }
+
+            # Smart detection logic - prioritize text-first for text-only PDFs
+            is_image_based = (
+                (criteria['very_low_text'] and criteria['has_images']) or  # Very little text + images ‚Üí OCR
+                (criteria['low_text'] and criteria['many_images']) or  # Low text + many images ‚Üí OCR
+                (criteria['many_images'] and criteria['text_to_image_ratio'])  # Many images + low text ratio ‚Üí OCR
+            ) and not criteria['no_images']  # Never use OCR for PDFs with no images
+
+            # Enhanced logging with detection criteria
+            detection_reason = []
+            if criteria['very_low_text'] and criteria['has_images']:
+                detection_reason.append("very_low_text_with_images")
+            if criteria['low_text'] and criteria['many_images']:
+                detection_reason.append("low_text_many_images")
+            if criteria['many_images'] and criteria['text_to_image_ratio']:
+                detection_reason.append("many_images_low_ratio")
+            if criteria['no_images']:
+                detection_reason.append("text_only_pdf")
+
+            self.logger.info(f"üìä PDF Analysis: {total_pages} pages, {avg_text_per_page:.1f} chars/page, "
+                           f"{avg_images_per_page:.1f} images/page")
+            self.logger.info(f"üéØ Detection: {'OCR-first' if is_image_based else 'Text-first'} "
+                           f"(reason: {', '.join(detection_reason) if detection_reason else 'text_dominant'})")
+
+            if is_image_based:
+                # Use OCR-first approach for image-based PDFs
+                self.logger.info("Detected image-based PDF, using OCR-first extraction")
+                try:
+                    markdown_content = self._extract_text_with_ocr(pdf_path, processing_options, progress_callback)
+                    if len(markdown_content.strip()) < 100:
+                        # If OCR also fails, try PyMuPDF4LLM as fallback
+                        self.logger.info("OCR yielded minimal content, trying PyMuPDF4LLM fallback")
+                        page_number = processing_options.get('page_number')
+                        fallback_content = extract_pdf_to_markdown(pdf_path, page_number)
+                        if len(fallback_content.strip()) > len(markdown_content.strip()):
+                            markdown_content = fallback_content
+                except Exception as ocr_error:
+                    self.logger.error(f"OCR extraction failed: {ocr_error}, trying PyMuPDF4LLM fallback")
+                    page_number = processing_options.get('page_number')
+                    markdown_content = extract_pdf_to_markdown(pdf_path, page_number)
+            else:
+                # Use PyMuPDF4LLM first for text-based PDFs
+                self.logger.info("Detected text-based PDF, using PyMuPDF4LLM extraction")
+
+                # Update progress: Starting text extraction (30%)
+                if progress_callback:
+                    try:
+                        # Only call if it's not a coroutine (sync callbacks only in sync function)
+                        if not inspect.iscoroutinefunction(progress_callback):
+                            progress_callback(
+                                progress=30,
+                                details={
+                                    "current_step": "Extracting text from PDF using PyMuPDF4LLM",
+                                    "total_pages": total_pages,
+                                    "extraction_method": "pymupdf4llm"
+                                }
+                            )
+                    except Exception as callback_error:
+                        self.logger.warning(f"Progress callback failed: {callback_error}")
+
+                page_number = processing_options.get('page_number')
+                markdown_content = extract_pdf_to_markdown(pdf_path, page_number)
+
+                # Check if we got meaningful text content
+                clean_content = markdown_content.replace('-', '').replace('\n', '').strip()
+
+                # If we only got page separators or very little content, try OCR
+                if len(clean_content) < 100:  # Less than 100 chars of actual content
+                    self.logger.info("Standard extraction yielded minimal text, attempting OCR extraction")
+                    try:
+                        ocr_content = self._extract_text_with_ocr(pdf_path, processing_options, progress_callback)
+
+                        if len(ocr_content.strip()) > len(clean_content):
+                            self.logger.info(f"OCR extraction successful: {len(ocr_content)} characters vs {len(clean_content)} from standard")
+                            markdown_content = ocr_content
+                        else:
+                            self.logger.info("OCR did not improve text extraction, using standard result")
+                    except Exception as ocr_error:
+                        self.logger.warning(f"OCR extraction failed: {ocr_error}, using standard result")
+
+            # Get basic metadata (page count, etc.)
+            import fitz  # PyMuPDF
+            doc = fitz.open(pdf_path)
+
+            # Helper function to parse PDF dates
+            def parse_pdf_date(date_str):
+                """Parse PDF date string to datetime or return None."""
+                if not date_str or date_str.strip() == '':
+                    return None
+                try:
+                    # PDF dates are often in format: D:YYYYMMDDHHmmSSOHH'mm'
+                    if date_str.startswith('D:'):
+                        date_str = date_str[2:]
+                    # Extract just the date part (YYYYMMDDHHMMSS)
+                    if len(date_str) >= 14:
+                        from datetime import datetime
+                        return datetime.strptime(date_str[:14], '%Y%m%d%H%M%S')
+                    elif len(date_str) >= 8:
+                        from datetime import datetime
+                        return datetime.strptime(date_str[:8], '%Y%m%d')
+                except:
+                    pass
+                return None
+
+            metadata = {
+                'page_count': doc.page_count,
+                'title': doc.metadata.get('title', '') or None,
+                'author': doc.metadata.get('author', '') or None,
+                'subject': doc.metadata.get('subject', '') or None,
+                'creator': doc.metadata.get('creator', '') or None,
+                'producer': doc.metadata.get('producer', '') or None,
+                'creation_date': parse_pdf_date(doc.metadata.get('creationDate', '')),
+                'modification_date': parse_pdf_date(doc.metadata.get('modDate', ''))
+            }
+            doc.close()
+
+            # Update progress: Text extraction complete (50%)
+            if progress_callback:
+                try:
+                    # Only call if it's not a coroutine (sync callbacks only in sync function)
+                    if not inspect.iscoroutinefunction(progress_callback):
+                        progress_callback(
+                            progress=50,
+                            details={
+                                "current_step": "Text extraction complete, preparing for chunking",
+                                "total_pages": total_pages,
+                                "text_length": len(markdown_content),
+                                "extraction_method": "pymupdf4llm"
+                            }
+                        )
+                except Exception as callback_error:
+                    self.logger.warning(f"Progress callback failed: {callback_error}")
+
+            # Explicit memory cleanup after extraction
+            import gc
+            gc.collect()
+
+            return markdown_content, metadata
+
+        except Exception as e:
+            # Cleanup on error
+            import gc
+            gc.collect()
+            raise PDFExtractionError(f"Markdown extraction failed: {str(e)}") from e
+
+    def _extract_text_with_ocr(
+        self,
+        pdf_path: str,
+        processing_options: Dict[str, Any],
+        progress_callback: Optional[callable] = None
+    ) -> str:
+        """
+        Extract text from PDF using OCR for text-as-images PDFs.
+
+        This method renders PDF pages as images and uses OCR to extract text.
+        Useful for PDFs where text is embedded as images or paths.
+        """
+        try:
+            import fitz  # PyMuPDF
+            from app.services.ocr_service import get_ocr_service, OCRConfig
+
+            # Initialize OCR service
+            ocr_languages = processing_options.get('ocr_languages', ['en'])
+            ocr_config = OCRConfig(
+                languages=ocr_languages,
+                confidence_threshold=0.3,  # Lower threshold for better recall
+                preprocessing_enabled=True,
+                fallback_to_tesseract=True
+            )
+            ocr_service = get_ocr_service(ocr_config)
+
+            doc = fitz.open(pdf_path)
+            all_text = []
+
+            # Process specific page or all pages
+            page_number = processing_options.get('page_number')
+            if page_number is not None:
+                page_range = [page_number - 1] if page_number > 0 else [0]
+            else:
+                # Process all pages for complete extraction
+                page_range = range(len(doc))
+
+            self.logger.info(f"Processing {len(page_range)} pages with OCR")
+
+            for i, page_num in enumerate(page_range):
+                if page_num < len(doc):
+                    page = doc.load_page(page_num)
+
+                    # Render page as high-quality image
+                    mat = fitz.Matrix(2.5, 2.5)  # 2.5x zoom for good OCR quality
+                    pix = page.get_pixmap(matrix=mat)
+                    img_data = pix.tobytes('png')
+
+                    # Extract text with OCR
+                    try:
+                        from PIL import Image
+                        import io
+
+                        img = Image.open(io.BytesIO(img_data))
+                        ocr_results = ocr_service.extract_text_from_image(img)
+
+                        # Combine all OCR results for this page
+                        page_text = []
+                        for result in ocr_results:
+                            if result.text.strip() and result.confidence > 0.3:
+                                page_text.append(result.text.strip())
+
+                        if page_text:
+                            combined_page_text = ' '.join(page_text)
+                            all_text.append(f"## Page {page_num + 1}\n\n{combined_page_text}\n")
+                            self.logger.debug(f"Page {page_num + 1}: Extracted {len(combined_page_text)} characters")
+
+                    except Exception as page_error:
+                        self.logger.warning(f"OCR failed for page {page_num + 1}: {page_error}")
+                        continue
+
+                # Log progress every 5 pages
+                if (i + 1) % 5 == 0 or (i + 1) == len(page_range):
+                    progress = ((i + 1) / len(page_range)) * 80  # OCR is 80% of total processing
+                    self.logger.info(f"üìÑ OCR Progress: {i + 1}/{len(page_range)} pages ({progress:.1f}%)")
+
+                    # Update job progress if callback provided
+                    if progress_callback:
+                        try:
+                            # Only call if it's not a coroutine (sync callbacks only in sync function)
+                            if not inspect.iscoroutinefunction(progress_callback):
+                                progress_callback(
+                                    progress=int(progress),
+                                    details={
+                                        "current_step": f"OCR processing: {i + 1}/{len(page_range)} pages",
+                                        "pages_processed": i + 1,
+                                        "total_pages": len(page_range),
+                                        "ocr_stage": "extracting_text"
+                                    }
+                                )
+                        except Exception as callback_error:
+                            self.logger.warning(f"Progress callback failed: {callback_error}")
+
+            doc.close()
+
+            # Combine all extracted text
+            final_text = '\n'.join(all_text)
+            self.logger.info(f"OCR extraction complete: {len(final_text)} total characters from {len(page_range)} pages")
+
+            # Update progress for chunk creation
+            if progress_callback:
+                try:
+                    # Only call if it's not a coroutine (sync callbacks only in sync function)
+                    if not inspect.iscoroutinefunction(progress_callback):
+                        progress_callback(
+                            progress=85,
+                            details={
+                                "current_step": "Creating text chunks for RAG pipeline",
+                                "pages_processed": len(page_range),
+                                "total_pages": len(page_range),
+                                "text_length": len(final_text),
+                                "ocr_stage": "creating_chunks"
+                            }
+                        )
+                except Exception as callback_error:
+                    self.logger.warning(f"Progress callback failed: {callback_error}")
+
+            # Explicit memory cleanup after OCR extraction
+            import gc
+            gc.collect()
+
+            return final_text
+
+        except Exception as e:
+            # Cleanup on error
+            import gc
+            gc.collect()
+            self.logger.error(f"OCR text extraction failed: {str(e)}")
+            raise PDFExtractionError(f"OCR text extraction failed: {str(e)}") from e
+
+    def _should_use_multimodal(self, extracted_images: List[Dict], markdown_content: str) -> bool:
+        """
+        Intelligent detection of whether multimodal processing would be beneficial.
+
+        Args:
+            extracted_images: List of extracted images
+            markdown_content: Extracted text content
+
+        Returns:
+            bool: True if multimodal processing is recommended
+        """
+        try:
+            # Criteria for multimodal processing
+            has_images = len(extracted_images) > 0
+            many_images = len(extracted_images) >= 3
+            low_text_content = len(markdown_content.strip()) < 500
+            moderate_text_content = len(markdown_content.strip()) < 2000
+
+            # Decision logic
+            if not has_images:
+                return False  # No images ‚Üí no multimodal needed
+
+            if many_images and low_text_content:
+                return True  # Many images + little text ‚Üí likely visual document
+
+            if has_images and moderate_text_content:
+                return True  # Some images + moderate text ‚Üí could benefit from multimodal
+
+            if len(extracted_images) >= 1 and low_text_content:
+                return True  # Any images + very little text ‚Üí likely needs OCR
+
+            return False  # Text-heavy document ‚Üí multimodal not needed
+
+        except Exception as e:
+            self.logger.warning(f"Multimodal detection failed: {e}, defaulting to False")
+            return False
+
+    async def _extract_images_async(
+        self,
+        pdf_path: str,
+        document_id: str,
+        processing_options: Dict[str, Any],
+        progress_callback: Optional[callable] = None
+    ) -> List[Dict[str, Any]]:
+        """
+        Enhanced async image extraction with Supabase Storage upload.
+
+        Features:
+        - Basic extraction using existing PyMuPDF functionality
+        - Image format conversion and optimization
+        - Advanced metadata extraction (EXIF, dimensions, quality metrics)
+        - Image enhancement and filtering options
+        - Upload to Supabase Storage instead of local storage
+        - Quality assessment and duplicate detection
+        """
+        try:
+            # Create output directory for images
+            output_dir = self._create_temp_directory(f"{document_id}_images")
+
+            # Use existing extractor function for basic extraction (run in executor for sync function)
+            loop = asyncio.get_event_loop()
+            page_number = processing_options.get('page_number')
+
+            await loop.run_in_executor(
+                None,
+                extract_json_and_images,
+                pdf_path,
+                output_dir,
+                page_number
+            )
+
+            # Process extracted images with advanced capabilities
+            images = []
+            image_dir = os.path.join(output_dir, 'images')
+
+            self.logger.info(f"üîç Checking for extracted images in: {image_dir}")
+            self.logger.info(f"   Image directory exists: {os.path.exists(image_dir)}")
+
+            if os.path.exists(image_dir):
+                image_files = os.listdir(image_dir)
+                self.logger.info(f"   Found {len(image_files)} files in image directory")
+
+                # Report progress: Image extraction found images
+                if progress_callback:
+                    try:
+                        import inspect
+                        if inspect.iscoroutinefunction(progress_callback):
+                            await progress_callback(
+                                progress=25,
+                                details={
+                                    "current_step": f"Processing {len(image_files)} extracted images",
+                                    "total_images": len(image_files),
+                                    "images_processed": 0
+                                }
+                            )
+                        else:
+                            progress_callback(
+                                progress=25,
+                                details={
+                                    "current_step": f"Processing {len(image_files)} extracted images",
+                                    "total_images": len(image_files),
+                                    "images_processed": 0
+                                }
+                            )
+                    except Exception as e:
+                        self.logger.warning(f"Progress callback failed: {e}")
+
+                # Process images in batches to reduce memory usage
+                batch_size = 10  # Process 10 images at a time
+                valid_image_files = [f for f in image_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'))]
+
+                for batch_start in range(0, len(valid_image_files), batch_size):
+                    batch_end = min(batch_start + batch_size, len(valid_image_files))
+                    batch_files = valid_image_files[batch_start:batch_end]
+
+                    self.logger.info(f"   Processing batch {batch_start // batch_size + 1}: images {batch_start + 1}-{batch_end} of {len(valid_image_files)}")
+
+                    # Process each image in the batch
+                    for idx, image_file in enumerate(batch_files):
+                        absolute_idx = batch_start + idx
+                        self.logger.debug(f"   Processing file: {image_file}")
+                        image_path = os.path.join(image_dir, image_file)
+
+                        # Process each image with advanced capabilities (now async)
+                        processed_image_info = await self._process_extracted_image(
+                            image_path,
+                            document_id,
+                            processing_options
+                        )
+
+                        if processed_image_info:
+                            images.append(processed_image_info)
+                            self.logger.info(f"   ‚úÖ Processed image: {image_file}")
+
+                            # Report progress: Image processing progress
+                            if progress_callback and absolute_idx % 5 == 0:  # Update every 5 images
+                                try:
+                                    import inspect
+                                    progress_pct = 25 + (absolute_idx / len(valid_image_files)) * 10  # 25-35% range
+                                    if inspect.iscoroutinefunction(progress_callback):
+                                        await progress_callback(
+                                            progress=int(progress_pct),
+                                            details={
+                                                "current_step": f"Processing images ({absolute_idx + 1}/{len(valid_image_files)})",
+                                                "total_images": len(valid_image_files),
+                                                "images_processed": absolute_idx + 1
+                                            }
+                                        )
+                                    else:
+                                        progress_callback(
+                                            progress=int(progress_pct),
+                                            details={
+                                                "current_step": f"Processing images ({absolute_idx + 1}/{len(valid_image_files)})",
+                                                "total_images": len(valid_image_files),
+                                                "images_processed": absolute_idx + 1
+                                            }
+                                        )
+                                except Exception as e:
+                                    self.logger.warning(f"Progress callback failed: {e}")
+                        else:
+                            self.logger.warning(f"   ‚ö†Ô∏è Failed to process image: {image_file}")
+
+                    # Force garbage collection after each batch to free memory
+                    import gc
+                    gc.collect()
+                    self.logger.info(f"   ‚úÖ Batch {batch_start // batch_size + 1} completed, memory freed")
+            else:
+                self.logger.warning(f"‚ö†Ô∏è Image directory does not exist: {image_dir}")
+                self.logger.warning(f"   Output directory contents: {os.listdir(output_dir) if os.path.exists(output_dir) else 'N/A'}")
+
+            # NOTE: Do NOT clean up temporary directory here - images are needed for llamaindex processing
+            # The llamaindex_service will clean up image files after processing
+            # (CLIP embeddings, Anthropic analysis, database storage, etc.)
+            # Directory cleanup will happen in llamaindex_service after image processing completes
+
+            # Apply post-processing filters if requested
+            if processing_options.get('remove_duplicates', True):
+                images = self._remove_duplicate_images(images)
+
+            if processing_options.get('quality_filter', True):
+                min_quality = processing_options.get('min_quality_score', 0.3)
+                images = [img for img in images if img.get('quality_score', 1.0) >= min_quality]
+
+            self.logger.info(f"Successfully extracted and uploaded {len(images)} images to Supabase Storage")
+            return images
+
+        except Exception as e:
+            raise PDFExtractionError(f"Enhanced image extraction failed: {str(e)}") from e
+    
+    async def _test_connection(self) -> dict:
+        """
+        Test basic functionality for health checks.
+        
+        Returns:
+            Dict with test results
+        """
+        try:
+            # Test basic imports and functionality
+            import pymupdf4llm
+            import tempfile
+            import os
+            
+            # Create a minimal test to verify the service is working
+            test_result = {
+                "pymupdf4llm_available": True,
+                "tempfile_access": os.access(tempfile.gettempdir(), os.W_OK),
+                "thread_pool_active": self.executor is not None
+            }
+            
+            return test_result
+            
+        except Exception as e:
+            self.logger.error(f"Health check test failed: {str(e)}")
+            return {
+                "error": str(e),
+                "pymupdf4llm_available": False,
+                "tempfile_access": False,
+                "thread_pool_active": False
+            }
+    
+    def _create_temp_directory(self, document_id: str) -> str:
+        """Create temporary directory for processing."""
+        temp_dir = os.path.join(self.temp_dir_base, f"pdf_processor_{document_id}")
+        os.makedirs(temp_dir, exist_ok=True)
+        self.logger.debug("Created temporary directory: %s", temp_dir)
+        return temp_dir
+    
+    def _cleanup_temp_files(self, temp_dir: str) -> None:
+        """Clean up temporary files after processing."""
+        try:
+            import shutil
+            if os.path.exists(temp_dir):
+                shutil.rmtree(temp_dir)
+                self.logger.debug("Cleaned up temporary directory: %s", temp_dir)
+        except Exception as e:
+            self.logger.warning("Failed to cleanup temporary directory %s: %s", temp_dir, str(e))
+    
+    def _calculate_content_metrics(self, content: str) -> Dict[str, int]:
+        """Calculate word count, character count, etc."""
+        if not content:
+            return {'word_count': 0, 'character_count': 0, 'line_count': 0}
+        
+        lines = content.split('\n')
+        words = content.split()
+        
+        return {
+            'word_count': len(words),
+            'character_count': len(content),
+            'line_count': len(lines)
+        }
+    
+    async def _process_extracted_image(
+        self,
+        image_path: str,
+        document_id: str,
+        processing_options: Dict[str, Any]
+    ) -> Optional[Dict[str, Any]]:
+        """
+        Process a single extracted image with advanced capabilities and upload to Supabase Storage.
+
+        Features:
+        - Format conversion and optimization
+        - Metadata extraction (EXIF, technical specs)
+        - Quality assessment
+        - Image enhancement options
+        - Upload to Supabase Storage instead of local storage
+        - Duplicate detection preparation
+        - Memory-efficient processing with explicit cleanup
+        """
+        import gc
+
+        try:
+            # Load image with PIL for metadata and basic processing
+            with Image.open(image_path) as pil_image:
+                # Extract basic metadata
+                basic_info = {
+                    'filename': os.path.basename(image_path),
+                    'path': image_path,  # Keep local path for processing
+                    'size_bytes': os.path.getsize(image_path),
+                    'format': pil_image.format or 'UNKNOWN',
+                    'mode': pil_image.mode,
+                    'dimensions': pil_image.size,
+                    'width': pil_image.width,
+                    'height': pil_image.height
+                }
+
+                # Extract EXIF metadata if available
+                exif_data = self._extract_exif_metadata(pil_image)
+                
+                # Load with OpenCV for advanced analysis (if available)
+                quality_metrics = {'overall_score': 0.5}  # Default fallback
+                image_hash = 'unavailable'
+                enhanced_path = None
+
+                if CV2_AVAILABLE:
+                    try:
+                        cv_image = cv2.imread(image_path)
+                        if cv_image is not None:
+                            # Calculate quality metrics
+                            quality_metrics = self._calculate_image_quality(cv_image)
+
+                            # Calculate image hash for duplicate detection
+                            image_hash = self._calculate_image_hash(cv_image)
+
+                            # Apply enhancements if requested
+                            if processing_options.get('enhance_images', False):
+                                enhanced_path = self._enhance_image(
+                                    cv_image,
+                                    image_path,
+                                    processing_options
+                                )
+                        else:
+                            self.logger.warning("Could not load image with OpenCV: %s", image_path)
+                    except Exception as e:
+                        self.logger.warning(f"OpenCV analysis failed for {image_path}: {e}")
+                else:
+                    self.logger.debug("OpenCV not available, using basic image analysis")
+
+                # Convert format if requested (common for both CV2 available and not available)
+                converted_path = None
+                target_format = processing_options.get('target_format')
+                if target_format and target_format.upper() != basic_info['format']:
+                    converted_path = self._convert_image_format(
+                        pil_image,
+                        image_path,
+                        target_format
+                    )
+
+                # Upload image to Supabase Storage
+                upload_result = await self._upload_image_to_storage(
+                    image_path,
+                    document_id,
+                    basic_info,
+                    converted_path or enhanced_path
+                )
+
+                if not upload_result.get('success'):
+                    self.logger.warning(f"Failed to upload image to storage: {upload_result.get('error')}")
+                    # Continue processing even if upload fails, but mark it
+                    upload_result = {'success': False, 'error': 'Upload failed', 'public_url': None}
+
+                # ‚úÖ CRITICAL FIX: Skip AI analysis during extraction to prevent blocking
+                # AI analysis will be performed AFTER chunks/images are saved to database
+                # This ensures data persistence even if AI processing fails or hangs
+                real_analysis_data = {
+                    'quality_score': quality_metrics['overall_score'],
+                    'confidence_score': 0.5,
+                    'analysis_pending': True,  # Flag to indicate analysis needs to be done later
+                    'image_url': upload_result.get('public_url'),  # Store URL for later analysis
+                    'document_id': document_id
+                }
+
+                self.logger.info(f"‚úÖ Image uploaded to storage, AI analysis deferred: {basic_info['filename']}")
+
+                # Combine all metadata with storage information
+                # AI analysis results will be added later via async update
+                result = {
+                    **basic_info,
+                    'exif': exif_data,
+                    'quality_score': real_analysis_data.get('quality_score', quality_metrics['overall_score']),
+                    'confidence_score': real_analysis_data.get('confidence_score', 0.5),
+                    'quality_metrics': quality_metrics,
+                    'image_hash': image_hash,
+                    'enhanced_path': enhanced_path,
+                    'converted_path': converted_path,
+                    'processing_timestamp': datetime.utcnow().isoformat(),
+                    # Storage information
+                    'storage_uploaded': upload_result.get('success', False),
+                    'storage_url': upload_result.get('public_url'),
+                    'storage_path': upload_result.get('storage_path'),
+                    'storage_bucket': upload_result.get('bucket', 'pdf-tiles'),
+                    # Analysis status (deferred for async processing)
+                    'analysis_pending': real_analysis_data.get('analysis_pending', False),
+                    'analysis_image_url': real_analysis_data.get('image_url'),
+                    'analysis_document_id': real_analysis_data.get('document_id')
+                }
+
+                # Explicit memory cleanup for large images
+                gc.collect()
+
+                return result
+
+        except Exception as e:
+            self.logger.error("Error processing image %s: %s", image_path, str(e))
+            gc.collect()  # Cleanup even on error
+            return None
+
+    async def _upload_image_to_storage(
+        self,
+        image_path: str,
+        document_id: str,
+        image_info: Dict[str, Any],
+        processed_path: str = None
+    ) -> Dict[str, Any]:
+        """
+        Upload extracted image to Supabase Storage.
+
+        Args:
+            image_path: Path to the original image file
+            document_id: Document ID for organizing images
+            image_info: Basic image information
+            processed_path: Path to processed/enhanced image (if available)
+
+        Returns:
+            Dictionary with upload result
+        """
+        try:
+            # Use processed image if available, otherwise use original
+            upload_path = processed_path if processed_path and os.path.exists(processed_path) else image_path
+
+            # Validate file exists
+            if not os.path.exists(upload_path):
+                raise FileNotFoundError(f"Image file not found: {upload_path}")
+
+            # Read image file as bytes
+            with open(upload_path, 'rb') as f:
+                image_data = f.read()
+
+            # Validate image_data is bytes
+            if not isinstance(image_data, bytes):
+                raise TypeError(f"Expected bytes, got {type(image_data).__name__}: {image_data}")
+
+            self.logger.debug(f"Read {len(image_data)} bytes from {upload_path}")
+
+            # Get Supabase client
+            supabase_client = get_supabase_client()
+
+            # Extract page number from filename if possible
+            filename = os.path.basename(upload_path)
+            page_number = None
+
+            # Try to extract page number from filename patterns like "page_1_image_0.png"
+            import re
+            page_match = re.search(r'page[_-]?(\d+)', filename, re.IGNORECASE)
+            if page_match:
+                page_number = int(page_match.group(1))
+
+            # Upload to Supabase Storage
+            upload_result = await supabase_client.upload_image_file(
+                image_data=image_data,
+                filename=filename,
+                document_id=document_id,
+                page_number=page_number
+            )
+
+            if upload_result.get('success'):
+                self.logger.info(f"Successfully uploaded image to storage: {upload_result.get('public_url')}")
+
+                # NOTE: Do NOT clean up local files here - they're needed for llamaindex processing
+                # (CLIP embeddings, Anthropic analysis, etc.)
+                # Files will be cleaned up after llamaindex processing completes
+
+            return upload_result
+
+        except Exception as e:
+            self.logger.error(f"Failed to upload image to storage: {str(e)}")
+            return {
+                'success': False,
+                'error': str(e)
+            }
+
+    def _extract_exif_metadata(self, pil_image: Image.Image) -> Dict[str, Any]:
+        """Extract EXIF metadata from PIL Image."""
+        exif_data = {}
+        try:
+            if hasattr(pil_image, '_getexif') and pil_image._getexif() is not None:
+                exif = pil_image._getexif()
+                for tag_id, value in exif.items():
+                    tag = TAGS.get(tag_id, tag_id)
+                    exif_data[tag] = value
+        except Exception as e:
+            self.logger.debug("Could not extract EXIF data: %s", str(e))
+        
+        return exif_data
+    
+    def _calculate_image_quality(self, cv_image: np.ndarray) -> Dict[str, float]:
+        """
+        Calculate various image quality metrics using OpenCV and scikit-image.
+        """
+        if not CV2_AVAILABLE:
+            return {
+                'sharpness': 0.5,
+                'contrast': 0.5,
+                'brightness': 0.5,
+                'noise_level': 0.5,
+                'overall_score': 0.5
+            }
+
+        try:
+            # Convert to grayscale for analysis
+            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
+
+            # Calculate sharpness using Laplacian variance
+            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
+            sharpness_score = min(laplacian_var / 1000.0, 1.0)  # Normalize
+
+            # Calculate contrast using standard deviation
+            contrast_score = min(gray.std() / 128.0, 1.0)  # Normalize
+            
+            # Calculate brightness (mean intensity)
+            brightness = gray.mean() / 255.0
+            brightness_score = 1.0 - abs(brightness - 0.5) * 2  # Penalize extreme brightness
+            
+            # Calculate noise level using high-frequency content
+            noise_level = filters.gaussian(gray, sigma=1).std()
+            noise_score = max(0, 1.0 - noise_level / 50.0)  # Lower noise = higher score
+            
+            # Calculate overall quality score (weighted average)
+            overall_score = (
+                sharpness_score * 0.3 +
+                contrast_score * 0.25 +
+                brightness_score * 0.25 +
+                noise_score * 0.2
+            )
+            
+            return {
+                'sharpness': float(sharpness_score),
+                'contrast': float(contrast_score),
+                'brightness': float(brightness_score),
+                'noise': float(noise_score),
+                'overall_score': float(overall_score)
+            }
+            
+        except Exception as e:
+            self.logger.error("Error calculating image quality: %s", str(e))
+            return {
+                'sharpness': 0.5,
+                'contrast': 0.5,
+                'brightness': 0.5,
+                'noise': 0.5,
+                'overall_score': 0.5
+            }
+    
+    def _calculate_image_hash(self, cv_image: np.ndarray) -> str:
+        """Calculate perceptual hash for duplicate detection."""
+        if not CV2_AVAILABLE:
+            return 'opencv_unavailable'
+
+        try:
+            # Resize to 8x8 for hash calculation
+            small = cv2.resize(cv_image, (8, 8))
+            gray_small = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)
+            
+            # Calculate average
+            avg = gray_small.mean()
+            
+            # Create hash based on whether each pixel is above/below average
+            hash_bits = []
+            for row in gray_small:
+                for pixel in row:
+                    hash_bits.append('1' if pixel > avg else '0')
+            
+            # Convert to hexadecimal
+            hash_str = hex(int(''.join(hash_bits), 2))[2:]
+            return hash_str.zfill(16)  # Pad to 16 characters
+            
+        except Exception as e:
+            self.logger.error("Error calculating image hash: %s", str(e))
+            return "0000000000000000"
+    
+    def _enhance_image(
+        self,
+        cv_image: np.ndarray,
+        original_path: str,
+        processing_options: Dict[str, Any]
+    ) -> Optional[str]:
+        """Apply image enhancements and save enhanced version."""
+        if not CV2_AVAILABLE:
+            self.logger.warning("OpenCV not available, cannot enhance image")
+            return None
+
+        try:
+            enhanced = cv_image.copy()
+            
+            # Apply sharpening if requested
+            if processing_options.get('sharpen', True):
+                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
+                enhanced = cv2.filter2D(enhanced, -1, kernel)
+            
+            # Apply contrast enhancement if requested
+            if processing_options.get('enhance_contrast', True):
+                lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
+                l, a, b = cv2.split(lab)
+                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
+                l = clahe.apply(l)
+                enhanced = cv2.merge([l, a, b])
+                enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
+            
+            # Apply noise reduction if requested
+            if processing_options.get('denoise', True):
+                enhanced = cv2.fastNlMeansDenoisingColored(enhanced, None, 10, 10, 7, 21)
+            
+            # Save enhanced image
+            base_name = os.path.splitext(original_path)[0]
+            enhanced_path = f"{base_name}_enhanced.jpg"
+            cv2.imwrite(enhanced_path, enhanced, [cv2.IMWRITE_JPEG_QUALITY, 95])
+            
+            return enhanced_path
+            
+        except Exception as e:
+            self.logger.error("Error enhancing image: %s", str(e))
+            return None
+    
+    def _convert_image_format(
+        self,
+        pil_image: Image.Image,
+        original_path: str,
+        target_format: str
+    ) -> Optional[str]:
+        """Convert image to target format with optimization."""
+        try:
+            base_name = os.path.splitext(original_path)[0]
+            target_format = target_format.upper()
+            
+            # Determine file extension and save parameters
+            if target_format == 'JPEG':
+                converted_path = f"{base_name}_converted.jpg"
+                save_kwargs = {'quality': 95, 'optimize': True}
+            elif target_format == 'PNG':
+                converted_path = f"{base_name}_converted.png"
+                save_kwargs = {'optimize': True}
+            elif target_format == 'WEBP':
+                converted_path = f"{base_name}_converted.webp"
+                save_kwargs = {'quality': 95, 'method': 6}
+            else:
+                self.logger.warning("Unsupported target format: %s", target_format)
+                return None
+            
+            # Convert and save
+            if pil_image.mode in ('RGBA', 'LA') and target_format == 'JPEG':
+                # Convert RGBA to RGB for JPEG
+                rgb_image = Image.new('RGB', pil_image.size, (255, 255, 255))
+                rgb_image.paste(pil_image, mask=pil_image.split()[-1] if pil_image.mode == 'RGBA' else None)
+                rgb_image.save(converted_path, target_format, **save_kwargs)
+            else:
+                pil_image.save(converted_path, target_format, **save_kwargs)
+            
+            return converted_path
+            
+        except Exception as e:
+            self.logger.error("Error converting image format: %s", str(e))
+            return None
+    
+    def _remove_duplicate_images(self, images: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
+        """Remove duplicate images based on perceptual hash similarity."""
+        if len(images) <= 1:
+            return images
+        
+        unique_images = []
+        seen_hashes = set()
+        
+        for image in images:
+            image_hash = image.get('image_hash', '')
+            
+            # Check for exact hash matches
+            if image_hash and image_hash not in seen_hashes:
+                seen_hashes.add(image_hash)
+                unique_images.append(image)
+            elif not image_hash:
+                # Keep images without hashes (fallback)
+                unique_images.append(image)
+        
+        self.logger.info(
+            "Duplicate removal: %d original images, %d unique images",
+            len(images), len(unique_images)
+        )
+        
+        return unique_images
+    
+    def _opencv_fast_text_detection(self, image_path: str) -> Dict[str, Any]:
+        """
+        Ultra-fast text detection using OpenCV edge detection and contour analysis.
+        
+        This is Phase 1 of the OCR filtering pipeline. It uses simple computer vision
+        techniques to detect text-like patterns without running expensive OCR.
+        
+        Speed: ~0.1 seconds per image (300x faster than EasyOCR)
+        
+        Args:
+            image_path: Path to the image file
+            
+        Returns:
+            Dict with:
+                - has_text: Boolean indicating if text patterns detected
+                - text_contours_count: Number of text-like contours found
+                - confidence: Confidence score (0.0-1.0)
+                - method: Detection method used
+        """
+        try:
+            import cv2
+            import numpy as np
+            
+            # Load image in grayscale
+            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
+            
+            if img is None:
+                self.logger.warning(f"Failed to load image for OpenCV detection: {image_path}")
+                return {
+                    'has_text': True,  # Default to True to avoid false negatives
+                    'text_contours_count': 0,
+                    'confidence': 0.0,
+                    'method': 'opencv_edge_detection',
+                    'error': 'failed_to_load_image'
+                }
+            
+            # Apply Gaussian blur to reduce noise
+            blurred = cv2.GaussianBlur(img, (3, 3), 0)
+            
+            # Apply edge detection (Canny algorithm)
+            edges = cv2.Canny(blurred, 50, 150)
+            
+            # Find contours (shapes in the image)
+            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
+            
+            # Count text-like contours
+            text_like_contours = 0
+            total_contours = len(contours)
+            
+            for contour in contours:
+                # Get bounding rectangle
+                x, y, w, h = cv2.boundingRect(contour)
+                
+                # Calculate aspect ratio
+                aspect_ratio = w / h if h > 0 else 0
+                
+                # Text characteristics:
+                # - Aspect ratio between 0.1 and 10 (not too wide, not too tall)
+                # - Minimum size (width > 10px, height > 5px)
+                # - Maximum size (not the entire image)
+                # - Reasonable area (not too small, not too large)
+                area = w * h
+                image_area = img.shape[0] * img.shape[1]
+                area_ratio = area / image_area if image_area > 0 else 0
+                
+                if (0.1 < aspect_ratio < 10 and 
+                    w > 10 and h > 5 and 
+                    w < img.shape[1] * 0.9 and h < img.shape[0] * 0.9 and
+                    0.0001 < area_ratio < 0.5):
+                    text_like_contours += 1
+            
+            # Decision threshold: If fewer than 10 text-like shapes, probably no text
+            has_text = text_like_contours >= 10
+            
+            # Calculate confidence (normalize to 0-1)
+            confidence = min(text_like_contours / 50, 1.0)
+            
+            self.logger.debug(
+                f"OpenCV text detection: {text_like_contours} text-like contours "
+                f"(total: {total_contours}) -> {'HAS TEXT' if has_text else 'NO TEXT'}"
+            )
+            
+            return {
+                'has_text': has_text,
+                'text_contours_count': text_like_contours,
+                'total_contours': total_contours,
+                'confidence': confidence,
+                'method': 'opencv_edge_detection'
+            }
+            
+        except Exception as e:
+            self.logger.error(f"OpenCV text detection failed: {str(e)}")
+            # On error, default to True to avoid false negatives
+            return {
+                'has_text': True,
+                'text_contours_count': 0,
+                'confidence': 0.0,
+                'method': 'opencv_edge_detection',
+                'error': str(e)
+            }
+
+    async def _should_image_have_ocr(self, image_path: str) -> Dict[str, Any]:
+        """
+        AI-powered decision on whether an image needs OCR processing.
+        
+        Uses CLIP embeddings to classify images as:
+        - RELEVANT: Product specs, dimensions, technical data, material properties
+        - IRRELEVANT: Historical photos, biographies, decorative images, mood boards
+        
+        Args:
+            image_path: Path to the image file
+            
+        Returns:
+            Dict with 'should_process' (bool), 'reason' (str), 'confidence' (float)
+        """
+        try:
+            from PIL import Image
+            import torch
+            
+            # Load image
+            if not os.path.exists(image_path):
+                return {'should_process': False, 'reason': 'file_not_found', 'confidence': 0.0}
+            
+            image = Image.open(image_path).convert('RGB')
+            
+            # Initialize CLIP model for OCR filtering (cached after first use)
+            from llama_index.embeddings.clip import ClipEmbedding
+            if not hasattr(self, '_clip_model_for_ocr'):
+                self._clip_model_for_ocr = ClipEmbedding(model_name="ViT-B/32")
+                self.logger.info("‚úÖ Initialized CLIP model for OCR filtering: ViT-B/32")
+            
+            clip_model = self._clip_model_for_ocr
+            
+            # Define text prompts for classification
+            relevant_prompts = [
+                "product specification table with dimensions and measurements",
+                "technical data sheet with material properties and numbers",
+                "dimension annotations and measurements on product image",
+                "material property chart or graph with data",
+                "product label with technical specifications and text",
+                "technical drawing with dimension callouts and numbers",
+                "size chart or measurement guide with numbers",
+                "product features list with specifications",
+                "CAD drawing with dimension annotations",
+                "engineering diagram with measurements"
+            ]
+            
+            irrelevant_prompts = [
+                "historical photograph of people without technical content",
+                "biography or portrait photo with captions",
+                "decorative mood board or lifestyle image",
+                "artistic photography without text or labels",
+                "interior design scene without specifications or measurements",
+                "pure product photo without text labels or annotations",
+                "texture or pattern sample without text",
+                "company history or timeline image",
+                "inspirational quote or decorative text",
+                "brand logo or company name only",
+                "artistic typography without technical information"
+            ]
+            
+            # Get image embedding
+            loop = asyncio.get_event_loop()
+            image_embedding = await loop.run_in_executor(
+                None,
+                clip_model.get_image_embedding,
+                image
+            )
+            image_embedding = image_embedding.tolist() if hasattr(image_embedding, 'tolist') else list(image_embedding)
+            
+            # Get text embeddings for all prompts
+            all_prompts = relevant_prompts + irrelevant_prompts
+            text_embeddings = []
+            for prompt in all_prompts:
+                text_emb = await loop.run_in_executor(
+                    None,
+                    clip_model.get_text_embedding,
+                    prompt
+                )
+                text_emb_list = text_emb.tolist() if hasattr(text_emb, 'tolist') else list(text_emb)
+                text_embeddings.append(text_emb_list)
+            
+            # Calculate similarities
+            image_embedding_tensor = torch.tensor(image_embedding).unsqueeze(0)
+            text_embeddings_tensor = torch.tensor(text_embeddings)
+            
+            # Cosine similarity
+            similarities = torch.nn.functional.cosine_similarity(
+                image_embedding_tensor,
+                text_embeddings_tensor
+            )
+            
+            # Split similarities
+            relevant_similarities = similarities[:len(relevant_prompts)]
+            irrelevant_similarities = similarities[len(relevant_prompts):]
+            
+            # Calculate scores
+            relevant_score = relevant_similarities.max().item()
+            irrelevant_score = irrelevant_similarities.max().item()
+            
+            # Decision logic
+            # If relevant score is significantly higher, process with OCR
+            score_diff = relevant_score - irrelevant_score
+            
+            # STRICT THRESHOLDS: Only process images with high confidence of technical content
+            # relevant_score > 0.35 = Must have strong similarity to technical prompts (was 0.25)
+            # score_diff > 0.15 = Must be significantly more technical than decorative (was 0.05)
+            if relevant_score > 0.35 and score_diff > 0.15:
+                # Image likely contains technical/specification content
+                should_process = True
+                reason = f"technical_content (relevant: {relevant_score:.3f}, irrelevant: {irrelevant_score:.3f})"
+                confidence = relevant_score
+            else:
+                # Image is likely decorative/historical
+                should_process = False
+                reason = f"decorative_content (relevant: {relevant_score:.3f}, irrelevant: {irrelevant_score:.3f})"
+                confidence = irrelevant_score
+            
+            return {
+                'should_process': should_process,
+                'reason': reason,
+                'confidence': confidence,
+                'relevant_score': relevant_score,
+                'irrelevant_score': irrelevant_score
+            }
+            
+        except Exception as e:
+            self.logger.warning(f"OCR classification failed for {image_path}: {e}")
+            # Default to processing if classification fails (safe fallback)
+            return {
+                'should_process': True,
+                'reason': f'classification_error: {str(e)}',
+                'confidence': 0.5
+            }
+
+    async def _process_images_with_ocr(
+        self,
+        extracted_images: List[Dict[str, Any]],
+        ocr_languages: List[str],
+        progress_callback: Optional[Callable] = None
+    ) -> Tuple[str, List[Dict[str, Any]]]:
+        """
+        Process extracted images with OCR using the OCR service.
+        Now includes AI-powered filtering to only process relevant images.
+
+        Args:
+            extracted_images: List of image dictionaries with metadata
+            ocr_languages: List of language codes for OCR processing
+            progress_callback: Optional callback to report progress
+
+        Returns:
+            Tuple of (combined_ocr_text, ocr_results_list)
+        """
+        try:
+            ocr_service = get_ocr_service(OCRConfig(languages=ocr_languages))
+
+            combined_ocr_text = ""
+            ocr_results = []
+            total_images = len(extracted_images)
+            
+            # PHASE 1: OpenCV Fast Text Detection (0.1s per image)
+            self.logger.info(f"üîç Phase 1: OpenCV fast text detection on {total_images} images")
+            opencv_passed = []
+            opencv_skipped = []
+            
+            for idx, image_data in enumerate(extracted_images):
+                image_path = image_data.get('path')
+                if not image_path or not os.path.exists(image_path):
+                    continue
+                
+                # Use OpenCV to quickly detect text patterns
+                opencv_result = self._opencv_fast_text_detection(image_path)
+                
+                if opencv_result['has_text']:
+                    opencv_passed.append({
+                        'data': image_data,
+                        'opencv_result': opencv_result
+                    })
+                    self.logger.debug(
+                        f"  ‚úÖ Image {idx+1}/{total_images}: OpenCV detected {opencv_result['text_contours_count']} text-like contours"
+                    )
+                else:
+                    opencv_skipped.append({
+                        'data': image_data,
+                        'opencv_result': opencv_result,
+                        'skip_reason': 'opencv_no_text',
+                        'metadata': {
+                            'ocr_status': 'skipped',
+                            'skip_reason': 'opencv_no_text',
+                            'text_contours_count': opencv_result['text_contours_count'],
+                            'can_reprocess': True
+                        }
+                    })
+                    self.logger.debug(
+                        f"  ‚è≠Ô∏è  Image {idx+1}/{total_images}: OpenCV SKIPPED - only {opencv_result['text_contours_count']} text-like contours"
+                    )
+            
+            self.logger.info(
+                f"üéØ Phase 1 Results: {len(opencv_passed)} images with text patterns, "
+                f"{len(opencv_skipped)} skipped (no text detected)"
+            )
+            
+            # PHASE 2: CLIP AI Classification (0.5s per image) - only on images that passed Phase 1
+            self.logger.info(f"ü§ñ Phase 2: CLIP AI classification on {len(opencv_passed)} images")
+            images_to_process = []
+            clip_skipped = []
+            
+            for idx, item in enumerate(opencv_passed):
+                image_data = item['data']
+                image_path = image_data.get('path')
+                
+                # Use CLIP AI to classify if text is technical or decorative
+                clip_decision = await self._should_image_have_ocr(image_path)
+                
+                if clip_decision['should_process']:
+                    images_to_process.append({
+                        'data': image_data,
+                        'opencv_result': item['opencv_result'],
+                        'clip_decision': clip_decision
+                    })
+                    self.logger.debug(
+                        f"  ‚úÖ Image {idx+1}/{len(opencv_passed)}: {clip_decision['reason']}"
+                    )
+                else:
+                    clip_skipped.append({
+                        'data': image_data,
+                        'opencv_result': item['opencv_result'],
+                        'clip_decision': clip_decision,
+                        'skip_reason': 'clip_decorative',
+                        'metadata': {
+                            'ocr_status': 'skipped',
+                            'skip_reason': 'clip_decorative',
+                            'relevant_score': clip_decision.get('relevant_score', 0),
+                            'irrelevant_score': clip_decision.get('irrelevant_score', 0),
+                            'can_reprocess': True
+                        }
+                    })
+                    self.logger.debug(
+                        f"  ‚è≠Ô∏è  Image {idx+1}/{len(opencv_passed)}: CLIP SKIPPED - {clip_decision['reason']}"
+                    )
+            
+            # Combine all skipped images
+            images_skipped = opencv_skipped + clip_skipped
+            
+            self.logger.info(
+                f"üéØ Phase 2 Results: {len(images_to_process)} technical images, "
+                f"{len(clip_skipped)} decorative images skipped"
+            )
+            self.logger.info(
+                f"üìä Total Filtering: {len(images_to_process)}/{total_images} images will be processed with OCR "
+                f"({len(images_skipped)} skipped: {len(opencv_skipped)} no text, {len(clip_skipped)} decorative)"
+            )
+            
+            # PHASE 3: Full EasyOCR Processing (30s per image) - only on images that passed both filters
+            self.logger.info(f"üìù Phase 3: Running EasyOCR on {len(images_to_process)} filtered images")
+            
+            for idx, item in enumerate(images_to_process):
+                image_data = item['data']
+                ocr_decision = item['decision']
+                image_path = image_data.get('path')
+
+                try:
+                    # Report progress
+                    if progress_callback:
+                        progress_percent = 25 + int((idx / len(images_to_process)) * 15) if len(images_to_process) > 0 else 25
+                        if inspect.iscoroutinefunction(progress_callback):
+                            await progress_callback(
+                                progress=progress_percent,
+                                details={
+                                    "current_step": f"Processing image {idx + 1}/{len(images_to_process)} with OCR (AI-filtered)",
+                                    "images_processed": idx + 1,
+                                    "total_images": len(images_to_process),
+                                    "images_skipped": len(images_skipped)
+                                }
+                            )
+                        else:
+                            progress_callback(
+                                progress=progress_percent,
+                                details={
+                                    "current_step": f"Processing image {idx + 1}/{len(images_to_process)} with OCR (AI-filtered)",
+                                    "images_processed": idx + 1,
+                                    "total_images": len(images_to_process),
+                                    "images_skipped": len(images_skipped)
+                                }
+                            )
+
+                    # Process image with OCR using the correct method
+                    loop = asyncio.get_event_loop()
+                    ocr_result_list = await loop.run_in_executor(
+                        None,
+                        ocr_service.extract_text_from_image,
+                        image_path
+                    )
+
+                    if ocr_result_list:
+                        # Combine all extracted text from the image
+                        extracted_text = " ".join([result.text for result in ocr_result_list])
+                        avg_confidence = sum([result.confidence for result in ocr_result_list]) / len(ocr_result_list) if ocr_result_list else 0.0
+
+                        combined_ocr_text += extracted_text + "\n"
+                        ocr_results.append({
+                            'image_path': image_path,
+                            'text': extracted_text,
+                            'confidence': avg_confidence,
+                            'language': ocr_languages[0] if ocr_languages else 'en',
+                            'regions_detected': len(ocr_result_list),
+                            'ai_classification': ocr_decision
+                        })
+                        
+                        self.logger.info(f"  ‚úÖ OCR extracted {len(ocr_result_list)} text regions from image {idx+1}")
+                    else:
+                        self.logger.info(f"  ‚ÑπÔ∏è  No text found in image {idx+1}")
+
+                except Exception as e:
+                    self.logger.warning("OCR processing failed for image %s: %s", image_path, str(e))
+                    ocr_results.append({
+                        'image_path': image_path,
+                        'text': '',
+                        'confidence': 0.0,
+                        'language': 'unknown',
+                        'error': str(e),
+                        'ai_classification': ocr_decision
+                    })
+            
+            # Add skipped images to results with metadata
+            for item in images_skipped:
+                image_data = item['data']
+                skip_reason = item.get('skip_reason', 'unknown')
+                metadata = item.get('metadata', {})
+                
+                # Build comprehensive metadata for skipped images
+                skip_metadata = {
+                    'ocr_status': 'skipped',
+                    'skip_reason': skip_reason,
+                    'can_reprocess': True
+                }
+                
+                # Add OpenCV results if available
+                if 'opencv_result' in item:
+                    skip_metadata['opencv_detection'] = {
+                        'text_contours_count': item['opencv_result'].get('text_contours_count', 0),
+                        'confidence': item['opencv_result'].get('confidence', 0.0)
+                    }
+                
+                # Add CLIP results if available
+                if 'clip_decision' in item:
+                    skip_metadata['clip_classification'] = {
+                        'relevant_score': item['clip_decision'].get('relevant_score', 0.0),
+                        'irrelevant_score': item['clip_decision'].get('irrelevant_score', 0.0),
+                        'reason': item['clip_decision'].get('reason', '')
+                    }
+                
+                ocr_results.append({
+                    'image_path': image_data.get('path'),
+                    'text': '',
+                    'confidence': 0.0,
+                    'language': 'skipped',
+                    'regions_detected': 0,
+                    'skipped': True,
+                    'skip_metadata': skip_metadata
+                })
+            
+            self.logger.info(f"‚úÖ OCR Processing Complete: {len(images_to_process)} processed, {len(images_skipped)} skipped")
+
+            return combined_ocr_text.strip(), ocr_results
+
+        except Exception as e:
+            self.logger.error("Error in OCR processing: %s", str(e))
+            return "", []
+
-- 
2.43.0


From 9a5d43d2d6cfe29b9512c247f78f8cf30af5ec60 Mon Sep 17 00:00:00 2001
From: Basilis Kanonidis <basiliskan@gmail.com>
Date: Wed, 29 Oct 2025 11:04:14 +0000
Subject: [PATCH 2/2] fix: Resolve admin module import conflict

- Renamed app/api/admin/ directory to admin_modules_old/ to avoid conflict with admin.py file
- Updated main.py to import from admin_modules_old.chunk_quality
- Fixed ImportError that was preventing service from starting

This allows both admin.py (with OCR optimization router) and admin modules (chunk_quality) to coexist.
---
 app/api/admin/__init__.py                     |    2 -
 app/api/admin_modules_old/__init__.py         |    1 +
 .../chunk_quality.py                          |    0
 app/api/rag_routes.py.backup                  | 2031 +++++++++++++++++
 app/main.py                                   |    2 +-
 deployment_error_logs.txt                     |   28 +
 package_status_report.txt                     |   16 +-
 7 files changed, 2065 insertions(+), 15 deletions(-)
 delete mode 100644 app/api/admin/__init__.py
 create mode 100644 app/api/admin_modules_old/__init__.py
 rename app/api/{admin => admin_modules_old}/chunk_quality.py (100%)
 create mode 100644 app/api/rag_routes.py.backup
 create mode 100644 deployment_error_logs.txt

diff --git a/app/api/admin/__init__.py b/app/api/admin/__init__.py
deleted file mode 100644
index dd760dc..0000000
--- a/app/api/admin/__init__.py
+++ /dev/null
@@ -1,2 +0,0 @@
-"""Admin API package"""
-
diff --git a/app/api/admin_modules_old/__init__.py b/app/api/admin_modules_old/__init__.py
new file mode 100644
index 0000000..7cb863e
--- /dev/null
+++ b/app/api/admin_modules_old/__init__.py
@@ -0,0 +1 @@
+"""Admin modules package"""
diff --git a/app/api/admin/chunk_quality.py b/app/api/admin_modules_old/chunk_quality.py
similarity index 100%
rename from app/api/admin/chunk_quality.py
rename to app/api/admin_modules_old/chunk_quality.py
diff --git a/app/api/rag_routes.py.backup b/app/api/rag_routes.py.backup
new file mode 100644
index 0000000..3a86334
--- /dev/null
+++ b/app/api/rag_routes.py.backup
@@ -0,0 +1,2031 @@
+"""
+RAG (Retrieval-Augmented Generation) API Routes
+
+This module provides comprehensive FastAPI endpoints for RAG functionality including
+document embedding, querying, chat interface, and document management.
+"""
+
+import logging
+from datetime import datetime
+from typing import Dict, List, Optional, Any
+from uuid import uuid4
+
+from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form, Query, status, BackgroundTasks
+from fastapi.responses import JSONResponse
+import asyncio
+try:
+    # Try Pydantic v2 first
+    from pydantic import BaseModel, Field, field_validator as validator
+except ImportError:
+    # Fall back to Pydantic v1
+    from pydantic import BaseModel, Field, validator
+
+from app.config import get_settings
+from app.services.llamaindex_service import LlamaIndexService
+from app.services.real_embeddings_service import RealEmbeddingsService
+from app.services.advanced_search_service import QueryType, SearchOperator
+from app.services.product_creation_service import ProductCreationService
+from app.services.job_recovery_service import JobRecoveryService
+from app.services.checkpoint_recovery_service import checkpoint_recovery_service, ProcessingStage
+from app.services.supabase_client import get_supabase_client
+from app.services.ai_model_tracker import AIModelTracker
+from app.utils.logging import PDFProcessingLogger
+
+logger = logging.getLogger(__name__)
+
+# Initialize router
+router = APIRouter(prefix="/api/rag", tags=["RAG"])
+
+# Job storage for async processing (in-memory cache)
+job_storage: Dict[str, Dict[str, Any]] = {}
+
+# Job recovery service (initialized on startup)
+job_recovery_service: Optional[JobRecoveryService] = None
+
+
+async def initialize_job_recovery():
+    """
+    Initialize job recovery service and mark any interrupted jobs.
+    This should be called on application startup.
+    """
+    global job_recovery_service
+
+    try:
+        logger.info("üîÑ Initializing job recovery service...")
+
+        supabase_client = get_supabase_client()
+        job_recovery_service = JobRecoveryService(supabase_client)
+
+        # Mark all processing jobs as interrupted (they were interrupted by restart)
+        interrupted_count = await job_recovery_service.mark_all_processing_as_interrupted(
+            reason="Service restart detected"
+        )
+
+        if interrupted_count > 0:
+            logger.warning(f"üõë Marked {interrupted_count} jobs as interrupted due to service restart")
+
+        # Get statistics
+        stats = await job_recovery_service.get_job_statistics()
+        logger.info(f"üìä Job statistics: {stats}")
+
+        # Cleanup old jobs (older than 7 days)
+        cleaned = await job_recovery_service.cleanup_old_jobs(days=7)
+        if cleaned > 0:
+            logger.info(f"üßπ Cleaned up {cleaned} old jobs")
+
+        logger.info("‚úÖ Job recovery service initialized successfully")
+
+    except Exception as e:
+        logger.error(f"‚ùå Failed to initialize job recovery service: {e}", exc_info=True)
+        # Don't fail startup if job recovery fails
+        job_recovery_service = None
+
+# Pydantic models for request/response validation
+class DocumentUploadRequest(BaseModel):
+    """Request model for document upload and processing."""
+    title: Optional[str] = Field(None, description="Document title")
+    description: Optional[str] = Field(None, description="Document description")
+    tags: Optional[List[str]] = Field(default_factory=list, description="Document tags")
+    chunk_size: Optional[int] = Field(1000, ge=100, le=4000, description="Chunk size for processing")
+    chunk_overlap: Optional[int] = Field(200, ge=0, le=1000, description="Chunk overlap")
+    enable_embedding: bool = Field(True, description="Enable automatic embedding generation")
+
+class DocumentUploadResponse(BaseModel):
+    """Response model for document upload."""
+    document_id: str = Field(..., description="Unique document identifier")
+    title: str = Field(..., description="Document title")
+    status: str = Field(..., description="Processing status")
+    chunks_created: int = Field(..., description="Number of chunks created")
+    embeddings_generated: bool = Field(..., description="Whether embeddings were generated")
+    processing_time: float = Field(..., description="Processing time in seconds")
+    message: str = Field(..., description="Status message")
+
+class QueryRequest(BaseModel):
+    """Request model for RAG queries."""
+    query: str = Field(..., min_length=1, max_length=2000, description="Query text")
+    top_k: Optional[int] = Field(5, ge=1, le=20, description="Number of top results to retrieve")
+    similarity_threshold: Optional[float] = Field(0.7, ge=0.0, le=1.0, description="Similarity threshold")
+    include_metadata: bool = Field(True, description="Include document metadata in response")
+    enable_reranking: bool = Field(True, description="Enable result reranking")
+    document_ids: Optional[List[str]] = Field(None, description="Filter by specific document IDs")
+
+class QueryResponse(BaseModel):
+    """Response model for RAG queries."""
+    query: str = Field(..., description="Original query")
+    answer: str = Field(..., description="Generated answer")
+    sources: List[Dict[str, Any]] = Field(..., description="Source documents and chunks")
+    confidence_score: float = Field(..., description="Confidence score for the answer")
+    processing_time: float = Field(..., description="Query processing time in seconds")
+    retrieved_chunks: int = Field(..., description="Number of chunks retrieved")
+
+class ChatRequest(BaseModel):
+    """Request model for conversational RAG."""
+    message: str = Field(..., min_length=1, max_length=2000, description="Chat message")
+    conversation_id: Optional[str] = Field(None, description="Conversation ID for context")
+    top_k: Optional[int] = Field(5, ge=1, le=20, description="Number of context chunks to retrieve")
+    include_history: bool = Field(True, description="Include conversation history in context")
+    document_ids: Optional[List[str]] = Field(None, description="Filter by specific document IDs")
+
+class ChatResponse(BaseModel):
+    """Response model for conversational RAG."""
+    message: str = Field(..., description="Original message")
+    response: str = Field(..., description="AI response")
+    conversation_id: str = Field(..., description="Conversation ID")
+    sources: List[Dict[str, Any]] = Field(..., description="Source documents used")
+    processing_time: float = Field(..., description="Response generation time")
+
+class SearchRequest(BaseModel):
+    """Request model for semantic search."""
+    query: str = Field(..., min_length=1, max_length=1000, description="Search query")
+    search_type: str = Field("semantic", pattern="^(semantic|hybrid|keyword)$", description="Search type")
+    top_k: Optional[int] = Field(10, ge=1, le=50, description="Number of results to return")
+    similarity_threshold: Optional[float] = Field(0.6, ge=0.0, le=1.0, description="Similarity threshold")
+    document_ids: Optional[List[str]] = Field(None, description="Filter by document IDs")
+    include_content: bool = Field(True, description="Include chunk content in results")
+
+class SearchResponse(BaseModel):
+    """Response model for semantic search."""
+    query: str = Field(..., description="Original search query")
+    results: List[Dict[str, Any]] = Field(..., description="Search results")
+    total_results: int = Field(..., description="Total number of results")
+    search_type: str = Field(..., description="Type of search performed")
+    processing_time: float = Field(..., description="Search processing time")
+
+class DocumentListResponse(BaseModel):
+    """Response model for document listing."""
+    documents: List[Dict[str, Any]] = Field(..., description="List of documents")
+    total_count: int = Field(..., description="Total number of documents")
+    page: int = Field(..., description="Current page number")
+    page_size: int = Field(..., description="Page size")
+
+class HealthCheckResponse(BaseModel):
+    """Response model for RAG health check."""
+    status: str = Field(..., description="Health status")
+    services: Dict[str, Dict[str, Any]] = Field(..., description="Service health details")
+    timestamp: str = Field(..., description="Health check timestamp")
+
+# Advanced Search Models for Phase 7 Features
+class MMRSearchRequest(BaseModel):
+    """Request model for MMR (Maximal Marginal Relevance) search."""
+    query: str = Field(..., min_length=1, max_length=2000, description="Search query")
+    top_k: Optional[int] = Field(10, ge=1, le=50, description="Number of initial results to retrieve")
+    diversity_threshold: Optional[float] = Field(0.7, ge=0.0, le=1.0, description="MMR diversity threshold")
+    lambda_param: Optional[float] = Field(0.5, ge=0.0, le=1.0, description="MMR lambda parameter for relevance vs diversity balance")
+    document_ids: Optional[List[str]] = Field(None, description="Filter by specific document IDs")
+    include_metadata: bool = Field(True, description="Include document metadata in response")
+
+class MMRSearchResponse(BaseModel):
+    """Response model for MMR search."""
+    query: str = Field(..., description="Original search query")
+    results: List[Dict[str, Any]] = Field(..., description="MMR search results with diversity scores")
+    total_results: int = Field(..., description="Total number of results")
+    diversity_score: float = Field(..., description="Overall diversity score of results")
+    processing_time: float = Field(..., description="Search processing time in seconds")
+
+class AdvancedQueryRequest(BaseModel):
+    """Request model for advanced query with optimization."""
+    query: str = Field(..., min_length=1, max_length=2000, description="Query text")
+    query_type: str = Field("semantic", pattern="^(factual|analytical|conversational|boolean|fuzzy|semantic)$", description="Type of query")
+    top_k: Optional[int] = Field(10, ge=1, le=50, description="Number of results to retrieve")
+    enable_expansion: bool = Field(True, description="Enable query expansion")
+    enable_rewriting: bool = Field(True, description="Enable query rewriting")
+    similarity_threshold: Optional[float] = Field(0.6, ge=0.0, le=1.0, description="Similarity threshold")
+    document_ids: Optional[List[str]] = Field(None, description="Filter by specific document IDs")
+    metadata_filters: Optional[Dict[str, Any]] = Field(None, description="Metadata-based filters")
+    search_operator: str = Field("AND", pattern="^(AND|OR|NOT)$", description="Search operator for multiple terms")
+
+class AdvancedQueryResponse(BaseModel):
+    """Response model for advanced query."""
+    original_query: str = Field(..., description="Original query text")
+    optimized_query: str = Field(..., description="Optimized/expanded query")
+    query_type: str = Field(..., description="Type of query processed")
+    results: List[Dict[str, Any]] = Field(..., description="Search results with relevance scores")
+    total_results: int = Field(..., description="Total number of results")
+    expansion_terms: List[str] = Field(..., description="Terms added during query expansion")
+    processing_time: float = Field(..., description="Query processing time in seconds")
+    confidence_score: float = Field(..., description="Overall confidence score")
+
+# Dependency functions
+async def get_llamaindex_service() -> LlamaIndexService:
+    """Get LlamaIndex service instance."""
+    from app.main import app
+    if not hasattr(app.state, 'llamaindex_service') or app.state.llamaindex_service is None:
+        raise HTTPException(
+            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
+            detail="LlamaIndex service is not available"
+        )
+    return app.state.llamaindex_service
+
+async def get_embedding_service() -> RealEmbeddingsService:
+    """Get embedding service instance."""
+    try:
+        supabase_client = get_supabase_client()
+        return RealEmbeddingsService(supabase_client=supabase_client)
+    except Exception as e:
+        logger.error(f"Failed to initialize embedding service: {e}")
+        raise HTTPException(
+            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
+            detail="Embedding service is not available"
+        )
+
+# API Endpoints
+
+@router.post("/documents/upload", response_model=DocumentUploadResponse)
+async def upload_document(
+    file: UploadFile = File(...),
+    title: Optional[str] = Form(None),
+    description: Optional[str] = Form(None),
+    tags: Optional[str] = Form(None),  # JSON string of tags
+    chunk_size: int = Form(1000),
+    chunk_overlap: int = Form(200),
+    enable_embedding: bool = Form(True),
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Upload and process a document for RAG functionality.
+    
+    This endpoint accepts document uploads, processes them into chunks,
+    generates embeddings, and stores them in the vector database.
+    """
+    start_time = datetime.utcnow()
+    
+    try:
+        # Validate file type
+        if not file.content_type or not file.content_type.startswith(('application/pdf', 'text/', 'application/msword')):
+            raise HTTPException(
+                status_code=status.HTTP_400_BAD_REQUEST,
+                detail="Unsupported file type. Please upload PDF, text, or Word documents."
+            )
+        
+        # Parse tags if provided
+        document_tags = []
+        if tags:
+            try:
+                import json
+                document_tags = json.loads(tags)
+            except json.JSONDecodeError:
+                document_tags = [tag.strip() for tag in tags.split(',')]
+        
+        # Generate document ID
+        document_id = str(uuid4())
+        
+        # Read file content
+        file_content = await file.read()
+        
+        # Process document through LlamaIndex service
+        processing_result = await llamaindex_service.index_document_content(
+            file_content=file_content,
+            document_id=document_id,
+            file_path=file.filename,
+            metadata={
+                "filename": file.filename,
+                "title": title or file.filename,
+                "description": description,
+                "tags": document_tags,
+                "source": "rag_upload"
+            },
+            chunk_size=chunk_size,
+            chunk_overlap=chunk_overlap
+        )
+        
+        processing_time = (datetime.utcnow() - start_time).total_seconds()
+
+        # Check if processing actually succeeded
+        result_status = processing_result.get('status', 'completed')
+        chunks_created = processing_result.get('statistics', {}).get('total_chunks', 0)
+
+        # If status is error, raise an exception with details
+        if result_status == 'error':
+            error_message = processing_result.get('error', 'Unknown error during document processing')
+            logger.error(f"Document processing failed for {document_id}: {error_message}")
+            raise HTTPException(
+                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+                detail=f"Document processing failed: {error_message}"
+            )
+
+        return DocumentUploadResponse(
+            document_id=document_id,
+            title=title or file.filename,
+            status=result_status,
+            chunks_created=chunks_created,
+            embeddings_generated=chunks_created > 0,  # Only true if chunks were actually created
+            processing_time=processing_time,
+            message=f"Document processed successfully: {chunks_created} chunks created"
+        )
+        
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Document upload failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Document processing failed: {str(e)}"
+        )
+
+@router.post("/documents/upload-async")
+async def upload_document_async(
+    background_tasks: BackgroundTasks,
+    file: UploadFile = File(...),
+    title: Optional[str] = Form(None),
+    description: Optional[str] = Form(None),
+    tags: Optional[str] = Form(None),
+    chunk_size: int = Form(1000),
+    chunk_overlap: int = Form(200),
+    enable_embedding: bool = Form(True),
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Upload and process a document asynchronously for RAG functionality.
+
+    Returns immediately with a job_id that can be used to check processing status.
+    Use GET /documents/job/{job_id} to check the status and get results.
+    """
+    try:
+        # Validate file type
+        if not file.content_type or not file.content_type.startswith(('application/pdf', 'text/', 'application/msword')):
+            raise HTTPException(
+                status_code=status.HTTP_400_BAD_REQUEST,
+                detail="Unsupported file type. Please upload PDF, text, or Word documents."
+            )
+
+        # Generate IDs
+        job_id = str(uuid4())
+        document_id = str(uuid4())
+
+        # Parse tags if provided
+        document_tags = []
+        if tags:
+            try:
+                import json
+                document_tags = json.loads(tags)
+            except json.JSONDecodeError:
+                document_tags = [tag.strip() for tag in tags.split(',')]
+
+        # Read file content
+        file_content = await file.read()
+
+        # Create placeholder document record FIRST (to satisfy foreign key constraint)
+        supabase_client = get_supabase_client()
+        try:
+            supabase_client.client.table('documents').insert({
+                'id': document_id,
+                'filename': file.filename,
+                'content_type': file.content_type,
+                'processing_status': 'processing',
+                'metadata': {
+                    'title': title or file.filename,
+                    'description': description,
+                    'catalog': catalog
+                }
+            }).execute()
+            logger.info(f"‚úÖ Created placeholder document record: {document_id}")
+        except Exception as e:
+            logger.error(f"Failed to create placeholder document: {e}")
+            # Continue anyway - the background task will create it
+
+        # Initialize job storage (in-memory)
+        job_storage[job_id] = {
+            "status": "pending",
+            "document_id": document_id,
+            "filename": file.filename,
+            "created_at": datetime.utcnow().isoformat(),
+            "progress": 0
+        }
+
+        # Persist job to database for recovery (now document exists)
+        if job_recovery_service:
+            await job_recovery_service.persist_job(
+                job_id=job_id,
+                document_id=document_id,
+                filename=file.filename,
+                status="pending",
+                metadata={
+                    "title": title,
+                    "description": description,
+                    "tags": document_tags,
+                    "chunk_size": chunk_size,
+                    "chunk_overlap": chunk_overlap
+                },
+                progress=0
+            )
+
+        # Start background processing
+        background_tasks.add_task(
+            process_document_background,
+            job_id=job_id,
+            document_id=document_id,
+            file_content=file_content,
+            filename=file.filename,
+            title=title,
+            description=description,
+            document_tags=document_tags,
+            chunk_size=chunk_size,
+            chunk_overlap=chunk_overlap,
+            llamaindex_service=llamaindex_service
+        )
+
+        return JSONResponse(
+            status_code=status.HTTP_202_ACCEPTED,
+            content={
+                "job_id": job_id,
+                "document_id": document_id,
+                "status": "pending",
+                "message": "Document upload accepted. Processing in background.",
+                "status_url": f"/api/rag/documents/job/{job_id}"
+            }
+        )
+
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Document upload failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Document upload failed: {str(e)}"
+        )
+
+
+@router.get("/documents/job/{job_id}")
+async def get_job_status(job_id: str):
+    """
+    Get the status of an async document processing job with checkpoint information.
+
+    Returns:
+        - Job status and progress
+        - Latest checkpoint information
+        - Detailed metadata including AI usage, chunks, images, products
+    """
+    # Check in-memory storage first
+    if job_id in job_storage:
+        job_data = job_storage[job_id]
+
+        # Enhance with checkpoint information
+        try:
+            last_checkpoint = await checkpoint_recovery_service.get_last_checkpoint(job_id)
+            if last_checkpoint:
+                job_data["last_checkpoint"] = {
+                    "stage": last_checkpoint.get('stage'),
+                    "created_at": last_checkpoint.get('created_at'),
+                    "data": last_checkpoint.get('checkpoint_data', {})
+                }
+        except Exception as e:
+            logger.error(f"Failed to get checkpoint for job {job_id}: {e}")
+
+        return JSONResponse(content=job_data)
+
+    # Check database for background_jobs
+    try:
+        supabase_client = get_supabase_client()
+        logger.info(f"üîç Checking database for job {job_id}")
+        response = supabase_client.client.table('background_jobs').select('*').eq('id', job_id).execute()
+        logger.info(f"üîç Database response: data={response.data}, count={len(response.data) if response.data else 0}")
+
+        if response.data and len(response.data) > 0:
+            job = response.data[0]
+            logger.info(f"‚úÖ Found job in database: {job['id']}, status={job['status']}")
+
+            job_response = {
+                "job_id": job['id'],
+                "status": job['status'],
+                "document_id": job.get('document_id'),
+                "progress": job.get('progress', 0),
+                "error": job.get('error'),
+                "metadata": job.get('metadata', {}),
+                "created_at": job.get('created_at'),
+                "updated_at": job.get('updated_at')
+            }
+
+            # Add checkpoint information
+            try:
+                last_checkpoint = await checkpoint_recovery_service.get_last_checkpoint(job_id)
+                if last_checkpoint:
+                    job_response["last_checkpoint"] = {
+                        "stage": last_checkpoint.get('stage'),
+                        "created_at": last_checkpoint.get('created_at'),
+                        "data": last_checkpoint.get('checkpoint_data', {})
+                    }
+            except Exception as e:
+                logger.error(f"Failed to get checkpoint for job {job_id}: {e}")
+
+            return JSONResponse(content=job_response)
+        else:
+            logger.warning(f"‚ö†Ô∏è Job {job_id} not found in database")
+    except Exception as e:
+        logger.error(f"Error checking database for job {job_id}: {e}", exc_info=True)
+
+    raise HTTPException(
+        status_code=status.HTTP_404_NOT_FOUND,
+        detail=f"Job {job_id} not found"
+    )
+
+
+@router.get("/jobs/{job_id}/checkpoints")
+async def get_job_checkpoints(job_id: str):
+    """
+    Get all checkpoints for a job.
+
+    Returns:
+        - List of all checkpoints with stage, data, and metadata
+        - Checkpoint count
+        - Processing timeline
+    """
+    try:
+        checkpoints = await checkpoint_recovery_service.get_all_checkpoints(job_id)
+
+        return JSONResponse(content={
+            "job_id": job_id,
+            "checkpoints": checkpoints,
+            "count": len(checkpoints),
+            "stages_completed": [cp.get('stage') for cp in checkpoints]
+        })
+    except Exception as e:
+        logger.error(f"Failed to get checkpoints for job {job_id}: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Failed to retrieve checkpoints: {str(e)}"
+        )
+
+
+@router.post("/jobs/{job_id}/restart")
+async def restart_job_from_checkpoint(job_id: str):
+    """
+    Manually restart a job from its last checkpoint.
+
+    This endpoint allows manual recovery of stuck or failed jobs.
+    The job will resume from the last successful checkpoint.
+    """
+    try:
+        # Get last checkpoint
+        last_checkpoint = await checkpoint_recovery_service.get_last_checkpoint(job_id)
+
+        if not last_checkpoint:
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail=f"No checkpoint found for job {job_id}"
+            )
+
+        # Verify checkpoint data exists
+        resume_stage = last_checkpoint.get('stage')
+        can_resume = await checkpoint_recovery_service.verify_checkpoint_data(job_id, resume_stage)
+
+        if not can_resume:
+            raise HTTPException(
+                status_code=status.HTTP_400_BAD_REQUEST,
+                detail=f"Checkpoint data verification failed for stage {resume_stage}"
+            )
+
+        # Mark job for restart
+        supabase_client = get_supabase_client()
+        supabase_client.client.table('background_jobs').update({
+            "status": "pending_restart",
+            "metadata": {
+                "restart_from_stage": resume_stage,
+                "restart_reason": "manual_restart",
+                "restart_at": datetime.utcnow().isoformat()
+            }
+        }).eq('id', job_id).execute()
+
+        logger.info(f"‚úÖ Job {job_id} marked for restart from {resume_stage}")
+
+        return JSONResponse(content={
+            "success": True,
+            "message": f"Job will restart from checkpoint: {resume_stage}",
+            "job_id": job_id,
+            "restart_stage": resume_stage,
+            "checkpoint_data": last_checkpoint.get('checkpoint_data', {})
+        })
+
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Failed to restart job {job_id}: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Failed to restart job: {str(e)}"
+        )
+
+
+# REMOVED: Duplicate endpoint - use /api/admin/jobs/{job_id}/status instead
+# This endpoint was conflicting with admin.py and never being reached due to router registration order
+
+
+async def create_products_background(
+    document_id: str,
+    workspace_id: str,
+    job_id: str
+):
+    """
+    Background task to create products from chunks with checkpoint support.
+    Runs separately to avoid blocking main PDF processing.
+    Creates a sub-job for tracking.
+    """
+    supabase_client = get_supabase_client()
+    sub_job_id = f"{job_id}_products"
+
+    try:
+        logger.info(f"üè≠ Starting background product creation for document {document_id}")
+
+        # Create sub-job in database
+        try:
+            supabase_client.client.table('background_jobs').insert({
+                "id": sub_job_id,
+                "parent_job_id": job_id,
+                "job_type": "product_creation",
+                "document_id": document_id,
+                "status": "processing",
+                "progress": 0,
+                "metadata": {
+                    "workspace_id": workspace_id,
+                    "started_at": datetime.utcnow().isoformat()
+                },
+                "created_at": datetime.utcnow().isoformat(),
+                "updated_at": datetime.utcnow().isoformat()
+            }).execute()
+            logger.info(f"‚úÖ Created sub-job {sub_job_id} for product creation")
+        except Exception as e:
+            logger.warning(f"‚ö†Ô∏è Failed to create sub-job: {e}")
+
+        product_service = ProductCreationService(supabase_client)
+
+        # Create PRODUCTS_DETECTED checkpoint before detection
+        await checkpoint_recovery_service.create_checkpoint(
+            job_id=job_id,
+            stage=ProcessingStage.PRODUCTS_DETECTED,
+            data={
+                "document_id": document_id,
+                "workspace_id": workspace_id,
+                "detection_started": True
+            },
+            metadata={
+                "current_step": "Detecting product candidates",
+                "timestamp": datetime.utcnow().isoformat()
+            }
+        )
+        logger.info(f"‚úÖ Created PRODUCTS_DETECTED checkpoint for job {job_id}")
+
+        # Use layout-based product detection
+        product_result = await product_service.create_products_from_layout_candidates(
+            document_id=document_id,
+            workspace_id=workspace_id,
+            min_confidence=0.5,
+            min_quality_score=0.5
+        )
+
+        products_created = product_result.get('products_created', 0)
+        logger.info(f"‚úÖ Background product creation completed: {products_created} products created")
+
+        # Update sub-job status to completed
+        try:
+            supabase_client.client.table('background_jobs').update({
+                "status": "completed",
+                "progress": 100,
+                "metadata": {
+                    "workspace_id": workspace_id,
+                    "products_created": products_created,
+                    "candidates_detected": product_result.get('candidates_detected', 0),
+                    "validation_passed": product_result.get('validation_passed', 0),
+                    "completed_at": datetime.utcnow().isoformat()
+                },
+                "updated_at": datetime.utcnow().isoformat()
+            }).eq('id', sub_job_id).execute()
+            logger.info(f"‚úÖ Marked sub-job {sub_job_id} as completed")
+        except Exception as e:
+            logger.warning(f"‚ö†Ô∏è Failed to update sub-job: {e}")
+
+        # Create PRODUCTS_CREATED checkpoint after successful creation
+        await checkpoint_recovery_service.create_checkpoint(
+            job_id=job_id,
+            stage=ProcessingStage.PRODUCTS_CREATED,
+            data={
+                "document_id": document_id,
+                "products_created": products_created,
+                "product_ids": product_result.get('product_ids', [])
+            },
+            metadata={
+                "current_step": "Products created successfully",
+                "candidates_detected": product_result.get('candidates_detected', 0),
+                "validation_passed": product_result.get('validation_passed', 0),
+                "timestamp": datetime.utcnow().isoformat()
+            }
+        )
+        logger.info(f"‚úÖ Created PRODUCTS_CREATED checkpoint for job {job_id}")
+
+        # Update job metadata with product count (in-memory)
+        if job_id in job_storage:
+            job_storage[job_id]["progress"] = 100  # Now fully complete
+            if "result" in job_storage[job_id]:
+                job_storage[job_id]["result"]["products_created"] = products_created
+                job_storage[job_id]["result"]["message"] = f"Document processed successfully: {products_created} products created"
+
+        # ‚úÖ FIX: Persist products_created to database background_jobs table
+        try:
+            job_recovery_service = JobRecoveryService(supabase_client)
+
+            # Get current job from database
+            current_job = await job_recovery_service.get_job(job_id)
+            if current_job:
+                # Update metadata with products_created
+                updated_metadata = current_job.get('metadata', {})
+                updated_metadata['products_created'] = products_created
+
+                # Persist updated metadata
+                await job_recovery_service.persist_job(
+                    job_id=job_id,
+                    document_id=document_id,
+                    filename=current_job.get('filename', 'unknown'),
+                    status='completed',
+                    progress=100,
+                    metadata=updated_metadata
+                )
+                logger.info(f"‚úÖ Persisted products_created={products_created} to database for job {job_id}")
+        except Exception as persist_error:
+            logger.error(f"‚ùå Failed to persist products_created to database: {persist_error}")
+
+    except Exception as e:
+        logger.error(f"‚ùå Background product creation failed: {e}", exc_info=True)
+
+        # Mark sub-job as failed
+        try:
+            supabase_client.client.table('background_jobs').update({
+                "status": "failed",
+                "error": str(e),
+                "metadata": {
+                    "workspace_id": workspace_id,
+                    "error_message": str(e),
+                    "failed_at": datetime.utcnow().isoformat()
+                },
+                "updated_at": datetime.utcnow().isoformat()
+            }).eq('id', sub_job_id).execute()
+            logger.info(f"‚úÖ Marked sub-job {sub_job_id} as failed")
+        except Exception as sub_error:
+            logger.warning(f"‚ö†Ô∏è Failed to update sub-job: {sub_error}")
+
+        # Create failed checkpoint
+        try:
+            await checkpoint_recovery_service.create_checkpoint(
+                job_id=job_id,
+                stage=ProcessingStage.PRODUCTS_DETECTED,
+                data={
+                    "document_id": document_id,
+                    "error": str(e),
+                    "failed": True
+                },
+                metadata={
+                    "current_step": "Product creation failed",
+                    "error_message": str(e),
+                    "timestamp": datetime.utcnow().isoformat()
+                }
+            )
+        except:
+            pass  # Don't fail if checkpoint creation fails
+
+
+async def process_images_background(
+    document_id: str,
+    job_id: str,
+    images_count: int
+):
+    """
+    Background task to process image AI analysis with checkpoint support.
+    Runs separately to avoid blocking main PDF processing.
+    Creates a sub-job for tracking.
+    """
+    supabase_client = get_supabase_client()
+    sub_job_id = f"{job_id}_images"
+
+    try:
+        logger.info(f"üñºÔ∏è Starting background image AI analysis for document {document_id}")
+
+        # Create sub-job in database
+        try:
+            supabase_client.client.table('background_jobs').insert({
+                "id": sub_job_id,
+                "parent_job_id": job_id,
+                "job_type": "image_analysis",
+                "document_id": document_id,
+                "status": "processing",
+                "progress": 0,
+                "metadata": {
+                    "images_count": images_count,
+                    "started_at": datetime.utcnow().isoformat()
+                },
+                "created_at": datetime.utcnow().isoformat(),
+                "updated_at": datetime.utcnow().isoformat()
+            }).execute()
+            logger.info(f"‚úÖ Created sub-job {sub_job_id} for image analysis")
+        except Exception as e:
+            logger.warning(f"‚ö†Ô∏è Failed to create sub-job: {e}")
+
+        # Run background image processing
+        from app.services.background_image_processor import start_background_image_processing
+        result = await start_background_image_processing(
+            document_id=document_id,
+            supabase_client=supabase_client
+        )
+
+        images_processed = result.get('total_processed', 0)
+        images_failed = result.get('total_failed', 0)
+        logger.info(f"‚úÖ Background image analysis completed: {images_processed} processed, {images_failed} failed")
+
+        # Update sub-job status to completed
+        try:
+            supabase_client.client.table('background_jobs').update({
+                "status": "completed",
+                "progress": 100,
+                "metadata": {
+                    "images_count": images_count,
+                    "images_processed": images_processed,
+                    "images_failed": images_failed,
+                    "batches_processed": result.get('batches_processed', 0),
+                    "completed_at": datetime.utcnow().isoformat()
+                },
+                "updated_at": datetime.utcnow().isoformat()
+            }).eq('id', sub_job_id).execute()
+            logger.info(f"‚úÖ Marked sub-job {sub_job_id} as completed")
+        except Exception as e:
+            logger.warning(f"‚ö†Ô∏è Failed to update sub-job: {e}")
+
+        # Create IMAGE_ANALYSIS_COMPLETED checkpoint
+        try:
+            await checkpoint_recovery_service.create_checkpoint(
+                job_id=job_id,
+                stage=ProcessingStage.COMPLETED,  # Use COMPLETED since this is the final stage
+                data={
+                    "document_id": document_id,
+                    "images_processed": images_processed,
+                    "images_failed": images_failed
+                },
+                metadata={
+                    "current_step": "Image AI analysis completed",
+                    "images_count": images_count,
+                    "timestamp": datetime.utcnow().isoformat()
+                }
+            )
+            logger.info(f"‚úÖ Created IMAGE_ANALYSIS_COMPLETED checkpoint for job {job_id}")
+        except Exception as e:
+            logger.warning(f"‚ö†Ô∏è Failed to create checkpoint: {e}")
+
+    except Exception as e:
+        logger.error(f"‚ùå Background image analysis failed: {e}", exc_info=True)
+
+        # Mark sub-job as failed
+        try:
+            supabase_client.client.table('background_jobs').update({
+                "status": "failed",
+                "error": str(e),
+                "metadata": {
+                    "images_count": images_count,
+                    "error_message": str(e),
+                    "failed_at": datetime.utcnow().isoformat()
+                },
+                "updated_at": datetime.utcnow().isoformat()
+            }).eq('id', sub_job_id).execute()
+            logger.info(f"‚úÖ Marked sub-job {sub_job_id} as failed")
+        except Exception as sub_error:
+            logger.warning(f"‚ö†Ô∏è Failed to update sub-job: {sub_error}")
+
+
+async def process_document_background(
+    job_id: str,
+    document_id: str,
+    file_content: bytes,
+    filename: str,
+    title: Optional[str],
+    description: Optional[str],
+    document_tags: List[str],
+    chunk_size: int,
+    chunk_overlap: int,
+    llamaindex_service: Optional[LlamaIndexService] = None
+):
+    """
+    Background task to process document with checkpoint recovery support.
+    """
+    start_time = datetime.utcnow()
+
+    logger.info(f"üìã BACKGROUND JOB STARTED: {job_id}")
+    logger.info(f"   Document ID: {document_id}")
+    logger.info(f"   Filename: {filename}")
+    logger.info(f"   Started at: {start_time.isoformat()}")
+
+    # Get LlamaIndex service from app state if not provided
+    if llamaindex_service is None:
+        try:
+            from app.main import app
+            if hasattr(app.state, 'llamaindex_service'):
+                llamaindex_service = app.state.llamaindex_service
+                logger.info("‚úÖ Retrieved LlamaIndex service from app state")
+            else:
+                logger.error("‚ùå LlamaIndex service not available in app state")
+                job_storage[job_id]["status"] = "failed"
+                job_storage[job_id]["error"] = "LlamaIndex service not available"
+                return
+        except Exception as e:
+            logger.error(f"‚ùå Failed to get LlamaIndex service: {e}")
+            job_storage[job_id]["status"] = "failed"
+            job_storage[job_id]["error"] = str(e)
+            return
+
+    # Check for existing checkpoint to resume from
+    last_checkpoint = None
+    resume_from_stage = None
+    try:
+        last_checkpoint = await checkpoint_recovery_service.get_last_checkpoint(job_id)
+        if last_checkpoint:
+            resume_from_stage = last_checkpoint.get('stage')
+            logger.info(f"üîÑ RESUMING FROM CHECKPOINT: {resume_from_stage}")
+            logger.info(f"   Checkpoint created at: {last_checkpoint.get('created_at')}")
+            logger.info(f"   Checkpoint data: {last_checkpoint.get('checkpoint_data', {}).keys()}")
+
+            # Verify checkpoint data exists in database
+            can_resume = await checkpoint_recovery_service.verify_checkpoint_data(job_id, resume_from_stage)
+            if not can_resume:
+                logger.warning(f"‚ö†Ô∏è Checkpoint data verification failed, starting from scratch")
+                resume_from_stage = None
+                last_checkpoint = None
+        else:
+            logger.info("üìù No checkpoint found, starting fresh processing")
+    except Exception as e:
+        logger.error(f"Failed to check for checkpoint: {e}", exc_info=True)
+        resume_from_stage = None
+
+    try:
+        # Create placeholder document record FIRST (required for foreign key constraint)
+        supabase_client = get_supabase_client()
+
+        # Skip document creation if resuming from checkpoint
+        if not resume_from_stage:
+            try:
+                supabase_client.client.table('documents').insert({
+                    "id": document_id,
+                    "workspace_id": "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
+                    "filename": filename,
+                    "content_type": "application/pdf",
+                    "file_size": len(file_content),
+                    "processing_status": "processing",
+                    "metadata": {
+                        "title": title or filename,
+                        "description": description,
+                        "tags": document_tags,
+                        "source": "rag_upload_async"
+                    },
+                    "created_at": start_time.isoformat(),
+                    "updated_at": start_time.isoformat()
+                }).execute()
+                logger.info(f"‚úÖ Created placeholder document record: {document_id}")
+            except Exception as doc_error:
+                logger.error(f"Failed to create document record: {doc_error}")
+                # Continue anyway - the document might already exist
+
+            # Create INITIALIZED checkpoint
+            await checkpoint_recovery_service.create_checkpoint(
+                job_id=job_id,
+                stage=ProcessingStage.INITIALIZED,
+                data={
+                    "document_id": document_id,
+                    "filename": filename,
+                    "file_size": len(file_content)
+                },
+                metadata={
+                    "title": title or filename,
+                    "description": description,
+                    "tags": document_tags
+                }
+            )
+            logger.info(f"‚úÖ Created INITIALIZED checkpoint for job {job_id}")
+
+        # Update status (in-memory)
+        job_storage[job_id]["status"] = "processing"
+        job_storage[job_id]["started_at"] = start_time.isoformat()
+        job_storage[job_id]["document_id"] = document_id
+        job_storage[job_id]["progress"] = 10
+
+        # Persist status change to database (now document exists)
+        if job_recovery_service:
+            await job_recovery_service.persist_job(
+                job_id=job_id,
+                document_id=document_id,
+                filename=filename,
+                status="processing",
+                progress=10
+            )
+
+        # Initialize AI model tracker for this job
+        ai_tracker = AIModelTracker(job_id)
+        job_storage[job_id]["ai_tracker"] = ai_tracker
+
+        # Define progress callback to update job progress with detailed metadata
+        async def update_progress(progress: int, details: dict = None):
+            """Update job progress in memory and database with detailed stats and AI tracking"""
+            job_storage[job_id]["progress"] = progress
+
+            # Build detailed metadata
+            detailed_metadata = {
+                "document_id": document_id,  # ‚úÖ FIX 1: Add document_id
+                "filename": filename,
+                "workspace_id": "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
+                "title": title or filename,
+                "description": description,
+                "tags": document_tags,
+                "source": "rag_upload_async"
+            }
+
+            # ‚úÖ FIX 2: Add detailed progress stats
+            if details:
+                detailed_metadata.update({
+                    "current_page": details.get("current_page"),
+                    "total_pages": details.get("total_pages"),
+                    "chunks_created": details.get("chunks_created", 0),
+                    "images_extracted": details.get("images_extracted", 0),
+                    "products_created": details.get("products_created", 0),
+                    "ai_usage": {
+                        "llama_calls": details.get("llama_calls", 0),
+                        "claude_calls": details.get("claude_calls", 0),
+                        "openai_calls": details.get("openai_calls", 0),
+                        "clip_embeddings": details.get("clip_embeddings", 0)
+                    },
+                    "embeddings_generated": {
+                        "text": details.get("text_embeddings", 0),
+                        "visual": details.get("visual_embeddings", 0),
+                        "color": details.get("color_embeddings", 0),
+                        "texture": details.get("texture_embeddings", 0),
+                        "application": details.get("application_embeddings", 0)
+                    },
+                    "current_step": details.get("current_step", "Processing")
+                })
+
+                # ‚úÖ Log AI model calls if provided
+                if details.get("ai_model_call"):
+                    ai_call = details["ai_model_call"]
+                    ai_tracker.log_model_call(
+                        model_name=ai_call.get("model_name", "Unknown"),
+                        stage=ai_call.get("stage", "unknown"),
+                        task=ai_call.get("task", "unknown"),
+                        latency_ms=ai_call.get("latency_ms", 0),
+                        confidence_score=ai_call.get("confidence_score"),
+                        result_summary=ai_call.get("result_summary"),
+                        items_processed=ai_call.get("items_processed", 0),
+                        input_tokens=ai_call.get("input_tokens"),
+                        output_tokens=ai_call.get("output_tokens"),
+                        success=ai_call.get("success", True),
+                        error=ai_call.get("error")
+                    )
+
+            # Add AI tracker summary to metadata
+            detailed_metadata["ai_tracking"] = ai_tracker.format_for_metadata()
+
+            # ‚úÖ FIX: Store metadata in job_storage so it's returned by get_job_status
+            job_storage[job_id]["metadata"] = detailed_metadata
+
+            if job_recovery_service:
+                await job_recovery_service.persist_job(
+                    job_id=job_id,
+                    document_id=document_id,
+                    filename=filename,
+                    status="processing",
+                    progress=progress,
+                    metadata=detailed_metadata
+                )
+            logger.info(f"üìä Job {job_id} progress: {progress}% - {detailed_metadata.get('current_step', 'Processing')}")
+
+
+
+        # Process document through LlamaIndex service with progress tracking and granular checkpoints
+        # Skip if resuming from a later checkpoint
+        if not resume_from_stage or resume_from_stage == ProcessingStage.INITIALIZED:
+            # Enhanced progress callback that creates checkpoints at each stage
+            async def enhanced_progress_callback(progress: int, details: dict = None):
+                """Enhanced progress callback with checkpoint creation"""
+                await update_progress(progress, details)
+
+                # Create checkpoints at specific progress milestones
+                current_step = details.get('current_step', '') if details else ''
+
+                # PDF_EXTRACTED checkpoint (after PDF extraction - 20%)
+                if progress == 20 and 'Extracting text and images' in current_step:
+                    await checkpoint_recovery_service.create_checkpoint(
+                        job_id=job_id,
+                        stage=ProcessingStage.PDF_EXTRACTED,
+                        data={
+                            "document_id": document_id,
+                            "total_pages": details.get('total_pages', 0) if details else 0
+                        },
+                        metadata={
+                            "current_step": current_step,
+                            "timestamp": datetime.utcnow().isoformat()
+                        }
+                    )
+                    logger.info(f"‚úÖ Created PDF_EXTRACTED checkpoint for job {job_id}")
+
+                # CHUNKS_CREATED checkpoint (after chunking - 40%)
+                elif progress == 40 and 'Creating semantic chunks' in current_step:
+                    await checkpoint_recovery_service.create_checkpoint(
+                        job_id=job_id,
+                        stage=ProcessingStage.CHUNKS_CREATED,
+                        data={
+                            "document_id": document_id,
+                            "total_pages": details.get('total_pages', 0) if details else 0,
+                            "images_extracted": details.get('images_extracted', 0) if details else 0
+                        },
+                        metadata={
+                            "current_step": current_step,
+                            "timestamp": datetime.utcnow().isoformat()
+                        }
+                    )
+                    logger.info(f"‚úÖ Created CHUNKS_CREATED checkpoint for job {job_id}")
+
+                # TEXT_EMBEDDINGS_GENERATED checkpoint (after embeddings - 60%)
+                elif progress == 60 and 'Generating embeddings' in current_step:
+                    await checkpoint_recovery_service.create_checkpoint(
+                        job_id=job_id,
+                        stage=ProcessingStage.TEXT_EMBEDDINGS_GENERATED,
+                        data={
+                            "document_id": document_id,
+                            "chunks_created": details.get('chunks_created', 0) if details else 0,
+                            "total_pages": details.get('total_pages', 0) if details else 0,
+                            "images_extracted": details.get('images_extracted', 0) if details else 0
+                        },
+                        metadata={
+                            "current_step": current_step,
+                            "text_embeddings": details.get('text_embeddings', 0) if details else 0,
+                            "openai_calls": details.get('openai_calls', 0) if details else 0,
+                            "timestamp": datetime.utcnow().isoformat()
+                        }
+                    )
+                    logger.info(f"‚úÖ Created TEXT_EMBEDDINGS_GENERATED checkpoint for job {job_id}")
+
+                # IMAGES_EXTRACTED checkpoint (after image processing - 80%)
+                elif progress == 80 and 'Processing images' in current_step:
+                    await checkpoint_recovery_service.create_checkpoint(
+                        job_id=job_id,
+                        stage=ProcessingStage.IMAGES_EXTRACTED,
+                        data={
+                            "document_id": document_id,
+                            "chunks_created": details.get('chunks_created', 0) if details else 0,
+                            "images_extracted": details.get('images_extracted', 0) if details else 0
+                        },
+                        metadata={
+                            "current_step": current_step,
+                            "timestamp": datetime.utcnow().isoformat()
+                        }
+                    )
+                    logger.info(f"‚úÖ Created IMAGES_EXTRACTED checkpoint for job {job_id}")
+
+            processing_result = await llamaindex_service.index_document_content(
+                file_content=file_content,
+                document_id=document_id,
+                file_path=filename,
+                metadata={
+                    "workspace_id": "ffafc28b-1b8b-4b0d-b226-9f9a6154004e",  # Default workspace UUID
+                    "filename": filename,
+                    "title": title or filename,
+                    "description": description,
+                    "tags": document_tags,
+                    "source": "rag_upload_async"
+                },
+                chunk_size=chunk_size,
+                chunk_overlap=chunk_overlap,
+                progress_callback=sync_progress_callback
+            )
+
+            # Create IMAGE_EMBEDDINGS_GENERATED checkpoint after CLIP embeddings
+            chunks_created = processing_result.get('statistics', {}).get('total_chunks', 0)
+            images_extracted = processing_result.get('statistics', {}).get('images_extracted', 0)
+            clip_embeddings = processing_result.get('statistics', {}).get('clip_embeddings_generated', 0)
+
+            if clip_embeddings > 0:
+                await checkpoint_recovery_service.create_checkpoint(
+                    job_id=job_id,
+                    stage=ProcessingStage.IMAGE_EMBEDDINGS_GENERATED,
+                    data={
+                        "document_id": document_id,
+                        "chunks_created": chunks_created,
+                        "images_extracted": images_extracted,
+                        "clip_embeddings": clip_embeddings
+                    },
+                    metadata={
+                        "current_step": "CLIP embeddings generated",
+                        "timestamp": datetime.utcnow().isoformat()
+                    }
+                )
+                logger.info(f"‚úÖ Created IMAGE_EMBEDDINGS_GENERATED checkpoint for job {job_id}")
+
+            if processing_result.get('status') == 'success':
+                # Create COMPLETED checkpoint with all processing data
+                await checkpoint_recovery_service.create_checkpoint(
+                    job_id=job_id,
+                    stage=ProcessingStage.COMPLETED,
+                    data={
+                        "document_id": document_id,
+                        "chunks_created": chunks_created,
+                        "images_extracted": images_extracted,
+                        "embeddings_generated": processing_result.get('statistics', {}).get('database_embeddings_stored', 0),
+                        "clip_embeddings": clip_embeddings
+                    },
+                    metadata={
+                        "processing_time": (datetime.utcnow() - start_time).total_seconds(),
+                        "statistics": processing_result.get('statistics', {})
+                    }
+                )
+                logger.info(f"‚úÖ Created COMPLETED checkpoint for job {job_id}")
+        else:
+            # Resuming from checkpoint - retrieve data from last checkpoint
+            logger.info(f"‚è≠Ô∏è Skipping document processing - resuming from {resume_from_stage}")
+            checkpoint_data = last_checkpoint.get('checkpoint_data', {})
+            processing_result = {
+                'status': 'success',
+                'statistics': {
+                    'total_chunks': checkpoint_data.get('chunks_created', 0),
+                    'images_extracted': checkpoint_data.get('images_extracted', 0),
+                    'database_embeddings_stored': checkpoint_data.get('embeddings_generated', 0),
+                    'clip_embeddings_generated': checkpoint_data.get('clip_embeddings', 0)
+                }
+            }
+
+        processing_time = (datetime.utcnow() - start_time).total_seconds()
+
+        # Check if processing succeeded
+        result_status = processing_result.get('status', 'completed')
+        chunks_created = processing_result.get('statistics', {}).get('total_chunks', 0)
+
+        if result_status == 'error':
+            error_message = processing_result.get('error', 'Unknown error during document processing')
+            job_storage[job_id]["status"] = "failed"
+            job_storage[job_id]["error"] = error_message
+            job_storage[job_id]["progress"] = 100
+        else:
+            # ‚úÖ FIX 3: Run product creation in background to prevent timeout
+            # Mark main job as 90% complete, product creation runs separately
+            products_created = 0
+
+            if chunks_created > 0:
+                logger.info(f"üè≠ Scheduling background product creation for document {document_id}")
+                # Start product creation in background (don't await)
+                import asyncio
+                asyncio.create_task(create_products_background(
+                    document_id=document_id,
+                    workspace_id="ffafc28b-1b8b-4b0d-b226-9f9a6154004e",
+                    job_id=job_id
+                ))
+                logger.info("‚úÖ Product creation scheduled in background")
+            else:
+                logger.info("‚ö†Ô∏è No chunks created, skipping product creation")
+
+            # ‚úÖ FIX 4: Start background image processing with sub-job tracking
+            images_extracted = processing_result.get('statistics', {}).get('total_images', 0)
+            if images_extracted > 0:
+                logger.info(f"üñºÔ∏è Scheduling background image AI analysis for {images_extracted} images")
+                asyncio.create_task(process_images_background(
+                    document_id=document_id,
+                    job_id=job_id,
+                    images_count=images_extracted
+                ))
+                logger.info("‚úÖ Image AI analysis scheduled in background")
+            else:
+                logger.info("‚ö†Ô∏è No images extracted, skipping background image analysis")
+
+            job_storage[job_id]["status"] = "completed"
+            job_storage[job_id]["progress"] = 90  # Main processing complete, products/images running in background
+            job_storage[job_id]["completed_at"] = datetime.utcnow().isoformat()
+            job_storage[job_id]["result"] = {
+                "document_id": document_id,
+                "title": title or filename,
+                "status": result_status,
+                "chunks_created": chunks_created,
+                "embeddings_generated": chunks_created > 0,
+                "products_created": products_created,
+                "processing_time": processing_time,
+                "message": f"Document processed successfully: {chunks_created} chunks created, {products_created} products created"
+            }
+
+            # Persist completion to database
+            if job_recovery_service:
+                await job_recovery_service.persist_job(
+                    job_id=job_id,
+                    document_id=document_id,
+                    filename=filename,
+                    status="completed",
+                    progress=100,
+                    metadata={
+                        "chunks_created": chunks_created,
+                        "products_created": products_created,
+                        "processing_time": processing_time
+                    }
+                )
+
+    except asyncio.CancelledError:
+        # Job was cancelled (likely due to service shutdown)
+        logger.error(f"üõë JOB INTERRUPTED: {job_id}")
+        logger.error(f"   Document ID: {document_id}")
+        logger.error(f"   Filename: {filename}")
+        logger.error(f"   Reason: Service shutdown or task cancellation")
+        logger.error(f"   Duration before interruption: {(datetime.utcnow() - start_time).total_seconds():.2f}s")
+
+        job_storage[job_id]["status"] = "interrupted"
+        job_storage[job_id]["error"] = "Job interrupted due to service shutdown"
+        job_storage[job_id]["progress"] = job_storage[job_id].get("progress", 0)
+        job_storage[job_id]["interrupted_at"] = datetime.utcnow().isoformat()
+
+        # Persist interruption to database
+        if job_recovery_service:
+            await job_recovery_service.mark_job_interrupted(
+                job_id=job_id,
+                reason="Service shutdown or task cancellation"
+            )
+
+        # Re-raise to allow proper cleanup
+        raise
+
+    except Exception as e:
+        logger.error(f"‚ùå BACKGROUND JOB FAILED: {job_id}")
+        logger.error(f"   Document ID: {document_id}")
+        logger.error(f"   Error: {e}", exc_info=True)
+        logger.error(f"   Duration before failure: {(datetime.utcnow() - start_time).total_seconds():.2f}s")
+
+        job_storage[job_id]["status"] = "failed"
+        job_storage[job_id]["error"] = str(e)
+        job_storage[job_id]["progress"] = 100
+        job_storage[job_id]["failed_at"] = datetime.utcnow().isoformat()
+
+        # Persist failure to database
+        if job_recovery_service:
+            await job_recovery_service.persist_job(
+                job_id=job_id,
+                document_id=document_id,
+                filename=filename,
+                status="failed",
+                progress=100,
+                error=str(e)
+            )
+
+    finally:
+        end_time = datetime.utcnow()
+        total_duration = (end_time - start_time).total_seconds()
+        final_status = job_storage[job_id].get("status", "unknown")
+
+        logger.info(f"üìã BACKGROUND JOB FINISHED: {job_id}")
+        logger.info(f"   Final status: {final_status}")
+        logger.info(f"   Total duration: {total_duration:.2f}s")
+        logger.info(f"   Ended at: {end_time.isoformat()}")
+
+
+@router.post("/query", response_model=QueryResponse)
+async def query_documents(
+    request: QueryRequest,
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Query documents using RAG (Retrieval-Augmented Generation).
+    
+    This endpoint performs semantic search over the document collection
+    and generates contextual answers using the retrieved information.
+    """
+    start_time = datetime.utcnow()
+    
+    try:
+        # Perform RAG query using advanced_rag_query
+        result = await llamaindex_service.advanced_rag_query(
+            query=request.query,
+            max_results=request.top_k,
+            similarity_threshold=request.similarity_threshold,
+            enable_reranking=request.enable_reranking,
+            query_type="factual"
+        )
+        
+        processing_time = (datetime.utcnow() - start_time).total_seconds()
+        
+        return QueryResponse(
+            query=request.query,
+            answer=result.get('answer', ''),
+            sources=result.get('sources', []),
+            confidence_score=result.get('confidence_score', 0.0),
+            processing_time=processing_time,
+            retrieved_chunks=len(result.get('sources', []))
+        )
+        
+    except Exception as e:
+        logger.error(f"Query processing failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Query processing failed: {str(e)}"
+        )
+
+@router.post("/chat", response_model=ChatResponse)
+async def chat_with_documents(
+    request: ChatRequest,
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Conversational interface for document Q&A.
+    
+    This endpoint maintains conversation context and provides
+    contextual responses based on the document collection.
+    """
+    start_time = datetime.utcnow()
+    
+    try:
+        # Generate conversation ID if not provided
+        conversation_id = request.conversation_id or str(uuid4())
+        
+        # Process chat message using advanced_rag_query
+        result = await llamaindex_service.advanced_rag_query(
+            query=request.message,
+            max_results=request.top_k,
+            query_type="conversational"
+        )
+        
+        processing_time = (datetime.utcnow() - start_time).total_seconds()
+        
+        return ChatResponse(
+            message=request.message,
+            response=result.get('response', ''),
+            conversation_id=conversation_id,
+            sources=result.get('sources', []),
+            processing_time=processing_time
+        )
+        
+    except Exception as e:
+        logger.error(f"Chat processing failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Chat processing failed: {str(e)}"
+        )
+
+@router.post("/search", response_model=SearchResponse)
+async def search_documents(
+    request: SearchRequest,
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Semantic search across document collection.
+    
+    This endpoint provides various search capabilities including
+    semantic, hybrid, and keyword search.
+    """
+    start_time = datetime.utcnow()
+    
+    try:
+        # Perform search using semantic_search_with_mmr
+        results = await llamaindex_service.semantic_search_with_mmr(
+            query=request.query,
+            k=request.top_k,
+            lambda_mult=0.5  # Default MMR parameter
+        )
+        
+        processing_time = (datetime.utcnow() - start_time).total_seconds()
+        
+        return SearchResponse(
+            query=request.query,
+            results=results.get('results', []),
+            total_results=results.get('total_results', 0),
+            search_type=request.search_type,
+            processing_time=processing_time
+        )
+        
+    except Exception as e:
+        logger.error(f"Search processing failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Search processing failed: {str(e)}"
+        )
+
+@router.get("/documents/documents/{document_id}/content")
+async def get_document_content(
+    document_id: str,
+    include_chunks: bool = Query(True, description="Include document chunks"),
+    include_images: bool = Query(True, description="Include document images"),
+    include_products: bool = Query(False, description="Include products created from document")
+):
+    """
+    Get complete document content with all AI analysis results.
+
+    Returns comprehensive document data including:
+    - Document metadata
+    - All chunks with embeddings
+    - All images with AI analysis (CLIP, Llama, Claude)
+    - All products created from the document
+    - Complete AI model usage statistics
+    """
+    try:
+        logger.info(f"üìä Fetching complete content for document {document_id}")
+        supabase_client = get_supabase_client()
+
+        # Get document metadata
+        doc_response = supabase_client.client.table('documents').select('*').eq('id', document_id).execute()
+        if not doc_response.data or len(doc_response.data) == 0:
+            raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
+
+        document = doc_response.data[0]
+        result = {
+            "id": document['id'],
+            "created_at": document['created_at'],
+            "metadata": document.get('metadata', {}),
+            "chunks": [],
+            "images": [],
+            "products": [],
+            "statistics": {}
+        }
+
+        # Get chunks with embeddings
+        if include_chunks:
+            logger.info(f"üìÑ Fetching chunks for document {document_id}")
+            chunks_response = supabase_client.client.table('document_chunks').select('*').eq('document_id', document_id).execute()
+            chunks = chunks_response.data or []
+
+            # Get embeddings for each chunk
+            for chunk in chunks:
+                embeddings_response = supabase_client.client.table('embeddings').select('*').eq('chunk_id', chunk['id']).execute()
+                chunk['embeddings'] = embeddings_response.data or []
+
+            result['chunks'] = chunks
+            logger.info(f"‚úÖ Fetched {len(chunks)} chunks")
+
+        # Get images with AI analysis
+        if include_images:
+            logger.info(f"üñºÔ∏è Fetching images for document {document_id}")
+            images_response = supabase_client.client.table('document_images').select('*').eq('document_id', document_id).execute()
+            result['images'] = images_response.data or []
+            logger.info(f"‚úÖ Fetched {len(result['images'])} images")
+
+        # Get products
+        if include_products:
+            logger.info(f"üè≠ Fetching products for document {document_id}")
+            products_response = supabase_client.client.table('products').select('*').eq('source_document_id', document_id).execute()
+            result['products'] = products_response.data or []
+            logger.info(f"‚úÖ Fetched {len(result['products'])} products")
+
+        # Calculate statistics
+        chunks_count = len(result['chunks'])
+        images_count = len(result['images'])
+        products_count = len(result['products'])
+
+        # Count embeddings
+        text_embeddings = sum(1 for chunk in result['chunks'] if chunk.get('embeddings'))
+        clip_embeddings = sum(1 for img in result['images'] if img.get('visual_clip_embedding_512'))
+        llama_analysis = sum(1 for img in result['images'] if img.get('llama_analysis'))
+        claude_validation = sum(1 for img in result['images'] if img.get('claude_validation'))
+        color_embeddings = sum(1 for img in result['images'] if img.get('color_embedding_256'))
+        texture_embeddings = sum(1 for img in result['images'] if img.get('texture_embedding_256'))
+        application_embeddings = sum(1 for img in result['images'] if img.get('application_embedding_512'))
+
+        result['statistics'] = {
+            "chunks_count": chunks_count,
+            "images_count": images_count,
+            "products_count": products_count,
+            "ai_usage": {
+                "openai_calls": text_embeddings,
+                "llama_calls": llama_analysis,
+                "claude_calls": claude_validation,
+                "clip_embeddings": clip_embeddings
+            },
+            "embeddings_generated": {
+                "text": text_embeddings,
+                "visual": clip_embeddings,
+                "color": color_embeddings,
+                "texture": texture_embeddings,
+                "application": application_embeddings,
+                "total": text_embeddings + clip_embeddings + color_embeddings + texture_embeddings + application_embeddings
+            },
+            "completion_rates": {
+                "text_embeddings": f"{(text_embeddings / chunks_count * 100) if chunks_count > 0 else 0:.1f}%",
+                "image_analysis": f"{(clip_embeddings / images_count * 100) if images_count > 0 else 0:.1f}%"
+            }
+        }
+
+        logger.info(f"‚úÖ Document content fetched successfully: {chunks_count} chunks, {images_count} images, {products_count} products")
+        return JSONResponse(content=result)
+
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"‚ùå Error fetching document content: {e}", exc_info=True)
+        raise HTTPException(status_code=500, detail=f"Error fetching document content: {str(e)}")
+
+
+@router.get("/documents", response_model=DocumentListResponse)
+async def list_documents(
+    page: int = Query(1, ge=1, description="Page number"),
+    page_size: int = Query(20, ge=1, le=100, description="Page size"),
+    search: Optional[str] = Query(None, description="Search term for filtering"),
+    tags: Optional[str] = Query(None, description="Comma-separated tags for filtering"),
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    List and filter documents in the collection.
+
+    This endpoint provides paginated access to the document collection
+    with optional filtering by search terms and tags.
+    """
+    try:
+        # Parse tags filter
+        tag_filter = []
+        if tags:
+            tag_filter = [tag.strip() for tag in tags.split(',')]
+        
+        # Get documents using list_indexed_documents
+        result = await llamaindex_service.list_indexed_documents()
+        
+        return DocumentListResponse(
+            documents=result.get('documents', []),
+            total_count=result.get('total_count', 0),
+            page=page,
+            page_size=page_size
+        )
+        
+    except Exception as e:
+        logger.error(f"Document listing failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Document listing failed: {str(e)}"
+        )
+
+@router.delete("/documents/{document_id}")
+async def delete_document(
+    document_id: str,
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Delete a document and its associated embeddings.
+    
+    This endpoint removes a document from the collection and
+    cleans up all associated data including embeddings and chunks.
+    """
+    try:
+        # Delete document
+        result = await llamaindex_service.delete_document(document_id)
+        
+        if not result.get('success', False):
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail="Document not found"
+            )
+        
+        return JSONResponse(
+            status_code=status.HTTP_200_OK,
+            content={
+                "message": "Document deleted successfully",
+                "document_id": document_id,
+                "chunks_deleted": result.get('chunks_deleted', 0)
+            }
+        )
+        
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Document deletion failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Document deletion failed: {str(e)}"
+        )
+
+@router.get("/health", response_model=HealthCheckResponse)
+async def rag_health_check(
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Health check for RAG services.
+    
+    This endpoint checks the health of all RAG-related services
+    including LlamaIndex, embedding service, and vector store.
+    """
+    try:
+        # Check LlamaIndex service health
+        llamaindex_health = await llamaindex_service.health_check()
+
+        # Try to check embedding service health (optional)
+        embedding_health = {"status": "unknown", "message": "Embedding service not available"}
+        try:
+            embedding_service = await get_embedding_service()
+            embedding_health = await embedding_service.health_check()
+        except Exception as e:
+            logger.warning(f"Embedding service health check failed: {e}")
+            embedding_health = {"status": "error", "error": str(e)}
+
+        # Determine overall status
+        overall_status = "healthy"
+        if llamaindex_health.get("status") != "healthy":
+            overall_status = "degraded"
+
+        return HealthCheckResponse(
+            status=overall_status,
+            services={
+                "llamaindex": llamaindex_health,
+                "embedding": embedding_health
+            },
+            timestamp=datetime.utcnow().isoformat()
+        )
+        
+    except Exception as e:
+        logger.error(f"RAG health check failed: {e}", exc_info=True)
+        return HealthCheckResponse(
+            status="unhealthy",
+            services={
+                "llamaindex": {"status": "error", "error": str(e)},
+                "embedding": {"status": "unknown"}
+            },
+            timestamp=datetime.utcnow().isoformat()
+        )
+
+@router.get("/stats")
+async def get_rag_statistics(
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Get RAG system statistics.
+    
+    This endpoint provides statistics about the RAG system including
+    document counts, embedding statistics, and performance metrics.
+    """
+    try:
+        # Get available statistics from the service
+        memory_stats = llamaindex_service.get_memory_stats()
+        health_check = await llamaindex_service.health_check()
+
+        # Combine statistics
+        stats = {
+            "memory": memory_stats,
+            "health": health_check,
+            "indices_count": len(llamaindex_service.indices),
+            "storage_dir": llamaindex_service.storage_dir,
+            "embedding_model": llamaindex_service.embedding_model,
+            "llm_model": llamaindex_service.llm_model
+        }
+
+        return JSONResponse(
+            status_code=status.HTTP_200_OK,
+            content={
+                "status": "success",
+                "statistics": stats,
+                "timestamp": datetime.utcnow().isoformat()
+            }
+        )
+        
+    except Exception as e:
+        logger.error(f"Statistics retrieval failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Statistics retrieval failed: {str(e)}"
+        )
+
+# Advanced Search Endpoints for Phase 7 Features
+
+@router.post("/search/mmr", response_model=MMRSearchResponse)
+async def mmr_search(
+    request: MMRSearchRequest,
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Perform MMR (Maximal Marginal Relevance) search for diverse results.
+    
+    This endpoint implements MMR search to provide diverse, non-redundant results
+    by balancing relevance and diversity using the lambda parameter.
+    """
+    try:
+        start_time = datetime.utcnow()
+        
+        # Call the MMR search method from LlamaIndex service
+        results = await llamaindex_service.semantic_search_with_mmr(
+            query=request.query,
+            top_k=request.top_k,
+            diversity_threshold=request.diversity_threshold,
+            lambda_param=request.lambda_param,
+            document_ids=request.document_ids,
+            include_metadata=request.include_metadata
+        )
+        
+        processing_time = (datetime.utcnow() - start_time).total_seconds()
+        
+        return MMRSearchResponse(
+            query=request.query,
+            results=results.get('results', []),
+            total_results=results.get('total_results', 0),
+            diversity_score=results.get('diversity_score', 0.0),
+            processing_time=processing_time
+        )
+        
+    except Exception as e:
+        logger.error(f"MMR search failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"MMR search failed: {str(e)}"
+        )
+
+@router.post("/search/advanced", response_model=AdvancedQueryResponse)
+async def advanced_query_search(
+    request: AdvancedQueryRequest,
+    llamaindex_service: LlamaIndexService = Depends(get_llamaindex_service)
+):
+    """
+    Perform advanced query search with optimization and expansion.
+    
+    This endpoint provides advanced query processing including query expansion,
+    rewriting, and optimization based on query type and search parameters.
+    """
+    try:
+        start_time = datetime.utcnow()
+        
+        # Convert string enums to proper enum types
+        query_type = QueryType(request.query_type.upper())
+        search_operator = SearchOperator(request.search_operator.upper())
+        
+        # Call the advanced query method from LlamaIndex service
+        results = await llamaindex_service.advanced_query_with_optimization(
+            query=request.query,
+            query_type=query_type,
+            top_k=request.top_k,
+            enable_expansion=request.enable_expansion,
+            enable_rewriting=request.enable_rewriting,
+            similarity_threshold=request.similarity_threshold,
+            document_ids=request.document_ids,
+            metadata_filters=request.metadata_filters,
+            search_operator=search_operator
+        )
+
+        processing_time = (datetime.utcnow() - start_time).total_seconds()
+
+        return AdvancedQueryResponse(
+            query=request.query,
+            results=results.get('results', []),
+            total_results=results.get('total_results', 0),
+            processing_time=processing_time,
+            query_type=request.query_type,
+            optimizations_applied=results.get('optimizations_applied', [])
+        )
+
+    except Exception as e:
+        logger.error(f"Advanced query search failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Advanced query search failed: {str(e)}"
+        )
+
+
+@router.get("/job/{job_id}/ai-tracking")
+async def get_job_ai_tracking(job_id: str):
+    """
+    Get detailed AI model tracking information for a job.
+
+    Returns comprehensive metrics on:
+    - Which AI models were used (LLAMA, Anthropic, CLIP, OpenAI)
+    - Confidence scores and results
+    - Token usage and processing time
+    - Success/failure rates
+    - Per-stage breakdown
+    """
+    try:
+        if job_id not in job_storage:
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail=f"Job {job_id} not found"
+            )
+
+        job_info = job_storage[job_id]
+        ai_tracker = job_info.get("ai_tracker")
+
+        if not ai_tracker:
+            return {
+                "job_id": job_id,
+                "message": "No AI tracking data available for this job",
+                "status": job_info.get("status", "unknown")
+            }
+
+        # Get comprehensive summary
+        summary = ai_tracker.get_job_summary()
+
+        return {
+            "job_id": job_id,
+            "status": job_info.get("status", "processing"),
+            "progress": job_info.get("progress", 0),
+            "ai_tracking": summary,
+            "metadata": job_info.get("metadata", {})
+        }
+
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Failed to get AI tracking for job {job_id}: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Failed to get AI tracking: {str(e)}"
+        )
+
+
+@router.get("/job/{job_id}/ai-tracking/stage/{stage}")
+async def get_job_ai_tracking_by_stage(job_id: str, stage: str):
+    """
+    Get AI model tracking information for a specific processing stage.
+
+    Args:
+        job_id: Job identifier
+        stage: Processing stage (classification, boundary_detection, embedding, etc.)
+
+    Returns:
+        Detailed metrics for the specified stage
+    """
+    try:
+        if job_id not in job_storage:
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail=f"Job {job_id} not found"
+            )
+
+        job_info = job_storage[job_id]
+        ai_tracker = job_info.get("ai_tracker")
+
+        if not ai_tracker:
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail="No AI tracking data available for this job"
+            )
+
+        stage_details = ai_tracker.get_stage_details(stage)
+
+        if not stage_details:
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail=f"No tracking data for stage: {stage}"
+            )
+
+        return stage_details
+
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Failed to get AI tracking for job {job_id} stage {stage}: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Failed to get AI tracking: {str(e)}"
+        )
+
+
+@router.get("/job/{job_id}/ai-tracking/model/{model_name}")
+async def get_job_ai_tracking_by_model(job_id: str, model_name: str):
+    """
+    Get AI model tracking information for a specific AI model.
+
+    Args:
+        job_id: Job identifier
+        model_name: AI model name (LLAMA, Anthropic, CLIP, OpenAI)
+
+    Returns:
+        Statistics for the specified AI model
+    """
+    try:
+        if job_id not in job_storage:
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail=f"Job {job_id} not found"
+            )
+
+        job_info = job_storage[job_id]
+        ai_tracker = job_info.get("ai_tracker")
+
+        if not ai_tracker:
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail="No AI tracking data available for this job"
+            )
+
+        model_stats = ai_tracker.get_model_stats(model_name)
+
+        if not model_stats:
+            raise HTTPException(
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail=f"No tracking data for model: {model_name}"
+            )
+
+        return model_stats
+
+    except HTTPException:
+        raise
+    except Exception as e:
+        logger.error(f"Failed to get AI tracking for job {job_id} model {model_name}: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Failed to get AI tracking: {str(e)}"
+        )
+        
+        processing_time = (datetime.utcnow() - start_time).total_seconds()
+        
+        return AdvancedQueryResponse(
+            original_query=request.query,
+            optimized_query=results.get('optimized_query', request.query),
+            query_type=request.query_type,
+            results=results.get('results', []),
+            total_results=results.get('total_results', 0),
+            expansion_terms=results.get('expansion_terms', []),
+            processing_time=processing_time,
+            confidence_score=results.get('confidence_score', 0.0)
+        )
+        
+    except ValueError as e:
+        logger.error(f"Invalid query parameters: {e}")
+        raise HTTPException(
+            status_code=status.HTTP_400_BAD_REQUEST,
+            detail=f"Invalid query parameters: {str(e)}"
+        )
+    except Exception as e:
+        logger.error(f"Advanced query search failed: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Advanced query search failed: {str(e)}"
+        )
\ No newline at end of file
diff --git a/app/main.py b/app/main.py
index 6eb4254..a2b23e5 100644
--- a/app/main.py
+++ b/app/main.py
@@ -1139,7 +1139,7 @@ from app.api.anthropic_routes import router as anthropic_router
 from app.api.products import router as products_router
 from app.api.embeddings import router as embeddings_router
 from app.api.monitoring_routes import router as monitoring_router
-from app.api.admin.chunk_quality import router as chunk_quality_router
+from app.api.admin_modules_old.chunk_quality import router as chunk_quality_router
 from app.api.ai_metrics_routes import router as ai_metrics_router
 from app.api.ai_services_routes import router as ai_services_router
 
diff --git a/deployment_error_logs.txt b/deployment_error_logs.txt
new file mode 100644
index 0000000..173c1d9
--- /dev/null
+++ b/deployment_error_logs.txt
@@ -0,0 +1,28 @@
+=== DEPLOYMENT ERROR LOGS ===
+Error: $1
+Time: $(date)
+Working Directory: $(pwd)
+Python Version: $(python --version 2>&1)
+Pip Version: $(pip --version 2>&1)
+Virtual Environment: $(which python)
+Recent pip install output:
+Installing llama-index...
+Requirement already satisfied: llama-index==0.10.57 in ./.venv/lib/python3.9/site-packages (0.10.57)
+Installing pymupdf4llm...
+Requirement already satisfied: pymupdf4llm==0.0.12 in ./.venv/lib/python3.9/site-packages (0.0.12)
+Requirement already satisfied: pymupdf>=1.24.2 in ./.venv/lib/python3.9/site-packages (from pymupdf4llm==0.0.12) (1.26.4)
+Installing llama-index...
+Requirement already satisfied: llama-index==0.10.57 in ./.venv/lib/python3.9/site-packages (0.10.57)
+Installing pymupdf4llm...
+Requirement already satisfied: pymupdf4llm==0.0.12 in ./.venv/lib/python3.9/site-packages (0.0.12)
+Requirement already satisfied: pymupdf>=1.24.2 in ./.venv/lib/python3.9/site-packages (from pymupdf4llm==0.0.12) (1.26.4)
+Installing llama-index...
+Requirement already satisfied: llama-index==0.10.57 in ./.venv/lib/python3.9/site-packages (0.10.57)
+Installing pymupdf4llm...
+Requirement already satisfied: pymupdf4llm==0.0.12 in ./.venv/lib/python3.9/site-packages (0.0.12)
+Requirement already satisfied: pymupdf>=1.24.2 in ./.venv/lib/python3.9/site-packages (from pymupdf4llm==0.0.12) (1.26.4)
+Installing llama-index...
+Requirement already satisfied: llama-index==0.10.57 in ./.venv/lib/python3.9/site-packages (0.10.57)
+Installing pymupdf4llm...
+Requirement already satisfied: pymupdf4llm==0.0.12 in ./.venv/lib/python3.9/site-packages (0.0.12)
+Requirement already satisfied: pymupdf>=1.24.2 in ./.venv/lib/python3.9/site-packages (from pymupdf4llm==0.0.12) (1.26.4)
diff --git a/package_status_report.txt b/package_status_report.txt
index 8f84211..43cad33 100644
--- a/package_status_report.txt
+++ b/package_status_report.txt
@@ -1,14 +1,6 @@
 | Package | Category | Description | Status | Version |
 |---------|----------|-------------|--------|---------|
-| **fastapi** | Web Framework | FastAPI web framework | üü¢ Installed | 0.115.0 |
-| **uvicorn** | Server | ASGI server | üü¢ Installed | 0.24.0 |
-| **opencv-python-headless** | Computer Vision | Image processing | üü¢ Installed | 4.8.0 |
-| **pymupdf4llm** | PDF Processing | PDF processing for LLM | üü¢ Installed | 0.0.12 |
-| **supabase** | Database | Database client | üü¢ Installed | 2.3.0 |
-| **numpy** | Computing | Numerical computing | üü¢ Installed | 1.26.4 |
-| **pandas** | Data | Data manipulation | üü¢ Installed | 2.1.4 |
-| **torch** | AI/ML | PyTorch deep learning | üü¢ Installed | 2.2.2+cpu |
-| **openai** | AI/ML | OpenAI API client | üü¢ Installed | 1.51.0 |
-| **anthropic** | AI/ML | Anthropic API client | üü¢ Installed | 0.23.1 |
-| **llama-index** | AI/ML | LLM framework | üî¥ Missing | N/A |
-
+| **System Dependencies** | System | Ubuntu packages for PyMuPDF | üî¥ Failed | N/A |
+| **Python Environment** | Runtime | Virtual environment setup | üü° Partial | N/A |
+| **Package Installation** | Dependencies | Python package installation | üî¥ Failed | N/A |
+| **Deployment Status** | Overall | Complete deployment process | üî¥ Failed | N/A |
-- 
2.43.0

