#!/usr/bin/env python3
"""
COMPREHENSIVE PDF PROCESSING END-TO-END TEST

Tests BOTH Claude Vision and GPT Vision models
Reports ALL 12 comprehensive metrics as requested:

1. Total Products discovered + time taken
2. Total Pages processed + time taken
3. Total Chunks created + time taken
4. Total Images processed + time taken
5. Total Embeddings created + time taken
6. Total Errors + time taken
7. Total Relationships created + time taken
8. Total Metadata extracted + time taken
9. Total Memory used + time
10. Total CPU used + time
11. Total Cost (AI API usage)
12. Total Time for entire process

Pipeline Stages (Internal Endpoints):
10. classify-images      (10-20%)  - Llama Vision + Claude validation
20. upload-images        (20-30%)  - Upload to Supabase Storage
30. save-images-db       (30-50%)  - Save to DB + SigLIP/CLIP embeddings
40. extract-metadata     (50-60%)  - AI metadata extraction (Claude/GPT)
50. create-chunks        (60-80%)  - Semantic chunking + text embeddings
60. create-relationships (80-100%) - Create all relationships
"""

import asyncio
import json
import time
import subprocess
import sys
import os
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path

import httpx

# Configuration
# Use production URL for K8s testing
MIVAA_API = os.getenv('MIVAA_API', 'https://v1api.materialshub.gr')
HARMONY_PDF_URL = 'https://bgbavxtjlbvgplozizxu.supabase.co/storage/v1/object/public/pdf-documents/harmony-signature-book-24-25.pdf'
WORKSPACE_ID = 'ffafc28b-1b8b-4b0d-b226-9f9a6154004e'

# Supabase authentication - get from environment or use service role key
SUPABASE_SERVICE_ROLE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY', 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImJnYmF2eHRqbGJ2Z3Bsb3ppenh1Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MTkwNjAzMSwiZXhwIjoyMDY3NDgyMDMxfQ.KCfP909Qttvs3jr4t1pTYMjACVz2-C-Ga4Xm_ZyecwM')

# Test both vision models
TEST_MODELS = ['claude-vision', 'gpt-vision']

# AI Model Pricing (per 1M tokens)
MODEL_PRICING = {
    'claude-sonnet-4-5-20250929': {'input': 3.00, 'output': 15.00},
    'claude-haiku-4-5-20251001': {'input': 0.80, 'output': 4.00},
    'gpt-4o': {'input': 2.50, 'output': 10.00},
    'text-embedding-3-small': {'input': 0.02, 'output': 0},
    'llama-vision': {'input': 0.18, 'output': 0.18}
}

# NOVA product search criteria
NOVA_PRODUCT = {
    'name': 'NOVA',
    'designer': 'SG NY',
    'searchTerms': ['NOVA', 'SG NY', 'SGNY']
}


# Logging utilities
def log(category: str, message: str, level: str = 'info'):
    """Log a message with emoji and category."""
    timestamp = datetime.now().isoformat()
    emoji_map = {
        'step': 'üìã',
        'info': 'üìù',
        'success': '‚úÖ',
        'error': '‚ùå',
        'warning': '‚ö†Ô∏è',
        'data': 'üìä'
    }
    emoji = emoji_map.get(level, 'üìù')
    print(f"{emoji} [{category}] {message}")


def log_section(title: str):
    """Log a section header."""
    print('\n' + '=' * 100)
    print(f'üéØ {title}')
    print('=' * 100)


# Get system metrics (memory, CPU) - works on Linux server
async def get_system_metrics() -> Dict[str, float]:
    """Get current system metrics for MIVAA process."""
    try:
        # Get MIVAA service PID
        pid_result = subprocess.run(
            ["pgrep", "-f", "uvicorn.*mivaa"],
            capture_output=True,
            text=True
        )
        pid = pid_result.stdout.strip().split('\n')[0] if pid_result.stdout else None
        
        if not pid:
            return {'memory_mb': 0, 'cpu_percent': 0}
        
        # Get memory (RSS in KB) and CPU
        ps_result = subprocess.run(
            ["ps", "-p", pid, "-o", "rss=,pcpu="],
            capture_output=True,
            text=True
        )
        
        if ps_result.returncode == 0:
            parts = ps_result.stdout.strip().split()
            mem_kb = float(parts[0]) if len(parts) > 0 else 0
            cpu = float(parts[1]) if len(parts) > 1 else 0
            
            return {
                'memory_mb': round(mem_kb / 1024),
                'cpu_percent': cpu
            }
    except Exception as e:
        log('METRICS', f'Error getting system metrics: {e}', 'warning')
    
    return {'memory_mb': 0, 'cpu_percent': 0}


# Calculate AI cost from API calls
def calculate_cost(ai_calls: List[Dict]) -> float:
    """Calculate total cost from AI API calls."""
    total_cost = 0.0
    
    for call in ai_calls:
        model = call.get('model')
        pricing = MODEL_PRICING.get(model)
        if not pricing:
            continue
        
        input_tokens = call.get('input_tokens', 0)
        output_tokens = call.get('output_tokens', 0)
        
        input_cost = (input_tokens / 1_000_000) * pricing['input']
        output_cost = (output_tokens / 1_000_000) * pricing['output']
        total_cost += input_cost + output_cost
    
    return total_cost


# Format time duration
def format_duration(ms: float) -> str:
    """Format milliseconds into human-readable duration."""
    if ms < 1000:
        return f"{ms:.0f}ms"
    elif ms < 60000:
        return f"{ms / 1000:.1f}s"
    else:
        return f"{ms / 60000:.1f}min"


# Cleanup function to delete all old test data
async def cleanup_old_test_data(client: httpx.AsyncClient):
    """Delete all old test data from database."""
    log('CLEANUP', 'Deleting all old test data from database...', 'step')

    try:
        # Find all Harmony PDF documents
        response = await client.get(
            f'{MIVAA_API}/api/rag/documents/jobs',
            params={'limit': 100, 'sort': 'created_at:desc'}
        )

        if response.status_code != 200:
            log('CLEANUP', 'Failed to fetch jobs list', 'warning')
            return

        data = response.json()
        jobs = data.get('jobs', [])

        # Filter for Harmony PDF jobs
        harmony_jobs = []
        for job in jobs:
            filename = job.get('metadata', {}).get('filename', '') or job.get('filename', '')
            if 'harmony-signature-book-24-25' in filename:
                harmony_jobs.append(job)

        if not harmony_jobs:
            log('CLEANUP', 'No old Harmony PDF jobs found', 'info')
            return

        log('CLEANUP', f'Found {len(harmony_jobs)} old Harmony PDF jobs to delete', 'info')

        # Delete each job and its associated data
        for job in harmony_jobs:
            job_id = job.get('id')
            document_id = job.get('document_id')

            log('CLEANUP', f'Deleting job {job_id} and document {document_id}...', 'info')

            try:
                delete_response = await client.delete(
                    f'{MIVAA_API}/api/rag/documents/jobs/{job_id}'
                )

                if delete_response.status_code == 200:
                    log('CLEANUP', f'‚úÖ Deleted job {job_id}', 'success')
                else:
                    log('CLEANUP', f'‚ö†Ô∏è Failed to delete job {job_id}: {delete_response.status_code}', 'warning')
            except Exception as e:
                log('CLEANUP', f'‚ùå Error deleting job {job_id}: {e}', 'error')

        log('CLEANUP', '‚úÖ Cleanup complete!', 'success')

        # Wait a bit for database cleanup to complete
        await asyncio.sleep(2)

    except Exception as e:
        log('CLEANUP', f'Error during cleanup: {e}', 'error')


# Upload PDF for NOVA extraction
async def upload_pdf_for_nova_extraction(
    client: httpx.AsyncClient,
    discovery_model: str = 'claude-vision'
) -> Dict[str, str]:
    """Upload PDF with specified discovery model."""
    log('UPLOAD', f'Using URL-based upload: {HARMONY_PDF_URL}', 'info')

    # Create form data with URL and processing options
    files = {
        'file_url': (None, HARMONY_PDF_URL),
        'title': (None, f'Comprehensive Test - {discovery_model}'),
        'description': (None, 'Extract all products from Harmony catalog'),
        'tags': (None, 'harmony,test,comprehensive'),
        'categories': (None, 'products'),  # Extract only products
        'processing_mode': (None, 'deep'),  # Deep mode for complete analysis
        'discovery_model': (None, discovery_model),  # Vision model for product discovery
        'chunk_size': (None, '1024'),
        'chunk_overlap': (None, '128'),
        'enable_prompt_enhancement': (None, 'true'),
        'workspace_id': (None, WORKSPACE_ID)
    }

    log('UPLOAD', f'Triggering Consolidated Upload via MIVAA API: {MIVAA_API}/api/rag/documents/upload', 'info')
    log('UPLOAD', f'Mode: deep | Categories: products | Discovery: {discovery_model} | Async: enabled', 'info')

    response = await client.post(
        f'{MIVAA_API}/api/rag/documents/upload',
        files=files,
        timeout=60.0
    )

    if response.status_code != 200:
        error_text = response.text
        raise Exception(f'Upload failed: {response.status_code} - {error_text}')

    result = response.json()

    log('UPLOAD', f'‚úÖ Job ID: {result["job_id"]}', 'success')
    log('UPLOAD', f'‚úÖ Document ID: {result["document_id"]}', 'success')
    log('UPLOAD', f'‚úÖ Status: {result["status"]}', 'success')

    if result.get('message'):
        log('UPLOAD', result['message'], 'info')

    if result.get('status_url'):
        log('UPLOAD', f'üìç Status URL: {result["status_url"]}', 'info')

    return {
        'job_id': result['job_id'],
        'document_id': result['document_id']
    }


# Validate data saved
async def validate_data_saved(
    client: httpx.AsyncClient,
    document_id: str,
    job_data: Dict
) -> Dict:
    """Validate that data is actually being saved to database via MIVAA API."""
    validation = {
        'chunks': 0,
        'images': 0,
        'products': 0,
        'relevancies': 0,
        'textEmbeddings': 0,
        'imageEmbeddings': 0
    }

    try:
        # Check chunks using consolidated RAG endpoint
        chunks_response = await client.get(
            f'{MIVAA_API}/api/rag/chunks',
            params={'document_id': document_id, 'limit': 1000}
        )
        if chunks_response.status_code == 200:
            chunks_data = chunks_response.json()
            chunks = chunks_data.get('chunks', [])
            validation['chunks'] = len(chunks)
            validation['textEmbeddings'] = sum(1 for c in chunks if c.get('embedding'))

        # Check images using consolidated RAG endpoint
        images_response = await client.get(
            f'{MIVAA_API}/api/rag/images',
            params={'document_id': document_id, 'limit': 1000}
        )
        if images_response.status_code == 200:
            images_data = images_response.json()
            images = images_data.get('images', [])
            validation['images'] = len(images)
            validation['imageEmbeddings'] = sum(1 for img in images if img.get('visual_clip_embedding_512'))

        # Check products using consolidated RAG endpoint
        products_response = await client.get(
            f'{MIVAA_API}/api/rag/products',
            params={'document_id': document_id, 'limit': 1000}
        )
        if products_response.status_code == 200:
            products_data = products_response.json()
            validation['products'] = len(products_data.get('products', []))

        # Check product-image relevancies
        product_image_rel_response = await client.get(
            f'{MIVAA_API}/api/rag/product-image-relationships',
            params={'document_id': document_id, 'limit': 1000}
        )
        if product_image_rel_response.status_code == 200:
            rel_data = product_image_rel_response.json()
            validation['relevancies'] = rel_data.get('count', 0)

        # Compare with job metadata
        job_chunks = job_data.get('metadata', {}).get('chunks_created', 0)
        job_images = job_data.get('metadata', {}).get('images_extracted', 0)
        job_products = job_data.get('metadata', {}).get('products_created', 0)

        chunks_match = validation['chunks'] == job_chunks
        # Images count mismatch is EXPECTED due to focused extraction filtering
        images_match = validation['images'] <= job_images
        products_match = validation['products'] == job_products

        log('VALIDATE', f"Chunks: {validation['chunks']}/{job_chunks} {'‚úÖ' if chunks_match else '‚ùå'}",
            'success' if chunks_match else 'error')
        log('VALIDATE', f"  - With Text Embeddings: {validation['textEmbeddings']}", 'info')
        log('VALIDATE', f"Images: {validation['images']}/{job_images} {'‚úÖ' if images_match else '‚ùå'}",
            'success' if images_match else 'info')
        if validation['images'] < job_images:
            log('VALIDATE', f"  ‚ÑπÔ∏è  Filtered {job_images - validation['images']} non-material images (expected behavior)", 'info')
        log('VALIDATE', f"  - With Image Embeddings: {validation['imageEmbeddings']}", 'info')
        log('VALIDATE', f"Products: {validation['products']}/{job_products} {'‚úÖ' if products_match else '‚ùå'}",
            'success' if products_match else 'error')
        log('VALIDATE', f"Relevancies: {validation['relevancies']}", 'info')

        return {
            'valid': chunks_match and images_match and products_match,
            'validation': validation,
            'expected': {'chunks': job_chunks, 'images': job_images, 'products': job_products}
        }
    except Exception as e:
        log('VALIDATE', f'Validation error: {e}', 'error')
        return {'valid': False, 'error': str(e)}


# Monitor job with comprehensive metrics collection
async def monitor_processing_job_with_metrics(
    client: httpx.AsyncClient,
    job_id: str,
    document_id: str,
    metrics: Dict
):
    """Monitor job progress and collect comprehensive metrics."""
    max_attempts = 480  # 2 hours
    poll_interval = 15  # 15 seconds
    last_progress = 0

    log('MONITOR', f'Starting job monitoring with metrics collection for: {job_id}', 'info')

    # Start system monitoring task
    system_samples = []

    async def sample_system_metrics():
        """Background task to sample system metrics."""
        while True:
            sys_metrics = await get_system_metrics()
            system_samples.append(sys_metrics)
            if sys_metrics['memory_mb'] > metrics['system']['peak_memory_mb']:
                metrics['system']['peak_memory_mb'] = sys_metrics['memory_mb']
            await asyncio.sleep(5)  # Sample every 5 seconds

    # Start background monitoring
    monitor_task = asyncio.create_task(sample_system_metrics())

    try:
        for attempt in range(1, max_attempts + 1):
            await asyncio.sleep(poll_interval)

            status_response = await client.get(f'{MIVAA_API}/api/rag/documents/job/{job_id}')
            if status_response.status_code != 200:
                continue

            job_data = status_response.json()
            status = job_data.get('status')
            progress = job_data.get('progress', 0)
            metadata = job_data.get('metadata', {})
            current_step = metadata.get('current_step') or metadata.get('stage') or 'Processing'

            # Track stage timings
            if current_step and current_step not in metrics['stages']:
                metrics['stages'][current_step] = {
                    'start': time.time() * 1000,
                    'end': None,
                    'duration_ms': 0
                }

            # Update metrics from job metadata
            if metadata.get('products_created'):
                metrics['products']['count'] = metadata['products_created']
            if metadata.get('total_pages'):
                metrics['pages']['count'] = metadata['total_pages']
            if metadata.get('chunks_created'):
                metrics['chunks']['count'] = metadata['chunks_created']
            if metadata.get('images_extracted'):
                metrics['images']['count'] = metadata['images_extracted']
            if metadata.get('errors'):
                errors = metadata['errors']
                metrics['errors']['count'] = len(errors) if isinstance(errors, list) else 0

            # Log progress
            progress_msg = f'[{attempt}/{max_attempts}] {status.upper()} ({progress}%) - {current_step}'
            if metadata.get('chunks_created'):
                progress_msg += f" | Chunks: {metadata['chunks_created']}"
            if metadata.get('images_extracted'):
                progress_msg += f" | Images: {metadata['images_extracted']}"
            if metadata.get('products_created'):
                progress_msg += f" | Products: {metadata['products_created']}"
            log('MONITOR', progress_msg, 'info')

            last_progress = progress

            if status == 'completed':
                log('MONITOR', '‚úÖ Job completed successfully!', 'success')

                # Mark all stages as complete
                current_time = time.time() * 1000
                for stage in metrics['stages']:
                    if not metrics['stages'][stage]['end']:
                        metrics['stages'][stage]['end'] = current_time
                        metrics['stages'][stage]['duration_ms'] = (
                            metrics['stages'][stage]['end'] - metrics['stages'][stage]['start']
                        )

                # Store system samples
                metrics['system']['samples'] = system_samples
                monitor_task.cancel()
                return

            if status == 'failed':
                monitor_task.cancel()
                raise Exception(f"Job failed: {metadata.get('error', 'Unknown error')}")

        monitor_task.cancel()
        raise Exception('Job monitoring timed out after 2 hours')

    except asyncio.CancelledError:
        pass
    except Exception as e:
        monitor_task.cancel()
        raise e


# Collect final metrics from database
async def collect_final_metrics(
    client: httpx.AsyncClient,
    document_id: str,
    metrics: Dict
):
    """Collect final metrics from database."""
    try:
        # Fetch all data
        chunks_response = await client.get(f'{MIVAA_API}/api/rag/documents/{document_id}/chunks')
        images_response = await client.get(f'{MIVAA_API}/api/rag/documents/{document_id}/images')
        products_response = await client.get(f'{MIVAA_API}/api/rag/documents/{document_id}/products')
        chunk_image_rel_response = await client.get(
            f'{MIVAA_API}/api/rag/documents/{document_id}/chunk-image-relevancies'
        )
        product_image_rel_response = await client.get(
            f'{MIVAA_API}/api/rag/documents/{document_id}/product-image-relevancies'
        )
        chunk_product_rel_response = await client.get(
            f'{MIVAA_API}/api/rag/documents/{document_id}/chunk-product-relevancies'
        )

        chunks = chunks_response.json() if chunks_response.status_code == 200 else []
        images = images_response.json() if images_response.status_code == 200 else []
        products = products_response.json() if products_response.status_code == 200 else []
        chunk_image_rels = chunk_image_rel_response.json() if chunk_image_rel_response.status_code == 200 else []
        product_image_rels = product_image_rel_response.json() if product_image_rel_response.status_code == 200 else []
        chunk_product_rels = chunk_product_rel_response.json() if chunk_product_rel_response.status_code == 200 else []

        # Update metrics
        metrics['chunks']['count'] = len(chunks)
        metrics['images']['count'] = len(images)
        metrics['products']['count'] = len(products)

        # Count embeddings
        metrics['embeddings']['text'] = sum(1 for c in chunks if c.get('embedding'))

        # Count CLIP embeddings (5 types per image)
        clip_count = 0
        for img in images:
            if img.get('visual_clip_embedding_512'):
                clip_count += 1
            if img.get('color_clip_embedding_512'):
                clip_count += 1
            if img.get('texture_clip_embedding_512'):
                clip_count += 1
            if img.get('application_clip_embedding_512'):
                clip_count += 1
            if img.get('material_clip_embedding_512'):
                clip_count += 1

        metrics['embeddings']['clip'] = clip_count
        metrics['embeddings']['total'] = metrics['embeddings']['text'] + metrics['embeddings']['clip']

        # Count relationships
        metrics['relationships']['chunk_image'] = len(chunk_image_rels)
        metrics['relationships']['product_image'] = len(product_image_rels)
        metrics['relationships']['chunk_product'] = len(chunk_product_rels)
        metrics['relationships']['total'] = (
            len(chunk_image_rels) + len(product_image_rels) + len(chunk_product_rels)
        )

        # Count metadata
        metrics['metadata']['chunks'] = sum(
            1 for c in chunks if c.get('metadata') and len(c['metadata']) > 0
        )
        metrics['metadata']['products'] = sum(
            1 for p in products if p.get('metadata') and len(p['metadata']) > 0
        )
        metrics['metadata']['total'] = metrics['metadata']['chunks'] + metrics['metadata']['products']

        # Calculate average CPU
        if metrics['system']['samples']:
            total_cpu = sum(s['cpu_percent'] for s in metrics['system']['samples'])
            metrics['system']['avg_cpu_percent'] = total_cpu / len(metrics['system']['samples'])

        log('COLLECT',
            f"‚úÖ Collected final metrics: {metrics['products']['count']} products, "
            f"{metrics['chunks']['count']} chunks, {metrics['images']['count']} images",
            'success')

    except Exception as e:
        log('COLLECT', f'Error collecting final metrics: {e}', 'error')


# Generate report for single model
def generate_model_report(metrics: Dict):
    """Generate comprehensive report for a single model."""
    log_section(f"REPORT FOR {metrics['model'].upper()}")

    print('\nüìä COMPREHENSIVE METRICS:\n')

    print(f"1Ô∏è‚É£  Products: {metrics['products']['count']} "
          f"(Time: {format_duration(metrics['products'].get('time_ms', 0))})")
    print(f"2Ô∏è‚É£  Pages: {metrics['pages']['count']} "
          f"(Time: {format_duration(metrics['pages'].get('time_ms', 0))})")
    print(f"3Ô∏è‚É£  Chunks: {metrics['chunks']['count']} "
          f"(Time: {format_duration(metrics['chunks'].get('time_ms', 0))})")
    print(f"4Ô∏è‚É£  Images: {metrics['images']['count']} "
          f"(Time: {format_duration(metrics['images'].get('time_ms', 0))})")
    print(f"5Ô∏è‚É£  Embeddings: {metrics['embeddings']['total']} "
          f"(Text: {metrics['embeddings']['text']}, CLIP: {metrics['embeddings']['clip']}) "
          f"(Time: {format_duration(metrics['embeddings'].get('time_ms', 0))})")
    print(f"6Ô∏è‚É£  Errors: {metrics['errors']['count']} "
          f"(Time: {format_duration(metrics['errors'].get('time_ms', 0))})")
    print(f"7Ô∏è‚É£  Relationships: {metrics['relationships']['total']} "
          f"(Chunk-Image: {metrics['relationships']['chunk_image']}, "
          f"Product-Image: {metrics['relationships']['product_image']}, "
          f"Chunk-Product: {metrics['relationships']['chunk_product']}) "
          f"(Time: {format_duration(metrics['relationships'].get('time_ms', 0))})")
    print(f"8Ô∏è‚É£  Metadata: {metrics['metadata']['total']} "
          f"(Chunks: {metrics['metadata']['chunks']}, Products: {metrics['metadata']['products']}) "
          f"(Time: {format_duration(metrics['metadata'].get('time_ms', 0))})")
    print(f"9Ô∏è‚É£  Memory: Peak {metrics['system']['peak_memory_mb']}MB "
          f"({len(metrics['system']['samples'])} samples)")
    print(f"üîü CPU: Average {metrics['system']['avg_cpu_percent']:.1f}%")
    print(f"1Ô∏è‚É£1Ô∏è‚É£ Cost: ${metrics['cost']['total_usd']:.4f} "
          f"({len(metrics['cost']['ai_calls'])} AI calls)")
    print(f"1Ô∏è‚É£2Ô∏è‚É£ Total Time: {format_duration(metrics['total_time_ms'])}")

    print('\n‚è±Ô∏è  STAGE TIMINGS:\n')
    for stage, timing in metrics['stages'].items():
        print(f"  {stage}: {format_duration(timing['duration_ms'])}")

    # Save detailed JSON report
    report_path = f"comprehensive-test-{metrics['model']}-{int(time.time() * 1000)}.json"
    with open(report_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    log('REPORT', f'Detailed report saved to: {report_path}', 'success')


# Generate comparison report for all models
def generate_comparison_report(all_results: List[Dict]):
    """Generate comparison report for all tested models."""
    log_section('COMPARISON REPORT - ALL MODELS')

    print('\nüìä MODEL COMPARISON:\n')

    successful_results = [r for r in all_results if not r.get('failed')]

    if not successful_results:
        log('REPORT', 'No successful tests to compare', 'error')
        return

    # Create comparison table
    print('‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê')
    print('‚îÇ Metric              ‚îÇ Claude Vision    ‚îÇ GPT Vision       ‚îÇ')
    print('‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§')

    metrics_paths = [
        'products.count', 'chunks.count', 'images.count', 'embeddings.total',
        'relationships.total', 'total_time_ms', 'system.peak_memory_mb', 'cost.total_usd'
    ]
    labels = [
        'Products', 'Chunks', 'Images', 'Embeddings',
        'Relationships', 'Total Time', 'Peak Memory (MB)', 'Cost (USD)'
    ]

    def get_nested_value(obj, path):
        """Get nested dictionary value by path."""
        if not obj:
            return 0
        keys = path.split('.')
        value = obj
        for key in keys:
            value = value.get(key, 0) if isinstance(value, dict) else 0
        return value

    for i, metric_path in enumerate(metrics_paths):
        label = labels[i]

        claude_result = next((r for r in successful_results if r['model'] == 'claude-vision'), None)
        gpt_result = next((r for r in successful_results if r['model'] == 'gpt-vision'), None)

        claude_value = get_nested_value(claude_result, metric_path)
        gpt_value = get_nested_value(gpt_result, metric_path)

        if metric_path == 'total_time_ms':
            claude_str = format_duration(claude_value)
            gpt_str = format_duration(gpt_value)
        elif metric_path == 'cost.total_usd':
            claude_str = f'${claude_value:.4f}'
            gpt_str = f'${gpt_value:.4f}'
        else:
            claude_str = str(claude_value)
            gpt_str = str(gpt_value)

        print(f'‚îÇ {label.ljust(19)} ‚îÇ {claude_str.ljust(16)} ‚îÇ {gpt_str.ljust(16)} ‚îÇ')

    print('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò')

    # Save comparison report
    comparison_path = f'comparison-report-{int(time.time() * 1000)}.json'
    with open(comparison_path, 'w') as f:
        json.dump(all_results, f, indent=2)
    log('REPORT', f'Comparison report saved to: {comparison_path}', 'success')


# Run test for single model with comprehensive metrics
async def run_single_model_test(discovery_model: str) -> Dict:
    """Run comprehensive test for a single vision model."""
    test_start = time.time() * 1000
    metrics = {
        'model': discovery_model,
        'stages': {},
        'products': {'count': 0, 'time_ms': 0},
        'pages': {'count': 0, 'time_ms': 0},
        'chunks': {'count': 0, 'time_ms': 0},
        'images': {'count': 0, 'time_ms': 0},
        'embeddings': {'text': 0, 'clip': 0, 'total': 0, 'time_ms': 0},
        'errors': {'count': 0, 'time_ms': 0},
        'relationships': {
            'chunk_image': 0, 'product_image': 0, 'chunk_product': 0,
            'total': 0, 'time_ms': 0
        },
        'metadata': {'chunks': 0, 'products': 0, 'total': 0, 'time_ms': 0},
        'system': {'peak_memory_mb': 0, 'avg_cpu_percent': 0, 'samples': []},
        'cost': {'total_usd': 0, 'ai_calls': []},
        'total_time_ms': 0
    }

    # Create HTTP client with authentication headers
    headers = {
        'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}',
        'apikey': SUPABASE_SERVICE_ROLE_KEY
    }

    async with httpx.AsyncClient(timeout=120.0, headers=headers) as client:
        try:
            # Step 0: Clean up old test data
            await cleanup_old_test_data(client)

            # Step 1: Upload PDF with specified discovery model
            log('UPLOAD', f'Starting PDF upload with discovery_model={discovery_model}', 'step')
            upload_result = await upload_pdf_for_nova_extraction(client, discovery_model)
            job_id = upload_result['job_id']
            document_id = upload_result['document_id']

            log('UPLOAD', f'Job ID: {job_id}', 'info')
            log('UPLOAD', f'Document ID: {document_id}', 'info')

            # Step 2: Monitor job with metrics collection
            log('MONITOR', 'Monitoring job progress and collecting metrics...', 'step')
            await monitor_processing_job_with_metrics(client, job_id, document_id, metrics)

            # Step 3: Collect final data
            log('COLLECT', 'Collecting final data from database...', 'step')
            await collect_final_metrics(client, document_id, metrics)

            # Calculate total time
            metrics['total_time_ms'] = time.time() * 1000 - test_start

            # Generate report for this model
            generate_model_report(metrics)

            return metrics

        except Exception as e:
            log('ERROR', f'Test failed: {e}', 'error')
            metrics['errors']['count'] += 1
            metrics['total_time_ms'] = time.time() * 1000 - test_start
            return metrics


# Main test function - tests BOTH vision models
async def run_nova_product_test():
    """Run comprehensive PDF processing test for both vision models."""
    log_section('COMPREHENSIVE PDF PROCESSING TEST - BOTH VISION MODELS')

    print(f'PDF: {HARMONY_PDF_URL}')
    print(f'Workspace: {WORKSPACE_ID}')
    print(f'MIVAA API: {MIVAA_API}')
    print(f'Models to test: {", ".join(TEST_MODELS)}\n')

    all_results = []

    for model in TEST_MODELS:
        log_section(f'TESTING MODEL: {model.upper()}')

        try:
            result = await run_single_model_test(model)
            all_results.append(result)

            # Wait between tests
            if model != TEST_MODELS[-1]:
                log('WAIT', 'Waiting 30 seconds before next test...', 'info')
                await asyncio.sleep(30)
        except Exception as e:
            log('ERROR', f'Test failed for {model}: {e}', 'error')
            all_results.append({'model': model, 'error': str(e), 'failed': True})

    # Generate comparison report
    generate_comparison_report(all_results)


# Health check before running tests
async def check_api_health():
    """Check if the API is healthy before running tests."""
    log_section('API HEALTH CHECK')

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(f'{MIVAA_API}/health')
            health_data = response.json()

            log('INFO', f'API Status: {health_data.get("status", "unknown")}', 'info')
            log('INFO', f'API Version: {health_data.get("version", "unknown")}', 'info')

            # Check each service
            services = health_data.get('services', {})
            all_healthy = True

            for service_name, service_status in services.items():
                status = service_status.get('status', 'unknown')
                message = service_status.get('message', '')

                if status == 'healthy':
                    log('SERVICE', f'{service_name}: ‚úÖ {message}', 'success')
                else:
                    log('SERVICE', f'{service_name}: ‚ùå {message}', 'error')
                    all_healthy = False

                    # Show config status if available (for database)
                    if 'config_status' in service_status:
                        config = service_status['config_status']
                        log('CONFIG', f'  SUPABASE_URL: {config.get("supabase_url", "UNKNOWN")}', 'warning')
                        log('CONFIG', f'  SUPABASE_ANON_KEY: {config.get("supabase_anon_key", "UNKNOWN")}', 'warning')
                        log('CONFIG', f'  SUPABASE_SERVICE_ROLE_KEY: {config.get("supabase_service_role_key", "UNKNOWN")}', 'warning')

            if not all_healthy:
                log('ERROR', 'API is not fully healthy. Cannot proceed with tests.', 'error')
                log('ERROR', 'Please fix the unhealthy services before running E2E tests.', 'error')
                sys.exit(1)

            log('SUCCESS', 'All services are healthy! Proceeding with tests...', 'success')
            return True

    except Exception as e:
        log('ERROR', f'Failed to check API health: {e}', 'error')
        log('ERROR', f'Make sure the API is running at {MIVAA_API}', 'error')
        sys.exit(1)


# Entry point
async def main():
    """Main entry point for the test script."""
    try:
        # Check API health first
        await check_api_health()

        # Run the tests
        await run_nova_product_test()
    except Exception as e:
        log('FATAL', f'Fatal error: {e}', 'error')
        sys.exit(1)


if __name__ == '__main__':
    # Run the async main function
    asyncio.run(main())

